{
 "metadata": {
  "name": "",
  "signature": "sha256:2a27b2dbfbc1e71514b7ce54e4b1a1b8fbfc1812b9fde0af5dcabb562d909ba6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import math\n",
      "from scipy.stats import vonmises\n",
      "\n",
      "nneuron = 121\n",
      "t_pi = 2*math.pi\n",
      "min_angle = 0\n",
      "max_angle = t_pi\n",
      "sprefs = np.linspace(min_angle, max_angle, nneuron)\n",
      "\n",
      "kappa_tc = t_pi/2\n",
      "ndata = 3000\n",
      "r_max = 30\n",
      "\n",
      "c_50 = 13.1\n",
      "\n",
      "def generate_popcode_data(ndata, nneuron, kappa_tc, c_50, r_max, noise, sort):\n",
      "    s = np.random.rand(2, ndata)*t_pi\n",
      "    if sort:\n",
      "        s = np.sort(s, axis=0)\n",
      "    c = np.ones((2, ndata)) * .5\n",
      "    c_rms = np.sqrt(np.square(c[0]) + np.square(c[1]))\n",
      "    stim_0 = c[0] * np.asarray([vonmises.pdf(s[0], kappa_tc, loc=sprefs[sp]) for sp in range(len(sprefs))])\n",
      "    stim_1 = c[1] * np.asarray([vonmises.pdf(s[1], kappa_tc, loc=sprefs[sp]) for sp in range(len(sprefs))])\n",
      "    r = r_max * (stim_0 + stim_1)/(c_50 + c_rms)\n",
      "    r = r.T\n",
      "    s = s.T\n",
      "    c = c.T\n",
      "    if noise == \"poisson\":\n",
      "        r = np.random.poisson(r) + 0.0\n",
      "    return r, s, c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ndata = 1\n",
      "r, s, _ = generate_popcode_data(ndata, nneuron, kappa_tc, c_50, r_max, \"poisson\", True)\n",
      "print s\n",
      "print np.sum(r, axis=1)\n",
      "print sprefs\n",
      "print r[0]\n",
      "print s[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.84462208  2.99038681]]\n",
        "[ 47.]\n",
        "[ 0.          0.05235988  0.10471976  0.15707963  0.20943951  0.26179939\n",
        "  0.31415927  0.36651914  0.41887902  0.4712389   0.52359878  0.57595865\n",
        "  0.62831853  0.68067841  0.73303829  0.78539816  0.83775804  0.89011792\n",
        "  0.9424778   0.99483767  1.04719755  1.09955743  1.15191731  1.20427718\n",
        "  1.25663706  1.30899694  1.36135682  1.41371669  1.46607657  1.51843645\n",
        "  1.57079633  1.6231562   1.67551608  1.72787596  1.78023584  1.83259571\n",
        "  1.88495559  1.93731547  1.98967535  2.04203522  2.0943951   2.14675498\n",
        "  2.19911486  2.25147474  2.30383461  2.35619449  2.40855437  2.46091425\n",
        "  2.51327412  2.565634    2.61799388  2.67035376  2.72271363  2.77507351\n",
        "  2.82743339  2.87979327  2.93215314  2.98451302  3.0368729   3.08923278\n",
        "  3.14159265  3.19395253  3.24631241  3.29867229  3.35103216  3.40339204\n",
        "  3.45575192  3.5081118   3.56047167  3.61283155  3.66519143  3.71755131\n",
        "  3.76991118  3.82227106  3.87463094  3.92699082  3.97935069  4.03171057\n",
        "  4.08407045  4.13643033  4.1887902   4.24115008  4.29350996  4.34586984\n",
        "  4.39822972  4.45058959  4.50294947  4.55530935  4.60766923  4.6600291\n",
        "  4.71238898  4.76474886  4.81710874  4.86946861  4.92182849  4.97418837\n",
        "  5.02654825  5.07890812  5.131268    5.18362788  5.23598776  5.28834763\n",
        "  5.34070751  5.39306739  5.44542727  5.49778714  5.55014702  5.6025069\n",
        "  5.65486678  5.70722665  5.75958653  5.81194641  5.86430629  5.91666616\n",
        "  5.96902604  6.02138592  6.0737458   6.12610567  6.17846555  6.23082543\n",
        "  6.28318531]\n",
        "[ 1.  0.  0.  0.  2.  0.  1.  0.  1.  1.  0.  2.  0.  0.  2.  2.  0.  1.\n",
        "  0.  1.  1.  2.  1.  2.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
        "  1.  0.  1.  2.  1.  2.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  1.\n",
        "  0.  1.  0.  0.  1.  2.  0.  2.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.\n",
        "  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.]\n",
        "[ 0.84462208  2.99038681]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pystan\n",
      "schools_code = \"\"\"\n",
      "data {\n",
      "    int<lower=0> J; // number of schools\n",
      "    real y[J]; // estimated treatment effects\n",
      "    real<lower=0> sigma[J]; // s.e. of effect estimates\n",
      "}\n",
      "parameters {\n",
      "    real mu;\n",
      "    real<lower=0> tau;\n",
      "    real eta[J];\n",
      "}\n",
      "transformed parameters {\n",
      "    real theta[J];\n",
      "    for (j in 1:J)\n",
      "        theta[j] <- mu + tau * eta[j];\n",
      "}\n",
      "model {\n",
      "    eta ~ normal(0, 1);\n",
      "    y ~ normal(theta, sigma);\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "schools_dat = {'J': 8,\n",
      "               'y': [28,  8, -3,  7, -1,  1, 18, 12],\n",
      "               'sigma': [15, 10, 16, 11,  9, 11, 10, 18]}\n",
      "\n",
      "fit = pystan.stan(model_code=schools_code, data=schools_dat,\n",
      "                  iter=1000, chains=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pystan\n",
      "neurons_code = \"\"\"\n",
      "data {\n",
      "    int<lower=0> N; // number of neurons\n",
      "    int r[N]; // neural response\n",
      "    real sprefs[N]; // preferred stimuli\n",
      "    real<lower=0> c_1;\n",
      "    real<lower=0> c_2;\n",
      "    real c_50;\n",
      "    int r_max;\n",
      "    real c_rms;\n",
      "}\n",
      "parameters {\n",
      "    real<lower=0> s_1;\n",
      "    real<lower=0> s_2;\n",
      "}\n",
      "transformed parameters {\n",
      "    real lambda[N];\n",
      "    for (n in 1:N)\n",
      "        lambda[n] <- r_max * ((c_1 * exp(von_mises_log(s_1, sprefs[n], pi())) + c_2 * exp(von_mises_log(s_1, sprefs[n], pi())))/(c_rms + c_50));\n",
      "}\n",
      "model {\n",
      "    s_1 ~ uniform(0, 2 * pi());\n",
      "    s_2 ~ uniform(s_1, 2 * pi());\n",
      "    r ~ poisson(lambda);\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "ndata = 1\n",
      "r, s, _ = generate_popcode_data(ndata, nneuron, kappa_tc, c_50, r_max, \"poisson\", True)\n",
      "\n",
      "neurons_dat = {'N': 121,\n",
      "               'r': r[0],\n",
      "               'sprefs': sprefs,\n",
      "               'c_1': .5,\n",
      "               'c_2': .5,\n",
      "               'c_50': 13.1,\n",
      "               'r_max': 30,\n",
      "               'c_rms': 0.707106781}\n",
      "\n",
      "fit = pystan.stan(model_code=neurons_code, data=neurons_dat,\n",
      "                  iter=1000, chains=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "RuntimeError",
       "evalue": "int variable contained non-int values; processing stage=data initialization; variable name=r; base type=int",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-10-4945b7724240>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m fit = pystan.stan(model_code=neurons_code, data=neurons_dat,\n\u001b[0;32m---> 42\u001b[0;31m                   iter=1000, chains=4)\n\u001b[0m",
        "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pystan/api.pyc\u001b[0m in \u001b[0;36mstan\u001b[0;34m(file, model_name, model_code, fit, data, pars, chains, iter, warmup, thin, init, seed, algorithm, control, sample_file, diagnostic_file, save_dso, verbose, boost_lib, eigen_lib, n_jobs, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m                      \u001b[0msample_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiagnostic_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiagnostic_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                      \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                      n_jobs=n_jobs, **kwargs)\n\u001b[0m\u001b[1;32m    386\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pystan/model.pyc\u001b[0m in \u001b[0;36msampling\u001b[0;34m(self, data, pars, chains, iter, warmup, thin, seed, init, sample_file, diagnostic_file, verbose, algorithm, control, n_jobs, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Algorithm must be one of {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0mfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0mm_pars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_param_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/var/folders/2p/qvz06jdn5_x7v40bbmgz6wzc0000gr/T/tmpm0krPl/pystan/stanfit4anon_model_58a27eb9e1016bcca9c9e3879dd2bc6d_e8b5c80817b36ec7a0a7f2e78695c4b4.so\u001b[0m in \u001b[0;36mstanfit4anon_model_58a27eb9e1016bcca9c9e3879dd2bc6d_e8b5c80817b36ec7a0a7f2e78695c4b4.StanFit4Model.__cinit__ (/var/folders/2p/qvz06jdn5_x7v40bbmgz6wzc0000gr/T/tmpm0krPl/pystan/stanfit4anon_model_58a27eb9e1016bcca9c9e3879dd2bc6d_e8b5c80817b36ec7a0a7f2e78695c4b4.cpp:7886)\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;31mRuntimeError\u001b[0m: int variable contained non-int values; processing stage=data initialization; variable name=r; base type=int"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Multilayer ReLU net\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import numpy\n",
      "\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "def relu(x):\n",
      "    return theano.tensor.switch(x<0, 0, x)\n",
      "\n",
      "class HiddenLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
      "                 activation=relu):\n",
      "        \"\"\"\n",
      "        Typical hidden layer of a MLP: units are fully-connected and have\n",
      "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
      "        and the bias vector b is of shape (n_out,).\n",
      "\n",
      "        NOTE : The nonlinearity used here is tanh\n",
      "\n",
      "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.dmatrix\n",
      "        :param input: a symbolic tensor of shape (n_examples, n_in)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: dimensionality of input\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of hidden units\n",
      "\n",
      "        :type activation: theano.Op or function\n",
      "        :param activation: Non linearity to be applied in the hidden\n",
      "                           layer\n",
      "        \"\"\"\n",
      "        self.input = input\n",
      "        # end-snippet-1\n",
      "\n",
      "        # `W` is initialized with `W_values` which is uniformely sampled\n",
      "        # from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))\n",
      "        # for tanh activation function\n",
      "        # the output of uniform if converted using asarray to dtype\n",
      "        # theano.config.floatX so that the code is runable on GPU\n",
      "        # Note : optimal initialization of weights is dependent on the\n",
      "        #        activation function used (among other things).\n",
      "        #        For example, results presented in [Xavier10] suggest that you\n",
      "        #        should use 4 times larger initial weights for sigmoid\n",
      "        #        compared to tanh\n",
      "        #        We have no info for other function, so we use the same as\n",
      "        #        tanh.\n",
      "        if W is None:\n",
      "            \"\"\"\n",
      "            W_values = numpy.asarray(\n",
      "                rng.uniform(\n",
      "                    low=-numpy.sqrt(6. / (n_in + n_out)),\n",
      "                    high=numpy.sqrt(6. / (n_in + n_out)),\n",
      "                    size=(n_in, n_out)\n",
      "                ),\n",
      "                dtype=theano.config.floatX\n",
      "            )\n",
      "            \"\"\"\n",
      "            W_values = .05 * numpy.random.randn(n_in, n_out)\n",
      "            #W_values = .1 * numpy.random.randn(n_in, n_out)\n",
      "            \n",
      "            \"\"\"\n",
      "            if activation == theano.tensor.nnet.sigmoid:\n",
      "                W_values *= 4\n",
      "            \"\"\"\n",
      "\n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        if b is None:\n",
      "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
      "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "\n",
      "        lin_output = T.dot(input, self.W) + self.b\n",
      "        self.output = (\n",
      "            lin_output if activation is None\n",
      "            else activation(lin_output)\n",
      "        )\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "class MLP(object):\n",
      "\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: symbolic variable that describes the input of the\n",
      "        architecture (one minibatch)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: number of input units, the dimension of the space in\n",
      "        which the datapoints lie\n",
      "\n",
      "        :type n_hidden: int\n",
      "        :param n_hidden: number of hidden units\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "        which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        # Since we are dealing with a one hidden layer MLP, this will translate\n",
      "        # into a HiddenLayer with a tanh activation function connected to the\n",
      "        # LogisticRegression layer; the activation function can be replaced by\n",
      "        # sigmoid or any other nonlinear function\n",
      "        self.hiddenLayer1 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            activation=relu\n",
      "        )\n",
      "        \n",
      "        self.hiddenLayer2 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=self.hiddenLayer1.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out,\n",
      "            activation=relu\n",
      "        )\n",
      "        \n",
      "        self.y_pred = self.hiddenLayer2.output\n",
      "        \n",
      "        # the parameters of the model are the parameters of the two layers it is made out of\n",
      "        self.params = self.hiddenLayer1.params + self.hiddenLayer2.params\n",
      "    \n",
      "    def mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "    \n",
      "    def sym_mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean(((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "                      * ((self.y_pred[1] - y[0]) ** 2 + (self.y_pred[0] - y[1]) ** 2))\n",
      "        \n",
      "\n",
      "def shared_dataset(data_xy, borrow=True):\n",
      "        \"\"\" Function that loads the dataset into shared variables\n",
      "\n",
      "        The reason we store our dataset in shared variables is to allow\n",
      "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
      "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
      "        is needed (the default behaviour if the data is not in a shared\n",
      "        variable) would lead to a large decrease in performance.\n",
      "        \"\"\"\n",
      "        data_x, data_y, _ = data_xy\n",
      "        shared_x = theano.shared(numpy.asarray(data_x,\n",
      "                                               dtype='float32'),\n",
      "                                 borrow=borrow)\n",
      "        shared_y = theano.shared(numpy.asarray(data_y,\n",
      "                                               dtype='float32'),\n",
      "                                 borrow=borrow)\n",
      "        # When storing data on the GPU it has to be stored as floats\n",
      "        # therefore we will store the labels as ``floatX`` as well\n",
      "        # (``shared_y`` does exactly that). But during our computations\n",
      "        # we need them as ints (we use labels as index, and if they are\n",
      "        # floats it doesn't make sense) therefore instead of returning\n",
      "        # ``shared_y`` we will have to cast it to int. This little hack\n",
      "        # lets ous get around this issue\n",
      "        return shared_x, shared_y\n",
      "\n",
      "def train_nn(dataset, n_hidden=20, learning_rate=0.01, n_epochs=10, batch_size=20, test_data=None):\n",
      "    \"\"\"\n",
      "    Demonstrate stochastic gradient descent optimization for a multilayer\n",
      "    perceptron\n",
      "\n",
      "    This is demonstrated on MNIST.\n",
      "\n",
      "    :type learning_rate: float\n",
      "    :param learning_rate: learning rate used (factor for the stochastic\n",
      "    gradient\n",
      "\n",
      "    :type n_epochs: int\n",
      "    :param n_epochs: maximal number of epochs to run the optimizer\n",
      "\n",
      "    :type dataset: string\n",
      "    :param dataset: the path of the MNIST dataset file from\n",
      "                 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
      "\n",
      "\n",
      "   \"\"\"\n",
      "    train_set_x, train_set_y = shared_dataset(dataset)\n",
      "\n",
      "    # compute number of minibatches for training, validation and testing\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    \n",
      "    test_batch_size = 1\n",
      "    \n",
      "    ######################\n",
      "    # BUILD ACTUAL MODEL #\n",
      "    ######################\n",
      "    print '... building the model'\n",
      "\n",
      "    # allocate symbolic variables for the data\n",
      "    index = T.lscalar()  # index to a [mini]batch\n",
      "    x = T.fmatrix('x')   # input data from visual neurons\n",
      "    y = T.fmatrix('y')  # posterior\n",
      "\n",
      "    rng = numpy.random.RandomState(1234)\n",
      "\n",
      "    # construct the MLP class\n",
      "    nn = MLP(\n",
      "        rng=rng,\n",
      "        input=x,\n",
      "        n_in=121,\n",
      "        n_hidden=n_hidden,\n",
      "        n_out=2\n",
      "    )\n",
      "\n",
      "    # the cost we minimize during training is the mean squared error; cost is expressed\n",
      "    # here symbolically\n",
      "    cost = nn.mse(y)\n",
      "\n",
      "    # compute the gradient of cost with respect to theta (sotred in params)\n",
      "    # the resulting gradients will be stored in a list gparams\n",
      "    gparams = [T.grad(cost, param) for param in nn.params]\n",
      "\n",
      "    # specify how to update the parameters of the model as a list of\n",
      "    # (variable, update expression) pairs\n",
      "\n",
      "    # given two list the zip A = [a1, a2, a3, a4] and B = [b1, b2, b3, b4] of\n",
      "    # same length, zip generates a list C of same size, where each element\n",
      "    # is a pair formed from the two lists :\n",
      "    #    C = [(a1, b1), (a2, b2), (a3, b3), (a4, b4)]\n",
      "    updates = [\n",
      "        (param, param - learning_rate * gparam)\n",
      "        for param, gparam in zip(nn.params, gparams)\n",
      "    ]\n",
      "\n",
      "    # compiling a Theano function `train_model` that returns the cost, but\n",
      "    # in the same time updates the parameter of the model based on the rules\n",
      "    # defined in `updates`\n",
      "    train_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=cost,\n",
      "        updates=updates,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "\n",
      "    ###############\n",
      "    # TRAIN MODEL #\n",
      "    ###############\n",
      "    print '... training'\n",
      "\n",
      "    start_time = time.clock()\n",
      "\n",
      "    epoch = 0\n",
      "    done_looping = False\n",
      "\n",
      "    while (epoch < n_epochs) and (not done_looping):\n",
      "        epoch = epoch + 1\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "            \n",
      "            minibatch_avg_cost = train_model(minibatch_index)\n",
      "            \n",
      "            #print \"epoch \" + repr(epoch)\n",
      "            #print \"minibatch \" + repr(minibatch_index + 1) + \"/\" + repr(n_train_batches)\n",
      "            #print \"average cost \" + repr(minibatch_avg_cost)\n",
      "\n",
      "    end_time = time.clock()\n",
      "\n",
      "    print 'testing'\n",
      "    test_batch_size = 1\n",
      "    test_set_x, test_set_y = shared_dataset(test_data)\n",
      "    test_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=nn.y_pred,\n",
      "        givens={\n",
      "            x: test_set_x[index * test_batch_size: (index + 1) * test_batch_size]\n",
      "        },\n",
      "    )\n",
      "    \n",
      "    true_ys = test_set_y.get_value()\n",
      "    pred_ys = numpy.zeros(len(true_ys))\n",
      "    for i in range(len(true_ys)):\n",
      "        #pred_ys[i] = test_model(i)\n",
      "        print test_model(i), true_ys[i]\n",
      "    \n",
      "    return pred_ys, true_ys\n",
      "\n",
      "train_data = generate_popcode_data(ndata, nneuron, kappa_tc, c_50, r_max, \"poisson\", True)\n",
      "test_data = generate_popcode_data(ndata, nneuron, kappa_tc, c_50, r_max, \"poisson\", True)\n",
      "nn, _ = train_nn(train_data, n_hidden=10, learning_rate=.01, n_epochs=100, test_data=test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}