{
 "metadata": {
  "name": "",
  "signature": "sha256:0cb23f3eb05514530db46c81d832fffd035b4aa00bbb3aedcb13b2f3848338f1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy as sp\n",
      "import math\n",
      "import random\n",
      "from scipy.stats import norm, poisson, entropy\n",
      "\n",
      "nneuron = 41\n",
      "\n",
      "sprefs = np.linspace(-60, 60, nneuron)\n",
      "\n",
      "sig1_sq      = 3**2\n",
      "sig2_sq      = 12**2\n",
      "sigtc_sq     = 10**2\n",
      "gains        = [1, 3, 15]\n",
      "ndatapergain = 3000\n",
      "mu = 0\n",
      "\n",
      "def generate_popcode_noisy_data(ndatapergain, nneuron, sig1_sq, sig2_sq, sigtc_sq, gains):\n",
      "    s1_s1 = np.random.normal(mu, math.sqrt(sig1_sq), ndatapergain/2)\n",
      "    s1_s2 = np.random.normal(mu, math.sqrt(sig2_sq), ndatapergain/2)\n",
      "    s1 = np.concatenate((s1_s1, s1_s2), axis=1)\n",
      "    sprefs_data = np.tile(sprefs, (ndatapergain, 1))\n",
      "    r1 = np.exp(-np.square((np.transpose(np.tile(s1, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    ar1 = np.sum(r1, axis=1)/sigtc_sq\n",
      "    br1 = np.sum(r1*sprefs_data, axis=1)/sigtc_sq\n",
      "    \n",
      "    term_1 = (1 + sig2_sq * ar1) / (1 + sig1_sq  * ar1)\n",
      "    term_2_num = (sig2_sq - sig1_sq) * br1**2\n",
      "    term_2_denom = 2 * (1 + sig1_sq  * ar1) * (1 + sig2_sq * ar1)\n",
      "    xpnt_a = .5 * np.log(term_1) - term_2_num/term_2_denom\n",
      "    p1 = 1/(1 + np.exp(-xpnt_a))\n",
      "    return r1, p1\n",
      "\n",
      "R, P = generate_popcode_noisy_data(ndatapergain, nneuron, sig1_sq, sig2_sq, sigtc_sq, gains)\n",
      "print R\n",
      "print P"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[  7.40900991e-10   4.95504111e-09   3.02864133e-08 ...,   5.66053618e-06\n",
        "    1.23839026e-06   2.47611598e-07]\n",
        " [  7.68713257e-08   4.09232262e-07   1.99108066e-06 ...,   1.00869815e-07\n",
        "    1.75662714e-08   2.79583444e-09]\n",
        " [  8.17266961e-09   4.87475336e-08   2.65738726e-07 ...,   8.07091644e-07\n",
        "    1.57479645e-07   2.80827414e-08]\n",
        " ..., \n",
        " [  1.99055121e-05   7.68497453e-05   2.71159613e-04 ...,   1.30192635e-10\n",
        "    1.64425195e-11   1.89785842e-12]\n",
        " [  9.86339133e-10   6.50945186e-09   3.92623303e-08 ...,   4.54887828e-06\n",
        "    9.82053942e-07   1.93767023e-07]\n",
        " [  4.00912945e-11   3.05519772e-10   2.12785514e-09 ...,   4.37350257e-05\n",
        "    1.09026357e-05   2.48397436e-06]]\n",
        "[ 0.62696013  0.69970751  0.72740228 ...,  0.06051833  0.64649615\n",
        "  0.32261149]\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "This tutorial introduces the multilayer perceptron using Theano.\n",
      "\n",
      " A multilayer perceptron is a logistic regressor where\n",
      "instead of feeding the input to the logistic regression you insert a\n",
      "intermediate layer, called the hidden layer, that has a nonlinear\n",
      "activation function (usually tanh or sigmoid) . One can use many such\n",
      "hidden layers making the architecture deep. The tutorial will also tackle\n",
      "the problem of MNIST digit classification.\n",
      "\n",
      ".. math::\n",
      "\n",
      "    f(x) = G( b^{(2)} + W^{(2)}( s( b^{(1)} + W^{(1)} x))),\n",
      "\n",
      "References:\n",
      "\n",
      "    - textbooks: \"Pattern Recognition and Machine Learning\" -\n",
      "                 Christopher M. Bishop, section 5\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import numpy\n",
      "\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "def relu(x):\n",
      "    return theano.tensor.switch(x<0, 0, x)\n",
      "\n",
      "class HiddenLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
      "                 activation=relu):\n",
      "        \"\"\"\n",
      "        Typical hidden layer of a MLP: units are fully-connected and have\n",
      "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
      "        and the bias vector b is of shape (n_out,).\n",
      "\n",
      "        NOTE : The nonlinearity used here is tanh\n",
      "\n",
      "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.dmatrix\n",
      "        :param input: a symbolic tensor of shape (n_examples, n_in)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: dimensionality of input\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of hidden units\n",
      "\n",
      "        :type activation: theano.Op or function\n",
      "        :param activation: Non linearity to be applied in the hidden\n",
      "                           layer\n",
      "        \"\"\"\n",
      "        self.input = input\n",
      "        # end-snippet-1\n",
      "\n",
      "        # `W` is initialized with `W_values` which is uniformely sampled\n",
      "        # from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))\n",
      "        # for tanh activation function\n",
      "        # the output of uniform if converted using asarray to dtype\n",
      "        # theano.config.floatX so that the code is runable on GPU\n",
      "        # Note : optimal initialization of weights is dependent on the\n",
      "        #        activation function used (among other things).\n",
      "        #        For example, results presented in [Xavier10] suggest that you\n",
      "        #        should use 4 times larger initial weights for sigmoid\n",
      "        #        compared to tanh\n",
      "        #        We have no info for other function, so we use the same as\n",
      "        #        tanh.\n",
      "        if W is None:\n",
      "            W_values = numpy.asarray(\n",
      "                rng.uniform(\n",
      "                    low=-numpy.sqrt(6. / (n_in + n_out)),\n",
      "                    high=numpy.sqrt(6. / (n_in + n_out)),\n",
      "                    size=(n_in, n_out)\n",
      "                ),\n",
      "                dtype=theano.config.floatX\n",
      "            )\n",
      "            if activation == theano.tensor.nnet.sigmoid:\n",
      "                W_values *= 4\n",
      "\n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        if b is None:\n",
      "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
      "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "\n",
      "        lin_output = T.dot(input, self.W) + self.b\n",
      "        self.output = (\n",
      "            lin_output if activation is None\n",
      "            else activation(lin_output)\n",
      "        )\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "class MLP(object):\n",
      "\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: symbolic variable that describes the input of the\n",
      "        architecture (one minibatch)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: number of input units, the dimension of the space in\n",
      "        which the datapoints lie\n",
      "\n",
      "        :type n_hidden: int\n",
      "        :param n_hidden: number of hidden units\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "        which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        # Since we are dealing with a one hidden layer MLP, this will translate\n",
      "        # into a HiddenLayer with a tanh activation function connected to the\n",
      "        # LogisticRegression layer; the activation function can be replaced by\n",
      "        # sigmoid or any other nonlinear function\n",
      "        self.hiddenLayer1 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            activation=relu\n",
      "        )\n",
      "        \n",
      "        self.hiddenLayer2 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=self.hiddenLayer1.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out,\n",
      "            activation=relu\n",
      "        )\n",
      "        \n",
      "        self.y_pred = self.hiddenLayer2.output\n",
      "        \n",
      "        # the parameters of the model are the parameters of the two layers it is made out of\n",
      "        self.params = self.hiddenLayer1.params + self.hiddenLayer2.params\n",
      "        \n",
      "    def mse(self, y):\n",
      "        # error between output and target\n",
      "        print self.y_pred.shape\n",
      "        print y\n",
      "        return T.mean((self.y_pred.T - y) ** 2)\n",
      "\n",
      "\n",
      "def test_mlp(dataset, n_hidden=2, learning_rate=0.01, n_epochs=10, batch_size=20):\n",
      "    \"\"\"\n",
      "    Demonstrate stochastic gradient descent optimization for a multilayer\n",
      "    perceptron\n",
      "\n",
      "    This is demonstrated on MNIST.\n",
      "\n",
      "    :type learning_rate: float\n",
      "    :param learning_rate: learning rate used (factor for the stochastic\n",
      "    gradient\n",
      "\n",
      "    :type n_epochs: int\n",
      "    :param n_epochs: maximal number of epochs to run the optimizer\n",
      "\n",
      "    :type dataset: string\n",
      "    :param dataset: the path of the MNIST dataset file from\n",
      "                 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
      "\n",
      "\n",
      "   \"\"\"\n",
      "    def shared_dataset(data_xy, borrow=True):\n",
      "        \"\"\" Function that loads the dataset into shared variables\n",
      "\n",
      "        The reason we store our dataset in shared variables is to allow\n",
      "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
      "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
      "        is needed (the default behaviour if the data is not in a shared\n",
      "        variable) would lead to a large decrease in performance.\n",
      "        \"\"\"\n",
      "        data_x, data_y = data_xy\n",
      "        shared_x = theano.shared(numpy.asarray(data_x,\n",
      "                                               dtype='int32'),\n",
      "                                 borrow=borrow)\n",
      "        shared_y = theano.shared(numpy.asarray(data_y,\n",
      "                                               dtype='int32'),\n",
      "                                 borrow=borrow)\n",
      "        # When storing data on the GPU it has to be stored as floats\n",
      "        # therefore we will store the labels as ``floatX`` as well\n",
      "        # (``shared_y`` does exactly that). But during our computations\n",
      "        # we need them as ints (we use labels as index, and if they are\n",
      "        # floats it doesn't make sense) therefore instead of returning\n",
      "        # ``shared_y`` we will have to cast it to int. This little hack\n",
      "        # lets ous get around this issue\n",
      "        #return shared_x, T.cast(shared_y, 'int32')\n",
      "        return shared_x, shared_y\n",
      "    \n",
      "    train_set_x, train_set_y = shared_dataset(dataset)\n",
      "    print train_set_x.get_value\n",
      "    print train_set_y.get_value\n",
      "    test_set_x, test_set_y = shared_dataset(dataset)\n",
      "\n",
      "    # compute number of minibatches for training, validation and testing\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "\n",
      "    ######################\n",
      "    # BUILD ACTUAL MODEL #\n",
      "    ######################\n",
      "    print '... building the model'\n",
      "\n",
      "    # allocate symbolic variables for the data\n",
      "    index = T.lscalar()  # index to a [mini]batch\n",
      "    x = T.imatrix('x')   # the data is presented as rasterized images\n",
      "    y = T.ivector('y')  # the labels are presented as 1D vector of\n",
      "                        # [int] labels\n",
      "\n",
      "    rng = numpy.random.RandomState(1234)\n",
      "\n",
      "    # construct the MLP class\n",
      "    classifier = MLP(\n",
      "        rng=rng,\n",
      "        input=x,\n",
      "        n_in=41,\n",
      "        n_hidden=n_hidden,\n",
      "        n_out=1\n",
      "    )\n",
      "\n",
      "    # the cost we minimize during training is the mean squared error; cost is expressed\n",
      "    # here symbolically\n",
      "    cost = classifier.mse(y)\n",
      "\n",
      "    # compute the gradient of cost with respect to theta (sotred in params)\n",
      "    # the resulting gradients will be stored in a list gparams\n",
      "    gparams = [T.grad(cost, param) for param in classifier.params]\n",
      "\n",
      "    # specify how to update the parameters of the model as a list of\n",
      "    # (variable, update expression) pairs\n",
      "\n",
      "    # given two list the zip A = [a1, a2, a3, a4] and B = [b1, b2, b3, b4] of\n",
      "    # same length, zip generates a list C of same size, where each element\n",
      "    # is a pair formed from the two lists :\n",
      "    #    C = [(a1, b1), (a2, b2), (a3, b3), (a4, b4)]\n",
      "    updates = [\n",
      "        (param, param - learning_rate * gparam)\n",
      "        for param, gparam in zip(classifier.params, gparams)\n",
      "    ]\n",
      "    \n",
      "    print train_set_x[1:20]\n",
      "\n",
      "    # compiling a Theano function `train_model` that returns the cost, but\n",
      "    # in the same time updates the parameter of the model based on the rules\n",
      "    # defined in `updates`\n",
      "    train_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=cost,\n",
      "        updates=updates,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "\n",
      "    ###############\n",
      "    # TRAIN MODEL #\n",
      "    ###############\n",
      "    print '... training'\n",
      "\n",
      "    start_time = time.clock()\n",
      "\n",
      "    epoch = 0\n",
      "    done_looping = False\n",
      "\n",
      "    while (epoch < n_epochs) and (not done_looping):\n",
      "        epoch = epoch + 1\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "            \n",
      "            minibatch_avg_cost = train_model(minibatch_index)\n",
      "            \n",
      "            print minibatch_avg_cost.real\n",
      "            print(\n",
      "                'epoch %i, minibatch %i/%i%, avg cost %i'\n",
      "                (\n",
      "                    epoch,\n",
      "                    minibatch_index + 1,\n",
      "                    n_train_batches,\n",
      "                    minibatch_avg_cost\n",
      "                )\n",
      "            )\n",
      "\n",
      "    end_time = time.clock()\n",
      "    print >> sys.stderr, ('The code ran for %.2fm' % ((end_time - start_time) / 60.))\n",
      "\n",
      "test_mlp(generate_popcode_noisy_data(ndatapergain, nneuron, sig1_sq, sig2_sq, sigtc_sq, gains))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<bound method TensorSharedVariable.get_value of <TensorType(int32, matrix)>>\n",
        "<bound method TensorSharedVariable.get_value of <TensorType(int32, vector)>>\n",
        "... building the model\n",
        "Shape.0\n",
        "y\n",
        "Subtensor{int64:int64:}.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "... training"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.0\n"
       ]
      },
      {
       "ename": "TypeError",
       "evalue": "'str' object is not callable",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-20-af903fe9168a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'The code ran for %.2fm'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m60.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m \u001b[0mtest_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_popcode_noisy_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndatapergain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnneuron\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig1_sq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig2_sq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigtc_sq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-20-af903fe9168a>\u001b[0m in \u001b[0;36mtest_mlp\u001b[0;34m(dataset, n_hidden, learning_rate, n_epochs, batch_size)\u001b[0m\n\u001b[1;32m    297\u001b[0m                     \u001b[0mminibatch_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                     \u001b[0mn_train_batches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mminibatch_avg_cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 )\n\u001b[1;32m    301\u001b[0m             )\n",
        "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}