{
 "metadata": {
  "name": "",
  "signature": "sha256:a0a1aac30bcfacb9054fa18dea315e99d1fef5781a2584280c0429fa97ec0dca"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy as sp\n",
      "import math\n",
      "import random\n",
      "from scipy.stats import norm, poisson, entropy\n",
      "\n",
      "nneuron = 41\n",
      "\n",
      "sprefs = np.linspace(-60, 60, nneuron)\n",
      "\n",
      "sig1_sq      = 3**2\n",
      "sig2_sq      = 12**2\n",
      "sigtc_sq     = 10**2\n",
      "gains        = [1, 3, 15]\n",
      "ndatapergain = 3000\n",
      "mu = 0\n",
      "\n",
      "def generate_popcode_noisy_data(ndatapergain, nneuron, sig1_sq, sig2_sq, sigtc_sq, gains):\n",
      "    s1_s1 = np.random.normal(mu, math.sqrt(sig1_sq), ndatapergain/2)\n",
      "    s1_s2 = np.random.normal(mu, math.sqrt(sig2_sq), ndatapergain/2)\n",
      "    s1 = np.concatenate((s1_s1, s1_s2), axis=1)\n",
      "    sprefs_data = np.tile(sprefs, (ndatapergain, 1))\n",
      "    r1 = np.exp(-np.square((np.transpose(np.tile(s1, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    ar1 = np.sum(r1, axis=1)/sigtc_sq\n",
      "    br1 = np.sum(r1*sprefs_data, axis=1)/sigtc_sq\n",
      "    \n",
      "    term_1 = (1 + sig2_sq * ar1) / (1 + sig1_sq  * ar1)\n",
      "    term_2_num = (sig2_sq - sig1_sq) * br1**2\n",
      "    term_2_denom = 2 * (1 + sig1_sq  * ar1) * (1 + sig2_sq * ar1)\n",
      "    xpnt_a = .5 * np.log(term_1) - term_2_num/term_2_denom\n",
      "    p1 = 1/(1 + np.exp(-xpnt_a))\n",
      "    return r1, p1\n",
      "\n",
      "R, P = generate_popcode_noisy_data(ndatapergain, nneuron, sig1_sq, sig2_sq, sigtc_sq, gains)\n",
      "print R\n",
      "print P"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[  2.15767637e-08   1.22622966e-07   6.36899374e-07 ...,   3.39174997e-07\n",
        "    6.30553894e-08   1.07135670e-08]\n",
        " [  2.30598127e-08   1.30611754e-07   6.76117610e-07 ...,   3.19043910e-07\n",
        "    5.91139280e-08   1.00101982e-08]\n",
        " [  1.32957756e-06   6.05252494e-06   2.51810012e-05 ...,   4.55378370e-09\n",
        "    6.78122541e-10   9.22905979e-11]\n",
        " ..., \n",
        " [  1.11021470e-10   8.09112854e-10   5.38920524e-09 ...,   2.22062845e-05\n",
        "    5.29408485e-06   1.15350476e-06]\n",
        " [  4.47675818e-12   3.74392481e-11   2.86156818e-10 ...,   1.67095251e-04\n",
        "    4.57130777e-05   1.14295811e-05]\n",
        " [  4.98803756e-20   8.06788548e-19   1.19262314e-17 ...,   1.43106129e-01\n",
        "    7.57183454e-02   3.66148698e-02]]\n",
        "[  7.30329660e-01   7.29747126e-01   4.22902241e-01 ...,   4.44557095e-01\n",
        "   1.07530233e-01   9.58588629e-11]\n"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "This tutorial introduces the multilayer perceptron using Theano.\n",
      "\n",
      " A multilayer perceptron is a logistic regressor where\n",
      "instead of feeding the input to the logistic regression you insert a\n",
      "intermediate layer, called the hidden layer, that has a nonlinear\n",
      "activation function (usually tanh or sigmoid) . One can use many such\n",
      "hidden layers making the architecture deep. The tutorial will also tackle\n",
      "the problem of MNIST digit classification.\n",
      "\n",
      ".. math::\n",
      "\n",
      "    f(x) = G( b^{(2)} + W^{(2)}( s( b^{(1)} + W^{(1)} x))),\n",
      "\n",
      "References:\n",
      "\n",
      "    - textbooks: \"Pattern Recognition and Machine Learning\" -\n",
      "                 Christopher M. Bishop, section 5\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import numpy\n",
      "\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "def relu(x):\n",
      "    return theano.tensor.switch(x<0, 0, x)\n",
      "\n",
      "class HiddenLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
      "                 activation=relu):\n",
      "        \"\"\"\n",
      "        Typical hidden layer of a MLP: units are fully-connected and have\n",
      "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
      "        and the bias vector b is of shape (n_out,).\n",
      "\n",
      "        NOTE : The nonlinearity used here is tanh\n",
      "\n",
      "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.dmatrix\n",
      "        :param input: a symbolic tensor of shape (n_examples, n_in)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: dimensionality of input\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of hidden units\n",
      "\n",
      "        :type activation: theano.Op or function\n",
      "        :param activation: Non linearity to be applied in the hidden\n",
      "                           layer\n",
      "        \"\"\"\n",
      "        self.input = input\n",
      "        # end-snippet-1\n",
      "\n",
      "        # `W` is initialized with `W_values` which is uniformely sampled\n",
      "        # from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))\n",
      "        # for tanh activation function\n",
      "        # the output of uniform if converted using asarray to dtype\n",
      "        # theano.config.floatX so that the code is runable on GPU\n",
      "        # Note : optimal initialization of weights is dependent on the\n",
      "        #        activation function used (among other things).\n",
      "        #        For example, results presented in [Xavier10] suggest that you\n",
      "        #        should use 4 times larger initial weights for sigmoid\n",
      "        #        compared to tanh\n",
      "        #        We have no info for other function, so we use the same as\n",
      "        #        tanh.\n",
      "        if W is None:\n",
      "            W_values = numpy.asarray(\n",
      "                rng.uniform(\n",
      "                    low=-numpy.sqrt(6. / (n_in + n_out)),\n",
      "                    high=numpy.sqrt(6. / (n_in + n_out)),\n",
      "                    size=(n_in, n_out)\n",
      "                ),\n",
      "                dtype=theano.config.floatX\n",
      "            )\n",
      "            if activation == theano.tensor.nnet.sigmoid:\n",
      "                W_values *= 4\n",
      "\n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        if b is None:\n",
      "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
      "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "\n",
      "        lin_output = T.dot(input, self.W) + self.b\n",
      "        self.output = (\n",
      "            lin_output if activation is None\n",
      "            else activation(lin_output)\n",
      "        )\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "\n",
      "# start-snippet-2\n",
      "class MLP(object):\n",
      "    \"\"\"Multi-Layer Perceptron Class\n",
      "\n",
      "    A multilayer perceptron is a feedforward artificial neural network model\n",
      "    that has one layer or more of hidden units and nonlinear activations.\n",
      "    Intermediate layers usually have as activation function tanh or the\n",
      "    sigmoid function (defined here by a ``HiddenLayer`` class)  while the\n",
      "    top layer is a softmax layer (defined here by a ``LogisticRegression``\n",
      "    class).\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: symbolic variable that describes the input of the\n",
      "        architecture (one minibatch)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: number of input units, the dimension of the space in\n",
      "        which the datapoints lie\n",
      "\n",
      "        :type n_hidden: int\n",
      "        :param n_hidden: number of hidden units\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "        which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        # Since we are dealing with a one hidden layer MLP, this will translate\n",
      "        # into a HiddenLayer with a tanh activation function connected to the\n",
      "        # LogisticRegression layer; the activation function can be replaced by\n",
      "        # sigmoid or any other nonlinear function\n",
      "        self.hiddenLayer1 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            activation=relu\n",
      "        )\n",
      "        \n",
      "        self.hiddenLayer2 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=self.hiddenLayer1.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out,\n",
      "            activation=relu\n",
      "        )\n",
      "        \n",
      "        self.y_pred = self.hiddenLayer2.output\n",
      "        \n",
      "        # the parameters of the model are the parameters of the two layer it is\n",
      "        # made out of\n",
      "        self.params = self.hiddenLayer1.params + self.hiddenLayer2.params\n",
      "        # end-snippet-3\n",
      "        \n",
      "    def mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean((self.y_pred - y) ** 2)\n",
      "\n",
      "\n",
      "def test_mlp(dataset, n_hidden=2, learning_rate=0.01, n_epochs=10, batch_size=20):\n",
      "    \"\"\"\n",
      "    Demonstrate stochastic gradient descent optimization for a multilayer\n",
      "    perceptron\n",
      "\n",
      "    This is demonstrated on MNIST.\n",
      "\n",
      "    :type learning_rate: float\n",
      "    :param learning_rate: learning rate used (factor for the stochastic\n",
      "    gradient\n",
      "\n",
      "    :type n_epochs: int\n",
      "    :param n_epochs: maximal number of epochs to run the optimizer\n",
      "\n",
      "    :type dataset: string\n",
      "    :param dataset: the path of the MNIST dataset file from\n",
      "                 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
      "\n",
      "\n",
      "   \"\"\"\n",
      "    def shared_dataset(data_xy, borrow=True):\n",
      "        \"\"\" Function that loads the dataset into shared variables\n",
      "\n",
      "        The reason we store our dataset in shared variables is to allow\n",
      "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
      "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
      "        is needed (the default behaviour if the data is not in a shared\n",
      "        variable) would lead to a large decrease in performance.\n",
      "        \"\"\"\n",
      "        data_x, data_y = data_xy\n",
      "        shared_x = theano.shared(numpy.asarray(data_x,\n",
      "                                               dtype=theano.config.floatX),\n",
      "                                 borrow=borrow)\n",
      "        shared_y = theano.shared(numpy.asarray(data_y,\n",
      "                                               dtype=theano.config.floatX),\n",
      "                                 borrow=borrow)\n",
      "        # When storing data on the GPU it has to be stored as floats\n",
      "        # therefore we will store the labels as ``floatX`` as well\n",
      "        # (``shared_y`` does exactly that). But during our computations\n",
      "        # we need them as ints (we use labels as index, and if they are\n",
      "        # floats it doesn't make sense) therefore instead of returning\n",
      "        # ``shared_y`` we will have to cast it to int. This little hack\n",
      "        # lets ous get around this issue\n",
      "        return shared_x, T.cast(shared_y, 'int32')\n",
      "    \n",
      "    train_set_x, train_set_y = dataset\n",
      "    print train_set_x[1]\n",
      "    print train_set_y[1]\n",
      "    test_set_x, test_set_y = dataset\n",
      "\n",
      "    # compute number of minibatches for training, validation and testing\n",
      "    n_train_batches = train_set_x.size / batch_size\n",
      "    n_test_batches = test_set_x.size / batch_size\n",
      "\n",
      "    ######################\n",
      "    # BUILD ACTUAL MODEL #\n",
      "    ######################\n",
      "    print '... building the model'\n",
      "\n",
      "    # allocate symbolic variables for the data\n",
      "    index = T.lscalar()  # index to a [mini]batch\n",
      "    x = T.ivector('x')  # the data is presented as rasterized images\n",
      "    y = T.ivector('y')  # the labels are presented as 1D vector of\n",
      "                        # [int] labels\n",
      "\n",
      "    rng = numpy.random.RandomState(1234)\n",
      "\n",
      "    # construct the MLP class\n",
      "    classifier = MLP(\n",
      "        rng=rng,\n",
      "        input=x,\n",
      "        n_in=41,\n",
      "        n_hidden=n_hidden,\n",
      "        n_out=1\n",
      "    )\n",
      "\n",
      "    # the cost we minimize during training is the mean squared error; cost is expressed\n",
      "    # here symbolically\n",
      "    cost = classifier.mse(y)\n",
      "\n",
      "    \"\"\"\n",
      "    # compiling a Theano function that computes the mistakes that are made\n",
      "    # by the model on a minibatch\n",
      "    test_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=classifier.mse(y),\n",
      "        givens={\n",
      "            x: test_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "            y: test_set_y[index * batch_size:(index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "    \"\"\"\n",
      "\n",
      "    # start-snippet-5\n",
      "    # compute the gradient of cost with respect to theta (sotred in params)\n",
      "    # the resulting gradients will be stored in a list gparams\n",
      "    gparams = [T.grad(cost, param) for param in classifier.params]\n",
      "\n",
      "    # specify how to update the parameters of the model as a list of\n",
      "    # (variable, update expression) pairs\n",
      "\n",
      "    # given two list the zip A = [a1, a2, a3, a4] and B = [b1, b2, b3, b4] of\n",
      "    # same length, zip generates a list C of same size, where each element\n",
      "    # is a pair formed from the two lists :\n",
      "    #    C = [(a1, b1), (a2, b2), (a3, b3), (a4, b4)]\n",
      "    updates = [\n",
      "        (param, param - learning_rate * gparam)\n",
      "        for param, gparam in zip(classifier.params, gparams)\n",
      "    ]\n",
      "\n",
      "    # compiling a Theano function `train_model` that returns the cost, but\n",
      "    # in the same time updates the parameter of the model based on the rules\n",
      "    # defined in `updates`\n",
      "    train_model = theano.function(inputs=[train_set_x,train_set_y], outputs=cost,\n",
      "            updates=updates)\n",
      "    # end-snippet-5\n",
      "\n",
      "    ###############\n",
      "    # TRAIN MODEL #\n",
      "    ###############\n",
      "    print '... training'\n",
      "\n",
      "    # early-stopping parameters\n",
      "    \"\"\"\n",
      "    patience = 10000  # look as this many examples regardless\n",
      "    patience_increase = 2  # wait this much longer when a new best is\n",
      "                           # found\n",
      "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
      "                                   # considered significant\n",
      "    validation_frequency = min(n_train_batches, patience / 2)\n",
      "                                  # go through this many\n",
      "                                  # minibatche before checking the network\n",
      "                                  # on the validation set; in this case we\n",
      "                                  # check every epoch\n",
      "\n",
      "    best_validation_loss = numpy.inf\n",
      "    best_iter = 0\n",
      "    test_score = 0.\n",
      "    \"\"\"\n",
      "    start_time = time.clock()\n",
      "\n",
      "    epoch = 0\n",
      "    done_looping = False\n",
      "\n",
      "    while (epoch < n_epochs) and (not done_looping):\n",
      "        epoch = epoch + 1\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "\n",
      "            minibatch_avg_cost = train_model(train_set_x[minibatch_index * batch_size:(minibatch_index + 1) * batch_size,:],\n",
      "                                       train_set_y[minibatch_index * batch_size:(minibatch_index + 1) * batch_size])\n",
      "\n",
      "            print(\n",
      "                'epoch %i, minibatch %i/%i%, avg cost %i'\n",
      "                (\n",
      "                    epoch,\n",
      "                    minibatch_index + 1,\n",
      "                    n_train_batches,\n",
      "                    minibatch_avg_cost\n",
      "                )\n",
      "            )\n",
      "\n",
      "\n",
      "    end_time = time.clock()\n",
      "    print >> sys.stderr, ('The code ran for %.2fm' % ((end_time - start_time) / 60.))\n",
      "\n",
      "test_mlp(generate_popcode_noisy_data(ndatapergain, nneuron, sig1_sq, sig2_sq, sigtc_sq, gains))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  1.04922310e-08   6.18165528e-08   3.32855103e-07   1.63801983e-06\n",
        "   7.36710033e-06   3.02822061e-05   1.13760616e-04   3.90579869e-04\n",
        "   1.22557889e-03   3.51468320e-03   9.21180202e-03   2.20656387e-02\n",
        "   4.83060922e-02   9.66497658e-02   1.76731190e-01   2.95351442e-01\n",
        "   4.51105878e-01   6.29696635e-01   8.03336973e-01   9.36650642e-01\n",
        "   9.98093005e-01   9.72026007e-01   8.65163631e-01   7.03772221e-01\n",
        "   5.23214154e-01   3.55500604e-01   2.20757110e-01   1.25286002e-01\n",
        "   6.49836202e-02   3.08048251e-02   1.33458762e-02   5.28431782e-03\n",
        "   1.91224822e-03   6.32430926e-04   1.91159298e-04   5.28069706e-05\n",
        "   1.33321618e-05   3.07626253e-06   6.48723620e-07   1.25028645e-07\n",
        "   2.20228151e-08]\n",
        "0.730161409092\n",
        "... building the model\n"
       ]
      },
      {
       "ename": "TypeError",
       "evalue": "Unknown parameter type: <type 'numpy.ndarray'>",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-91-fee518f53609>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'The code ran for %.2fm'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m60.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m \u001b[0mtest_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_popcode_noisy_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndatapergain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnneuron\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig1_sq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig2_sq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigtc_sq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-91-fee518f53609>\u001b[0m in \u001b[0;36mtest_mlp\u001b[0;34m(dataset, n_hidden, learning_rate, n_epochs, batch_size)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;31m# defined in `updates`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     train_model = theano.function(inputs=[train_set_x,train_set_y], outputs=cost,\n\u001b[0;32m--> 288\u001b[0;31m             updates=updates)\n\u001b[0m\u001b[1;32m    289\u001b[0m     \u001b[0;31m# end-snippet-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/theano/compile/function.pyc\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0mallow_input_downcast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_input_downcast\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                 profile=profile)\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;31m# borrowed used defined inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mpfunc\u001b[0;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# transform params into theano.compile.In objects.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     inputs = [_pfunc_param_to_in(p, allow_downcast=allow_input_downcast)\n\u001b[0;32m--> 449\u001b[0;31m               for p in params]\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Check if some variable is present more than once in inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36m_pfunc_param_to_in\u001b[0;34m(param, strict, allow_downcast)\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0mallow_downcast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_downcast\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 implicit=param.implicit)\n\u001b[0;32m--> 529\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown parameter type: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: Unknown parameter type: <type 'numpy.ndarray'>"
       ]
      }
     ],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}