{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Describe training. Different ways of training, generalization across gains. Bias and variance as a function of Delta s and gains (think abpout how to plot smartly, to avoid having too many plots)\n",
    "    \n",
    "Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson\n",
    "import matplotlib.patches as mpatches\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "\n",
    "nneuron = 61\n",
    "min_angle = -90\n",
    "max_angle = 90\n",
    "sprefs = np.linspace(min_angle, max_angle, nneuron)\n",
    "eps = np.finfo(np.float64).eps\n",
    "sigtc_sq = float(10**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cartesian(arrays, out=None):\n",
    "    \"\"\"Generate a cartesian product of input arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    arrays : list of array-like\n",
    "        1-D arrays to form the cartesian product of.\n",
    "    out : ndarray\n",
    "        Array to place the cartesian product in.\n",
    "    Returns\n",
    "    -------\n",
    "    out : ndarray\n",
    "        2-D array of shape (M, len(arrays)) containing cartesian products\n",
    "        formed of input arrays.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> cartesian(([1, 2, 3], [4, 5], [6, 7]))\n",
    "    array([[1, 4, 6],\n",
    "           [1, 4, 7],\n",
    "           [1, 5, 6],\n",
    "           [1, 5, 7],\n",
    "           [2, 4, 6],\n",
    "           [2, 4, 7],\n",
    "           [2, 5, 6],\n",
    "           [2, 5, 7],\n",
    "           [3, 4, 6],\n",
    "           [3, 4, 7],\n",
    "           [3, 5, 6],\n",
    "           [3, 5, 7]])\n",
    "    \"\"\"\n",
    "    arrays = [np.asarray(x) for x in arrays]\n",
    "    shape = (len(x) for x in arrays)\n",
    "    dtype = arrays[0].dtype\n",
    "\n",
    "    ix = np.indices(shape)\n",
    "    ix = ix.reshape(len(arrays), -1).T\n",
    "\n",
    "    if out is None:\n",
    "        out = np.empty_like(ix, dtype=dtype)\n",
    "\n",
    "    for n, arr in enumerate(arrays):\n",
    "        out[:, n] = arrays[n][ix[:, n]]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_s(ndata, sort):\n",
    "    s = np.random.rand(2, ndata) * 120 - 60\n",
    "    if sort:\n",
    "        s = np.sort(s, axis=0)\n",
    "    return s[0], s[1]\n",
    "\n",
    "def random_c(ndata, ndims, low, high, sort):\n",
    "    c_range = high - low\n",
    "    if ndims == 1:\n",
    "        c = np.random.rand(ndims, ndata)[0] * c_range + low\n",
    "    else:\n",
    "        c = np.random.rand(ndims, ndata) * c_range + low\n",
    "    if sort:\n",
    "        c = np.sort(c, axis=0)\n",
    "    return c\n",
    "    \n",
    "def generate_popcode_data(ndata, nneuron, sigtc_sq, r_max, noise, sort, s_0, s_1, c_0, c_1, c_50=13.1):\n",
    "    c_rms = np.sqrt(np.square(c_0) + np.square(c_1))\n",
    "    sprefs_data = np.tile(sprefs, (ndata, 1))\n",
    "    s_0t = np.exp(-np.square((np.transpose(np.tile(s_0, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
    "    stim_0 = c_0 * s_0t.T\n",
    "    s_1t = np.exp(-np.square((np.transpose(np.tile(s_1, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
    "    stim_1 = c_1 * s_1t.T\n",
    "    #r = r_max * (stim_0 + stim_1)/(c_50 + c_rms)\n",
    "    r = r_max * (stim_0 + stim_1)\n",
    "    r = r.T\n",
    "    s = np.array((s_0, s_1)).T\n",
    "    s = s/90\n",
    "    c = np.array((c_0, c_1)).T\n",
    "    if noise == \"poisson\":\n",
    "        r = np.random.poisson(r) + 0.0\n",
    "    return r, s, c\n",
    "\n",
    "def generate_trainset(ndata, highlow=False, discrete_c=None, low=.3, high=.7, r_max=10):\n",
    "    s_0, s_1 = random_s(ndata, True)\n",
    "    if highlow:\n",
    "        c_0, c_1 = np.concatenate((np.ones((2, ndata/2)) * low, np.ones((2, ndata/2)) * high), axis=1)\n",
    "    elif discrete_c:\n",
    "        cs = np.linspace(low, high, discrete_c)\n",
    "        perm_cs = cartesian((cs, cs)).T\n",
    "        c_0, c_1 = np.repeat(perm_cs, ndata/(discrete_c**2), axis=1)\n",
    "        print ndata/(discrete_c**2), \"trials per contrast level\"\n",
    "        if ndata%(discrete_c**2) != 0:\n",
    "            print \"Not divisible, only generated\", ndata / (discrete_c**2) * (discrete_c**2), \"trials\"\n",
    "        ndata = ndata / (discrete_c**2) * (discrete_c**2)\n",
    "    else:\n",
    "        c_0, c_1 = np.ones((2, ndata)) * .5\n",
    "    r, s, c = generate_popcode_data(ndata, nneuron, sigtc_sq, r_max, \"poisson\", True, s_0, s_1, c_0, c_1)\n",
    "    return r, s, c\n",
    "\n",
    "def generate_testset(ndata, stim_0=None, stim_1=None, con_0=None, con_1=None, discrete_c=None, low=.5, high=.5, r_max=10):\n",
    "    if con_0:\n",
    "        c_0 = np.ones(ndata) * con_0\n",
    "        c_1 = np.ones(ndata) * con_1\n",
    "    else:\n",
    "        c_range = high - low\n",
    "        if discrete_c:\n",
    "            cs = np.linspace(low, high, discrete_c)\n",
    "            perm_cs = cartesian((cs, cs)).T\n",
    "            c_0, c_1 = np.repeat(perm_cs, ndata/(discrete_c**2), axis=1)\n",
    "            print ndata/(discrete_c**2), \"trials per contrast level\"\n",
    "            if ndata%(discrete_c**2) != 0:\n",
    "                print \"Not divisible, only generated\", ndata / (discrete_c**2) * (discrete_c**2), \"trials\"\n",
    "            ndata = ndata / (discrete_c**2) * (discrete_c**2)\n",
    "        else:\n",
    "            c_0, c_1 = np.random.rand(2, ndata) * c_range + low\n",
    "    if not stim_0:\n",
    "        s_0, s_1 = random_s(ndata, True)\n",
    "    else:\n",
    "        s_0, s_1 = np.ones((2, ndata))\n",
    "        s_0 = s_0 * stim_0\n",
    "        s_1 = s_1 * stim_1\n",
    "    r, s, c = generate_popcode_data(ndata, nneuron, sigtc_sq, r_max, \"poisson\", True, s_0, s_1, c_0, c_1)\n",
    "    return r, s, c\n",
    "\n",
    "def generate_trainset_cat(ndata, low=.3, high=1.3, crange=.5, r_max=10):\n",
    "    numvec = np.random.binomial(1, .5, size=ndata).astype(int)\n",
    "    c_0 = random_c(ndata, 1, high, high+crange, True)\n",
    "    c_1 = random_c(ndata, 1, low, low+crange, True)\n",
    "    s_0, s_1 = np.random.rand(2, ndata) * 120 - 60\n",
    "    r, numvec, s, c  = generate_popcode_data_cat(ndata, numvec, nneuron, sigtc_sq, c_50, r_max, \"poisson\", s_0, s_1, c_0, c_1)\n",
    "    y = s[range(ndata), numvec]\n",
    "    return r, y, s, c, numvec \n",
    "    \n",
    "def generate_popcode_data_cat(ndata, numvec, nneuron, sigtc_sq, c_50, r_max, noise, s_0, s_1, c_0, c_1):\n",
    "    c0vec = c_0 * np.ones(ndata)\n",
    "    c1vec = c_1 * numvec\n",
    "    c_rms = np.sqrt(np.square(c0vec) + np.square(c1vec))\n",
    "    sprefs_data = np.tile(sprefs, (ndata, 1))\n",
    "    s_0t = np.exp(-np.square((np.transpose(np.tile(s_0, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
    "    stim_0 = c0vec * s_0t.T\n",
    "    s_1t = np.exp(-np.square((np.transpose(np.tile(s_1, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
    "    stim_1 = c1vec * s_1t.T\n",
    "    #r = r_max * (stim_0 + stim_1)/(c_50 + c_rms)\n",
    "    r = r_max * (stim_0 + stim_1)/(c_rms)\n",
    "    #r = r_max * (stim_0 + stim_1)\n",
    "    r = r.T\n",
    "    s = np.array((s_0, s_1)).T\n",
    "    s = s/90\n",
    "    c = np.array((c_0, c_1)).T\n",
    "    if noise == \"poisson\":\n",
    "        r = np.random.poisson(r) + 0.0\n",
    "    return r, numvec, s, c "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posterior computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lik_means(s_1, s_2, c_0=.5, c_1=.5, sprefs=sprefs, sigtc_sq=sigtc_sq, r_max=10):\n",
    "    sprefs_data = np.tile(sprefs, (len(s_1), 1))\n",
    "    s_0t = np.exp(-np.square((np.transpose(np.tile(s_1, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
    "    stim_0 = c_0 * s_0t.T\n",
    "    s_1t = np.exp(-np.square((np.transpose(np.tile(s_2, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
    "    stim_1 = c_1 * s_1t.T\n",
    "    r = r_max * (stim_0 + stim_1)\n",
    "    return r.T\n",
    "def posterior(r, means, s1_grid, s2_grid):\n",
    "    ns_liks = poisson.pmf(r, mu=means)\n",
    "    stim_liks = np.prod(ns_liks, axis=1)\n",
    "    #p_s = 2/14400\n",
    "    #logp_s = np.log(p_s)\n",
    "    logp_s = -3.8573325\n",
    "    loglik = np.sum(np.log(ns_liks), axis=1)\n",
    "    mean1 = np.sum(s1_grid * np.exp(loglik + logp_s)/np.sum(np.exp(loglik + logp_s)))\n",
    "    mean2 = np.sum(s2_grid * np.exp(loglik + logp_s)/np.sum(np.exp(loglik + logp_s)))\n",
    "    expsquare1 = np.sum(np.square(s1_grid) * np.exp(loglik + logp_s)/np.sum(np.exp(loglik + logp_s)))\n",
    "    expsquare2 = np.sum(np.square(s2_grid) * np.exp(loglik + logp_s)/np.sum(np.exp(loglik + logp_s)))\n",
    "    var1 = expsquare1 - np.square(mean1)\n",
    "    var2 = expsquare2 - np.square(mean2)\n",
    "    return mean1, mean2, var1, var2\n",
    "def posterior_setup(low=.3, high=.7, discrete_c = 3, num_s=100, r_max=10):\n",
    "    grid = np.linspace(-60, 60, num_s)\n",
    "    s1s = np.concatenate([[grid[i]]*(num_s-i) for i in range(num_s)])\n",
    "    cs = np.linspace(low, high, discrete_c)\n",
    "    s1_grid, c1_grid, c2_grid = cartesian((s1s, cs, cs)).T\n",
    "    s2s = np.concatenate([grid[i:num_s+1] for i in range(num_s)])\n",
    "    s2_grid = np.repeat(s2s, (discrete_c**2), axis=0)\n",
    "    means = lik_means(s1_grid, s2_grid, c_0=c1_grid, c_1=c2_grid, r_max=r_max)\n",
    "    partial_post = partial(posterior, means=means, s1_grid=s1_grid, s2_grid=s2_grid)\n",
    "    return partial_post\n",
    "def get_posteriors(r, post_func):\n",
    "    posteriors = {'mean_s1': None, 'mean_s2': None, 'var_s1': None, 'var_s2': None}\n",
    "    p = np.array([post_func(r[i]) for i in range(len(r))]).T\n",
    "    posteriors['mean_s1'], posteriors['mean_s2'], posteriors['var_s1'], posteriors['var_s2'] = p\n",
    "    return posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multilayer ReLU net\n",
    "\"\"\"\n",
    "\n",
    "def relu(x):\n",
    "    return theano.tensor.switch(x<0, 0, x)\n",
    "\n",
    "class HiddenLayer(object):\n",
    "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
    "                 activation=T.nnet.sigmoid):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        :type rng: np.random.RandomState\n",
    "        :param rng: a random number generator used to initialize weights\n",
    "\n",
    "        :type input: theano.tensor.dmatrix\n",
    "        :param input: a symbolic tensor of shape (n_examples, n_in)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: theano.Op or function\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        if W is None:\n",
    "            W_values = (1/np.sqrt(n_in)) * np.random.randn(n_in, n_out)\n",
    "            \n",
    "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "\n",
    "        if b is None:\n",
    "            b_values = np.zeros((n_out,), dtype=theano.config.floatX)\n",
    "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
    "\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "        lin_output = T.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if activation is None\n",
    "            else activation(lin_output)\n",
    "        )\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "class MLP(object):\n",
    "\n",
    "\n",
    "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
    "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
    "\n",
    "        :type rng: np.random.RandomState\n",
    "        :param rng: a random number generator used to initialize weights\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: symbolic variable that describes the input of the\n",
    "        architecture (one minibatch)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: number of input units, the dimension of the space in\n",
    "        which the datapoints lie\n",
    "\n",
    "        :type n_hidden: int\n",
    "        :param n_hidden: number of hidden units\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of output units, the dimension of the space in\n",
    "        which the labels lie\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.hiddenLayer1 = HiddenLayer(\n",
    "            rng=rng,\n",
    "            input=input,\n",
    "            n_in=n_in,\n",
    "            n_out=n_hidden,\n",
    "            #activation=T.nnet.sigmoid\n",
    "            activation=relu\n",
    "        )\n",
    "        \n",
    "        self.hiddenLayer2 = HiddenLayer(\n",
    "            rng=rng,\n",
    "            input=self.hiddenLayer1.output,\n",
    "            n_in=n_hidden,\n",
    "            n_out=n_out,\n",
    "            #activation=relu\n",
    "            activation=None\n",
    "        )\n",
    "        \n",
    "        self.y_pred = self.hiddenLayer2.output\n",
    "        \n",
    "        # the parameters of the model are the parameters of the two layers it is made out of\n",
    "        self.params = self.hiddenLayer1.params + self.hiddenLayer2.params\n",
    "    \n",
    "    def get_params(self):\n",
    "\n",
    "        params = {}\n",
    "        for param in self.params:\n",
    "            name = param.name\n",
    "            if name in params:\n",
    "                name = name, 2\n",
    "            params[name] = param.get_value()\n",
    "        return params\n",
    "    \n",
    "    def mse(self, y):\n",
    "        # error between output and target\n",
    "        if y.ndim == 1:\n",
    "            se = (self.y_pred.T - y)**2\n",
    "        else:\n",
    "            se = T.sum((self.y_pred - y)**2, axis=1)\n",
    "        return T.mean(se)\n",
    "        \n",
    "    \n",
    "    def valid_mse(self, y):\n",
    "        if y.ndim == 1:\n",
    "            se = (self.y_pred.T * 90 - y * 90)**2\n",
    "        else:\n",
    "            se = T.sum((self.y_pred * 90 - y * 90)**2, axis=1)\n",
    "        return T.mean(se)\n",
    "\n",
    "    \n",
    "class Perceptron(object):\n",
    "\n",
    "\n",
    "    def __init__(self, rng, input, n_in, n_out):\n",
    "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
    "\n",
    "        :type rng: np.random.RandomState\n",
    "        :param rng: a random number generator used to initialize weights\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: symbolic variable that describes the input of the\n",
    "        architecture (one minibatch)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: number of input units, the dimension of the space in\n",
    "        which the datapoints lie\n",
    "\n",
    "        :type n_hidden: int\n",
    "        :param n_hidden: number of hidden units\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of output units, the dimension of the space in\n",
    "        which the labels lie\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.layer = HiddenLayer(\n",
    "            rng=rng,\n",
    "            input=input,\n",
    "            n_in=n_in,\n",
    "            n_out=n_out,\n",
    "            #activation=T.nnet.sigmoid\n",
    "            activation=relu\n",
    "        )\n",
    "        \n",
    "        self.y_pred = self.layer.output\n",
    "        \n",
    "        # the parameters of the model are the parameters of the two layers it is made out of\n",
    "        self.params = self.layer.params\n",
    "        \n",
    "    def get_params(self):\n",
    "\n",
    "        params = {}\n",
    "        for param in self.params:\n",
    "            name = param.name\n",
    "            if name in params:\n",
    "                name = name, 2\n",
    "            params[name] = param.get_value()\n",
    "        return params\n",
    "    \n",
    "    def mse(self, y):\n",
    "        # error between output and target\n",
    "        if y.ndim == 1:\n",
    "            se = (self.y_pred.T - y)**2\n",
    "        else:\n",
    "            se = T.sum((self.y_pred - y)**2, axis=1)\n",
    "        return T.mean(se)\n",
    "    \n",
    "    def valid_mse(self, y):\n",
    "        return mse(self, y)\n",
    "        \n",
    "\n",
    "def shared_dataset(data_xy, borrow=True, no_c=False):\n",
    "        \"\"\" Function that loads the dataset into shared variables\n",
    "        \"\"\"\n",
    "        data_x, data_y = data_xy[:2]\n",
    "        shared_x = theano.shared(np.asarray(data_x,\n",
    "                                               dtype='float32'),\n",
    "                                 borrow=borrow)\n",
    "        shared_y = theano.shared(np.asarray(data_y,\n",
    "                                               dtype='float32'),\n",
    "                                 borrow=borrow)\n",
    "        return shared_x, shared_y\n",
    "\n",
    "def train_nn(train_dataset, valid_dataset=None, n_hidden=20, learning_rate=0.01, n_epochs=10, batch_size=20, linear=False, mult_ys=True, rho=0, nesterov=True, mu=0, n_in=61, n_out=2):\n",
    "    \"\"\"\n",
    "    Demonstrate stochastic gradient descent optimization for a multilayer\n",
    "    perceptron\n",
    "\n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used (factor for the stochastic\n",
    "    gradient\n",
    "\n",
    "    :type n_epochs: int\n",
    "    :param n_epochs: maximal number of epochs to run the optimizer\n",
    "\n",
    "   \"\"\"\n",
    "    train_set_x, train_set_y = shared_dataset(train_dataset)\n",
    "    if valid_dataset:\n",
    "        valid_set_x, valid_set_y = shared_dataset(valid_dataset)\n",
    "    \n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "    \n",
    "    \n",
    "    ######################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print '... building the model'\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "    x = T.fmatrix('x')   # input data from visual neurons\n",
    "    if n_out == 1:\n",
    "        y = T.fvector('y') # ground truth\n",
    "    else:\n",
    "        y = T.fmatrix('y')  # ground truth\n",
    "\n",
    "    rng = np.random.RandomState(1234)\n",
    "\n",
    "    # construct the MLP class\n",
    "    if linear:\n",
    "        nn = Perceptron(rng=rng, input=x, n_in=n_in, n_out=n_out)\n",
    "    else:\n",
    "        nn = MLP(rng=rng, input=x, n_in=n_in, n_hidden=n_hidden, n_out=n_out)\n",
    "    \n",
    "    cost = nn.mse(y)\n",
    "    \n",
    "    def RMSprop(cost, params, learning_rate=0.001, rho=0.9, epsilon=1e-6, mu=0, nesterov=False):\n",
    "        gparams = T.grad(cost, params)\n",
    "        updates = []\n",
    "        for p, g in zip(params, gparams):\n",
    "            v = theano.shared(p.get_value() * 0.)\n",
    "            ms = theano.shared(p.get_value() * 0.)\n",
    "            ms_new = rho * ms + (1 - rho) * g ** 2\n",
    "            gradient_scaling = T.sqrt(ms_new + epsilon)\n",
    "            g = g / gradient_scaling\n",
    "            \"\"\"\n",
    "            (1) v_t = mu * v_t-1 - lr * gradient_f(params_t)\n",
    "            or\n",
    "            classic\n",
    "            (2) params_t = params_t-1 + v_t\n",
    "            nesterov\n",
    "            (7) params_t = params_t-1 + mu * v_t - lr * gradient_f(params_t-1)\n",
    "            (8) params_t = params_t-1 + mu**2 * v_t-1 - (1+mu) * lr * gradient_f(params_t-1)\n",
    "            \"\"\"\n",
    "            v_new = mu * v - (1 - mu) * learning_rate * g\n",
    "            if nesterov:\n",
    "                p_new = p + mu * v_new - (1 - mu) * learning_rate * g\n",
    "            else:\n",
    "                p_new = p + v_new\n",
    "            updates.append((ms, ms_new))\n",
    "            updates.append((v, v_new))\n",
    "            updates.append((p, p_new))\n",
    "                \n",
    "        return updates\n",
    "    \n",
    "    if rho:\n",
    "        updates = RMSprop(cost, nn.params, learning_rate=learning_rate, rho=rho, mu=mu, nesterov=nesterov)\n",
    "    else:\n",
    "        # compute the gradient of cost with respect to theta (sotred in params)\n",
    "        # the resulting gradients will be stored in a list gparams\n",
    "        gparams = [T.grad(cost, param) for param in nn.params]\n",
    "\n",
    "        # specify how to update the parameters of the model as a list of\n",
    "        # (variable, update expression) pairs\n",
    "\n",
    "        updates = [\n",
    "            (param, param - learning_rate * gparam)\n",
    "            for param, gparam in zip(nn.params, gparams)\n",
    "        ]\n",
    "    \n",
    "    def inspect_inputs(i, node, fn):\n",
    "        print i, node, \"input(s) value(s):\", [input[0] for input in fn.inputs]\n",
    "\n",
    "    def inspect_outputs(i, node, fn):\n",
    "        print \"output(s) value(s):\", [output[0] for output in fn.outputs]\n",
    "\n",
    "    # compiling a Theano function `train_model` that returns the cost, but\n",
    "    # in the same time updates the parameter of the model based on the rules\n",
    "    # defined in `updates`\n",
    "    train_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if mult_ys:\n",
    "        valid_mse = nn.valid_mse(y)\n",
    "    else:\n",
    "        valid_mse = cost\n",
    "    \n",
    "    validate_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=valid_mse,\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size:(index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size:(index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    print '... training'\n",
    "\n",
    "    epoch = 0 \n",
    "\n",
    "    if valid_dataset:\n",
    "        valid_mse = np.zeros(n_epochs)\n",
    "\n",
    "    while (epoch < n_epochs):\n",
    "        for minibatch_index in xrange(n_train_batches):\n",
    "            \n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            \n",
    "        if valid_dataset:\n",
    "            validation_losses = [validate_model(i) for i in xrange(n_valid_batches)]\n",
    "            this_validation_loss = np.mean(validation_losses)\n",
    "            valid_mse[epoch] = this_validation_loss\n",
    "            print(\n",
    "                'epoch %i, validation error %f' %\n",
    "                (\n",
    "                    epoch,\n",
    "                    this_validation_loss,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        epoch = epoch + 1\n",
    "\n",
    "    if valid_dataset:\n",
    "        return nn, x, valid_mse\n",
    "    return nn, x\n",
    "\n",
    "def test_nn(nn, nnx, test_data):\n",
    "    print 'testing'\n",
    "    test_batch_size = 1\n",
    "    test_set_x, test_set_y = shared_dataset(test_data)\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "    x = nnx   # input data from visual neurons\n",
    "    test_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=nn.y_pred,\n",
    "        givens={\n",
    "            x: test_set_x[index * test_batch_size: (index + 1) * test_batch_size]\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    true_ys = test_set_y.get_value()\n",
    "    pred_ys = np.zeros((len(true_ys), 2))\n",
    "    for i in range(len(true_ys)):\n",
    "        pred_ys[i] = test_model(i)\n",
    "        #print test_model(i)[0], true_ys[i]\n",
    "        #print test_model(i)[0] * 90, true_ys[i]\n",
    "    \n",
    "    #print nn.get_params()\n",
    "    return pred_ys, true_ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bf9840f99c17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqty1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print df[df.qty1 == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2222 trials per contrast level\n",
      "33 trials per contrast level\n",
      "... building the model\n",
      "... training\n",
      "epoch 0, validation error 406.223333\n",
      "epoch 1, validation error 377.721167\n",
      "epoch 2, validation error 354.328205\n",
      "epoch 3, validation error 335.464903\n",
      "epoch 4, validation error 318.233668\n",
      "epoch 5, validation error 303.331440\n",
      "epoch 6, validation error 292.498311\n",
      "epoch 7, validation error 283.713510\n",
      "epoch 8, validation error 276.927608\n",
      "epoch 9, validation error 271.638353\n",
      "epoch 10, validation error 266.568532\n",
      "epoch 11, validation error 262.175182\n",
      "epoch 12, validation error 258.278312\n",
      "epoch 13, validation error 254.896142\n",
      "epoch 14, validation error 251.893894\n",
      "epoch 15, validation error 248.760721\n",
      "epoch 16, validation error 246.141164\n",
      "epoch 17, validation error 243.668964\n",
      "epoch 18, validation error 241.674605\n",
      "epoch 19, validation error 239.813908\n",
      "epoch 20, validation error 237.914299\n",
      "epoch 21, validation error 236.138632\n",
      "epoch 22, validation error 234.533183\n",
      "epoch 23, validation error 232.936289\n",
      "epoch 24, validation error 231.482206\n",
      "epoch 25, validation error 230.145046\n",
      "epoch 26, validation error 228.790927\n",
      "epoch 27, validation error 227.424293\n",
      "epoch 28, validation error 226.083960\n",
      "epoch 29, validation error 224.776851\n",
      "epoch 30, validation error 223.490676\n",
      "epoch 31, validation error 222.205529\n",
      "epoch 32, validation error 220.940770\n",
      "epoch 33, validation error 219.750494\n",
      "epoch 34, validation error 218.590756\n",
      "epoch 35, validation error 217.501683\n",
      "epoch 36, validation error 216.371711\n",
      "epoch 37, validation error 215.329561\n",
      "epoch 38, validation error 214.280348\n",
      "epoch 39, validation error 213.342864\n",
      "epoch 40, validation error 212.375938\n",
      "epoch 41, validation error 211.636761\n",
      "epoch 42, validation error 210.837662\n",
      "epoch 43, validation error 210.056663\n",
      "epoch 44, validation error 209.254129\n",
      "epoch 45, validation error 208.302766\n",
      "epoch 46, validation error 207.410387\n",
      "epoch 47, validation error 206.384864\n",
      "epoch 48, validation error 205.277623\n",
      "epoch 49, validation error 204.221063\n",
      "epoch 50, validation error 203.189867\n",
      "epoch 51, validation error 202.169226\n",
      "epoch 52, validation error 201.149790\n",
      "epoch 53, validation error 200.107234\n",
      "epoch 54, validation error 199.071611\n",
      "epoch 55, validation error 197.954540\n",
      "epoch 56, validation error 196.909626\n",
      "epoch 57, validation error 195.722408\n",
      "epoch 58, validation error 194.384626\n",
      "epoch 59, validation error 193.041056\n",
      "epoch 60, validation error 191.685374\n",
      "epoch 61, validation error 190.284620\n",
      "epoch 62, validation error 188.836763\n",
      "epoch 63, validation error 187.319177\n",
      "epoch 64, validation error 185.828692\n",
      "epoch 65, validation error 184.207413\n",
      "epoch 66, validation error 182.640327\n",
      "epoch 67, validation error 181.150874\n",
      "epoch 68, validation error 179.408963\n",
      "epoch 69, validation error 177.595070\n",
      "epoch 70, validation error 175.812998\n",
      "epoch 71, validation error 174.081261\n",
      "epoch 72, validation error 172.209170\n",
      "epoch 73, validation error 170.349887\n",
      "epoch 74, validation error 168.440095\n",
      "epoch 75, validation error 166.538002\n",
      "epoch 76, validation error 164.589044\n",
      "epoch 77, validation error 162.550361\n",
      "epoch 78, validation error 160.426355\n",
      "epoch 79, validation error 158.359430\n",
      "epoch 80, validation error 156.486103\n",
      "epoch 81, validation error 154.495154\n",
      "epoch 82, validation error 152.330767\n",
      "epoch 83, validation error 150.378670\n",
      "epoch 84, validation error 148.052037\n",
      "epoch 85, validation error 146.102048\n",
      "epoch 86, validation error 144.093423\n",
      "epoch 87, validation error 142.144944\n",
      "epoch 88, validation error 140.166521\n",
      "epoch 89, validation error 138.175974\n",
      "epoch 90, validation error 136.351099\n",
      "epoch 91, validation error 134.461492\n",
      "epoch 92, validation error 132.399299\n",
      "epoch 93, validation error 130.528920\n",
      "epoch 94, validation error 128.625959\n",
      "epoch 95, validation error 126.607758\n",
      "epoch 96, validation error 124.740588\n",
      "epoch 97, validation error 122.869822\n",
      "epoch 98, validation error 121.123898\n",
      "epoch 99, validation error 119.380068\n",
      "... building the model\n",
      "... training\n",
      "epoch 0, validation error 423.551476\n",
      "epoch 1, validation error 364.284845\n",
      "epoch 2, validation error 344.840370\n",
      "epoch 3, validation error 333.342052\n",
      "epoch 4, validation error 323.701545\n",
      "epoch 5, validation error 315.799184\n",
      "epoch 6, validation error 309.184411\n",
      "epoch 7, validation error 303.382227\n",
      "epoch 8, validation error 298.660731\n",
      "epoch 9, validation error 294.222188\n",
      "epoch 10, validation error 290.278867\n",
      "epoch 11, validation error 286.510954\n",
      "epoch 12, validation error 283.060027\n",
      "epoch 13, validation error 279.926670\n",
      "epoch 14, validation error 276.808520\n",
      "epoch 15, validation error 274.038757\n",
      "epoch 16, validation error 271.275143\n",
      "epoch 17, validation error 268.529726\n",
      "epoch 18, validation error 265.728643\n",
      "epoch 19, validation error 263.106423\n",
      "epoch 20, validation error 260.680215\n",
      "epoch 21, validation error 258.376041\n",
      "epoch 22, validation error 256.116416\n",
      "epoch 23, validation error 253.902398\n",
      "epoch 24, validation error 251.905309\n",
      "epoch 25, validation error 250.058498\n",
      "epoch 26, validation error 248.258924\n",
      "epoch 27, validation error 246.464285\n",
      "epoch 28, validation error 244.808505\n",
      "epoch 29, validation error 243.208051\n",
      "epoch 30, validation error 241.646863\n",
      "epoch 31, validation error 240.175370\n",
      "epoch 32, validation error 238.742256\n",
      "epoch 33, validation error 237.356982\n",
      "epoch 34, validation error 235.946148\n",
      "epoch 35, validation error 234.595428\n",
      "epoch 36, validation error 233.222942\n",
      "epoch 37, validation error 231.898910\n",
      "epoch 38, validation error 230.620911\n",
      "epoch 39, validation error 229.403189\n",
      "epoch 40, validation error 228.143881\n",
      "epoch 41, validation error 226.790352\n",
      "epoch 42, validation error 225.495488\n",
      "epoch 43, validation error 224.143132\n",
      "epoch 44, validation error 222.772579\n",
      "epoch 45, validation error 221.445975\n",
      "epoch 46, validation error 220.068004\n",
      "epoch 47, validation error 218.686714\n",
      "epoch 48, validation error 217.362030\n",
      "epoch 49, validation error 216.197877\n",
      "epoch 50, validation error 214.630256\n",
      "epoch 51, validation error 213.187055\n",
      "epoch 52, validation error 211.739518\n",
      "epoch 53, validation error 210.406410\n",
      "epoch 54, validation error 208.894239\n",
      "epoch 55, validation error 207.557866\n",
      "epoch 56, validation error 205.955514\n",
      "epoch 57, validation error 204.215789\n",
      "epoch 58, validation error 202.678925\n",
      "epoch 59, validation error 201.138222\n",
      "epoch 60, validation error 199.646793\n",
      "epoch 61, validation error 198.113129\n",
      "epoch 62, validation error 196.536274\n",
      "epoch 63, validation error 194.939255\n",
      "epoch 64, validation error 193.289813\n",
      "epoch 65, validation error 191.599458\n",
      "epoch 66, validation error 189.846669\n",
      "epoch 67, validation error 188.105116\n",
      "epoch 68, validation error 186.065245\n",
      "epoch 69, validation error 184.345921\n",
      "epoch 70, validation error 182.509562\n",
      "epoch 71, validation error 180.649932\n",
      "epoch 72, validation error 178.831510\n",
      "epoch 73, validation error 177.069047\n",
      "epoch 74, validation error 175.318683\n",
      "epoch 75, validation error 173.522144\n",
      "epoch 76, validation error 171.757586\n",
      "epoch 77, validation error 170.009885\n",
      "epoch 78, validation error 168.215493\n",
      "epoch 79, validation error 166.309008\n",
      "epoch 80, validation error 164.525675\n",
      "epoch 81, validation error 162.696926\n",
      "epoch 82, validation error 160.924903\n",
      "epoch 83, validation error 159.266979\n",
      "epoch 84, validation error 157.602756\n",
      "epoch 85, validation error 155.977389\n",
      "epoch 86, validation error 154.420265\n",
      "epoch 87, validation error 152.889414\n",
      "epoch 88, validation error 151.424058\n",
      "epoch 89, validation error 149.973017\n",
      "epoch 90, validation error 148.576902\n",
      "epoch 91, validation error 147.195788\n",
      "epoch 92, validation error 145.861342\n",
      "epoch 93, validation error 144.584897\n",
      "epoch 94, validation error 143.259596\n",
      "epoch 95, validation error 142.013910\n",
      "epoch 96, validation error 140.831233\n",
      "epoch 97, validation error 139.652072\n",
      "epoch 98, validation error 138.471188\n",
      "epoch 99, validation error 137.364484\n",
      "... building the model\n",
      "... training\n",
      "epoch 0, validation error 384.136004\n",
      "epoch 1, validation error 354.396223\n",
      "epoch 2, validation error 344.074293\n",
      "epoch 3, validation error 318.269104\n",
      "epoch 4, validation error 302.444986\n",
      "epoch 5, validation error 288.752997\n",
      "epoch 6, validation error 277.792531\n",
      "epoch 7, validation error 268.272933\n",
      "epoch 8, validation error 261.404687\n",
      "epoch 9, validation error 256.510581\n",
      "epoch 10, validation error 252.301325\n",
      "epoch 11, validation error 248.780121\n",
      "epoch 12, validation error 245.743075\n",
      "epoch 13, validation error 243.158600\n",
      "epoch 14, validation error 240.912814\n",
      "epoch 15, validation error 238.756343\n",
      "epoch 16, validation error 236.750891\n",
      "epoch 17, validation error 234.847805\n",
      "epoch 18, validation error 233.027932\n",
      "epoch 19, validation error 231.387503\n",
      "epoch 20, validation error 229.999137\n",
      "epoch 21, validation error 228.591641\n",
      "epoch 22, validation error 226.824645\n",
      "epoch 23, validation error 225.100075\n",
      "epoch 24, validation error 223.325599\n",
      "epoch 25, validation error 221.716866\n",
      "epoch 26, validation error 220.238650\n",
      "epoch 27, validation error 218.766902\n",
      "epoch 28, validation error 217.370037\n",
      "epoch 29, validation error 215.779217\n",
      "epoch 30, validation error 214.678516\n",
      "epoch 31, validation error 213.181665\n",
      "epoch 32, validation error 211.713186\n",
      "epoch 33, validation error 210.299825\n",
      "epoch 34, validation error 208.705406\n",
      "epoch 35, validation error 207.191485\n",
      "epoch 36, validation error 205.752678\n",
      "epoch 37, validation error 204.394457\n",
      "epoch 38, validation error 202.956837\n",
      "epoch 39, validation error 201.656774\n",
      "epoch 40, validation error 200.178700\n",
      "epoch 41, validation error 198.690264\n",
      "epoch 42, validation error 197.227832\n",
      "epoch 43, validation error 195.694416\n",
      "epoch 44, validation error 194.169587\n",
      "epoch 45, validation error 192.654786"
     ]
    }
   ],
   "source": [
    "train_data_1 = generate_testset(19998, discrete_c=3, low=.3, high=.7)\n",
    "valid_data_1 = generate_testset(297, discrete_c=3, low=.3, high=.7)\n",
    "lrs = [.01, .005, .001, .0005, .0001, .00001]\n",
    "rhos = [0, .75, .9, .95, .99, .999]\n",
    "mus = [0, .75, .9, .95, .99, .999]\n",
    "nests = [True, False]\n",
    "nrows = len(lrs) * len(rhos) * len(mus) * len(nests)\n",
    "nn_df = pd.DataFrame(index=np.arange(0, nrows), columns=('lr', 'rho', 'mu', 'nest', 'valid_nn') )\n",
    "nns = {}\n",
    "ind = 0\n",
    "for lr in lrs:\n",
    "    for rho in rhos:\n",
    "        for mu in mus:\n",
    "            for n in nests:\n",
    "                nn, nnx, valid_nn = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=lr, n_epochs=100, rho=rho, mu=mu, nesterov=n)\n",
    "                nn_df.loc[ind] = [lr, rho, mu, n, valid_nn[99]]\n",
    "                nns[lr, rho, mu, n] = nn, nnx, valid_nn\n",
    "                ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
