{
 "metadata": {
  "name": "",
  "signature": "sha256:1350cf02c3836f0e6e51b2844b2bed8639c75db3308c8499285d1d7e79033bb1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import math\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "import pystan\n",
      "import matplotlib.pyplot as plt\n",
      "import argparse\n",
      "\n",
      "nneuron = 61\n",
      "min_angle = -90\n",
      "max_angle = 90\n",
      "sprefs = np.linspace(min_angle, max_angle, nneuron)\n",
      "ndata = 3000\n",
      "eps = np.finfo(np.float64).eps\n",
      "\n",
      "r_max = 10\n",
      "sigtc_sq = float(10**2)\n",
      "sigtc = 10\n",
      "c_50 = 13.1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def random_s(ndata, sort):\n",
      "    s = np.random.rand(2, ndata) * 120 - 60\n",
      "    if sort:\n",
      "        s = np.sort(s, axis=0)\n",
      "    return s[0], s[1]\n",
      "\n",
      "def generate_trainset(ndata, r_max=10):\n",
      "    s_0, s_1 = random_s(ndata, True)\n",
      "    c_0, c_1 = np.ones((2, ndata)) * .5\n",
      "    r, s, c = generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True, s_0, s_1, c_0, c_1)\n",
      "    return r, s, c\n",
      "\n",
      "def generate_s1set(ndata):\n",
      "    s_0, s_1 = random_s(ndata, True)\n",
      "    c_0 = np.ones(ndata)\n",
      "    c_1 = np.zeros(ndata)\n",
      "    r, s, c = generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True, s_0, s_1, c_0, c_1)\n",
      "    return r, s, c\n",
      "    \n",
      "def generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, noise, sort, s_0, s_1, c_0, c_1):\n",
      "    c_rms = np.sqrt(np.square(c_0) + np.square(c_1))\n",
      "    sprefs_data = np.tile(sprefs, (ndata, 1))\n",
      "    s_0t = np.exp(-np.square((np.transpose(np.tile(s_0, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_0 = c_0 * s_0t.T\n",
      "    s_1t = np.exp(-np.square((np.transpose(np.tile(s_1, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_1 = c_1 * s_1t.T\n",
      "    #r = r_max * (stim_0 + stim_1)/(c_50 + c_rms)\n",
      "    r = r_max * (stim_0 + stim_1)\n",
      "    r = r.T\n",
      "    s = np.array((s_0, s_1)).T\n",
      "    s = s/90\n",
      "    c = np.array((c_0, c_1)).T\n",
      "    if noise == \"poisson\":\n",
      "        r = np.random.poisson(r) + 0.0\n",
      "    return r, s, c\n",
      "\n",
      "def generate_s_data(stim_0, stim_1, ndata, con_0=.5, con_1=.5, r_max=10):\n",
      "    #c_0, c_1 = np.ones((2, ndata)) * .5\n",
      "    c_0 = np.ones(ndata) * con_0\n",
      "    c_1 = np.ones(ndata) * con_1\n",
      "    s_0, s_1 = np.ones((2, ndata))\n",
      "    s_0 = s_0 * stim_0\n",
      "    s_1 = s_1 * stim_1\n",
      "    r, s, c = generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True, s_0, s_1, c_0, c_1)\n",
      "    return r, s, c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "neurons_code = \"\"\"\n",
      "    data {\n",
      "        int<lower=0> N; // number of neurons\n",
      "        int r[N]; // neural response\n",
      "        real sprefs[N]; // preferred stimuli\n",
      "        real<lower=0> c_1;\n",
      "        real<lower=0> c_2;\n",
      "        int r_max;\n",
      "        //real c_rms;\n",
      "        //real c_50;\n",
      "        //real<lower=0> sig_tc;\n",
      "        real<lower=0> sigtc_sq;\n",
      "    }\n",
      "    parameters {\n",
      "        real s_1;\n",
      "        //real s_2;\n",
      "        real<lower=s_1> s_2;\n",
      "    }\n",
      "    transformed parameters {\n",
      "        real lambda[N];\n",
      "        for (n in 1:N)\n",
      "            // lambda[n] <- r_max * ((c_1 * exp(normal_log(s_1, sprefs[n], sig_tc)) + c_2 * exp(normal_log(s_2, sprefs[n], sig_tc)))/(c_rms + c_50));\n",
      "            // lambda[n] <- r_max * (c_1 * exp(normal_log(s_1, sprefs[n], sig_tc)) + c_2 * exp(normal_log(s_2, sprefs[n], sig_tc)));\n",
      "            lambda[n] <- r_max * (c_1 * exp(- square(s_1 - sprefs[n])/(2 * sigtc_sq)) + c_2 * exp(- square(s_2 - sprefs[n])/(2 * sigtc_sq)));\n",
      "    }\n",
      "    model {\n",
      "        s_1 ~ uniform(-60, 60);\n",
      "        //s_2 ~ uniform(-60, 60);\n",
      "        s_2 ~ uniform(s_1, 60);\n",
      "        r ~ poisson(lambda);\n",
      "    }\n",
      "    \"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fisher_inf(s_0, s_1, c_0, c_1, r_max=10):\n",
      "    fs_0 = np.exp(-np.square((np.transpose(np.tile(s_0, (nneuron, 1))) - sprefs))/(2 * sigtc_sq))[0]\n",
      "    qs_0 = r_max * c_0 * fs_0\n",
      "    df_s0 = ((-s_0 + sprefs)/sigtc_sq) * qs_0\n",
      "    fs_1 = np.exp(-np.square((np.transpose(np.tile(s_1, (nneuron, 1))) - sprefs))/(2 * sigtc_sq))[0]\n",
      "    qs_1 = r_max * c_1 * fs_1\n",
      "    df_s1 = ((-s_1 + sprefs)/sigtc_sq) * qs_1\n",
      "    Q = qs_0 + qs_1\n",
      "    Q_inv = 1/Q\n",
      "    J_11 = np.sum(np.square(df_s0) * Q_inv)\n",
      "    J_22 = np.sum(np.square(df_s1) * Q_inv)\n",
      "    J_12 = J_21 = np.sum(df_s0 * df_s1 * Q_inv)\n",
      "    fisher = np.linalg.inv([[J_11, J_12], [J_21, J_22]])\n",
      "    return fisher\n",
      "\n",
      "def fit_optimal(r, sm, s=None, N=61, init=None, sprefs=sprefs, sampling=False, sort=False, c_1=.5, c_2=.5, c_50=13.1, r_max=10, c_rms=0.707106781, sig_tc=10, sigtc_sq=10**2):\n",
      "    ndata = len(r)\n",
      "    neurons_dat = {'N': 61,\n",
      "                   'r': r[0].astype(int),\n",
      "                   'sprefs': sprefs,\n",
      "                   'c_1': .5,\n",
      "                   'c_2': .5,\n",
      "                   'c_50': 13.1,\n",
      "                   'r_max': r_max,\n",
      "                   'c_rms': 0.707106781,\n",
      "                   'sig_tc': 10,\n",
      "                   'sigtc_sq': sigtc_sq}\n",
      "\n",
      "    optimal = np.zeros((2, ndata))\n",
      "    print init\n",
      "    for i in range(len(r)):\n",
      "        neurons_dat['r'] = r[i].astype(int)\n",
      "        if sampling:\n",
      "            op = sm.sampling(data=neurons_dat)\n",
      "            optimal[0][i], optimal[1][i] = np.mean(op['s_1']), np.mean(op['s_2'])\n",
      "        else:\n",
      "            if s.any():\n",
      "                init = {'s_1':max(s[0][i], -60 + eps), 's_2':min(s[1][i] + eps, 60 - eps)}\n",
      "                print init\n",
      "                op = sm.optimizing(data=neurons_dat, init=init)\n",
      "            elif not init:\n",
      "                op = sm.optimizing(data=neurons_dat)\n",
      "            else:     \n",
      "                op = sm.optimizing(data=neurons_dat, init=init)\n",
      "            optimal[0][i], optimal[1][i] = op['s_1'], op['s_2']\n",
      "        if sort:\n",
      "            optimal = np.sort(optimal, axis=0)\n",
      "    return optimal"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Multilayer ReLU net\n",
      "\"\"\"\n",
      "\n",
      "def relu(x):\n",
      "    return theano.tensor.switch(x<0, 0, x)\n",
      "\n",
      "class HiddenLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
      "                 activation=T.nnet.sigmoid):\n",
      "        \"\"\"\n",
      "        Typical hidden layer of a MLP: units are fully-connected and have\n",
      "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
      "        and the bias vector b is of shape (n_out,).\n",
      "\n",
      "        :type rng: np.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.dmatrix\n",
      "        :param input: a symbolic tensor of shape (n_examples, n_in)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: dimensionality of input\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of hidden units\n",
      "\n",
      "        :type activation: theano.Op or function\n",
      "        :param activation: Non linearity to be applied in the hidden\n",
      "                           layer\n",
      "        \"\"\"\n",
      "        self.input = input\n",
      "        if W is None:\n",
      "            W_values = (1/np.sqrt(n_in)) * np.random.randn(n_in, n_out)\n",
      "            \n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        if b is None:\n",
      "            b_values = np.zeros((n_out,), dtype=theano.config.floatX)\n",
      "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "\n",
      "        lin_output = T.dot(input, self.W) + self.b\n",
      "        self.output = (\n",
      "            lin_output if activation is None\n",
      "            else activation(lin_output)\n",
      "        )\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "class COMLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None):\n",
      "        \"\"\"\n",
      "        Layer with Center of Mass decoder\n",
      "        Params same as above\n",
      "        \"\"\"\n",
      "        self.input = input\n",
      "        if W is None:\n",
      "            W_values = (1/np.sqrt(n_in)) * np.random.randn(n_in, n_out)\n",
      "\n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        \n",
      "        self.ones = np.ones((n_in, n_out))\n",
      "        \n",
      "        self.output = T.dot(input, self.W)/T.dot(input, self.ones)\n",
      "        \n",
      "        # parameters of the model\n",
      "        self.params = [self.W]\n",
      "\n",
      "class MLP(object):\n",
      "\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
      "\n",
      "        :type rng: np.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: symbolic variable that describes the input of the\n",
      "        architecture (one minibatch)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: number of input units, the dimension of the space in\n",
      "        which the datapoints lie\n",
      "\n",
      "        :type n_hidden: int\n",
      "        :param n_hidden: number of hidden units\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "        which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        self.hiddenLayer1 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            #activation=T.nnet.sigmoid\n",
      "            activation=relu\n",
      "        )\n",
      "        \n",
      "        self.hiddenLayer2 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=self.hiddenLayer1.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out,\n",
      "            #activation=relu\n",
      "            activation=None\n",
      "        )\n",
      "        \n",
      "        self.y_pred = self.hiddenLayer2.output\n",
      "        \n",
      "        # the parameters of the model are the parameters of the two layers it is made out of\n",
      "        self.params = self.hiddenLayer1.params + self.hiddenLayer2.params\n",
      "    \n",
      "    def get_params(self):\n",
      "\n",
      "        params = {}\n",
      "        for param in self.params:\n",
      "            name = param.name\n",
      "            if name in params:\n",
      "                name = name, 2\n",
      "            params[name] = param.get_value()\n",
      "        return params\n",
      "    \n",
      "    def mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "    \n",
      "    def mse_s1(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean((self.y_pred[0] - y[0]) ** 2)\n",
      "    \n",
      "    def valid_mse(self, y):\n",
      "        return T.mean(((self.y_pred[0] * 90) - (y[0] * 90)) ** 2 + ((self.y_pred[1] * 90) - (y[1] * 90)) ** 2)\n",
      "    \n",
      "    def sym_mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean(((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "                      * ((self.y_pred[1] - y[0]) ** 2 + (self.y_pred[0] - y[1]) ** 2))\n",
      "        \n",
      "class COMMLP(object):\n",
      "\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"\n",
      "        Params same as above\n",
      "        \"\"\"\n",
      "\n",
      "        self.hiddenLayer1 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            activation=T.nnet.sigmoid\n",
      "        )\n",
      "        \n",
      "        self.hiddenLayer2 = COMLayer(\n",
      "            rng=rng,\n",
      "            input=self.hiddenLayer1.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out,\n",
      "        )\n",
      "        \n",
      "        self.y_pred = self.hiddenLayer2.output\n",
      "        \n",
      "        # the parameters of the model are the parameters of the two layers it is made out of\n",
      "        self.params = self.hiddenLayer1.params + self.hiddenLayer2.params\n",
      "    \n",
      "    def get_params(self):\n",
      "        params = {}\n",
      "        for param in self.params:\n",
      "            name = param.name\n",
      "            if name in params:\n",
      "                name = name, 2\n",
      "            params[name] = param.get_value()\n",
      "        return params\n",
      "    \n",
      "    def mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "    \n",
      "    def sym_mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean(((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "                      * ((self.y_pred[1] - y[0]) ** 2 + (self.y_pred[0] - y[1]) ** 2))\n",
      "        \n",
      "\n",
      "def shared_dataset(data_xy, borrow=True):\n",
      "        \"\"\" Function that loads the dataset into shared variables\n",
      "        \"\"\"\n",
      "        data_x, data_y, _ = data_xy\n",
      "        shared_x = theano.shared(np.asarray(data_x,\n",
      "                                               dtype='float32'),\n",
      "                                 borrow=borrow)\n",
      "        shared_y = theano.shared(np.asarray(data_y,\n",
      "                                               dtype='float32'),\n",
      "                                 borrow=borrow)\n",
      "        return shared_x, shared_y\n",
      "\n",
      "def train_nn(train_dataset, valid_dataset=None, n_hidden=20, learning_rate=0.01, n_epochs=10, batch_size=20, test_data=None, COM=False, RMSProp=False, nesterov=True, momentum=0, n_in=61, n_out=2):\n",
      "    \"\"\"\n",
      "    Demonstrate stochastic gradient descent optimization for a multilayer\n",
      "    perceptron\n",
      "\n",
      "    :type learning_rate: float\n",
      "    :param learning_rate: learning rate used (factor for the stochastic\n",
      "    gradient\n",
      "\n",
      "    :type n_epochs: int\n",
      "    :param n_epochs: maximal number of epochs to run the optimizer\n",
      "\n",
      "   \"\"\"\n",
      "    train_set_x, train_set_y = shared_dataset(train_dataset)\n",
      "    if valid_dataset:\n",
      "        valid_set_x, valid_set_y = shared_dataset(valid_dataset)\n",
      "\n",
      "    # compute number of minibatches for training, validation and testing\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    n_valid_batches = n_train_batches\n",
      "    \n",
      "    \n",
      "    ######################\n",
      "    # BUILD ACTUAL MODEL #\n",
      "    ######################\n",
      "    print '... building the model'\n",
      "\n",
      "    # allocate symbolic variables for the data\n",
      "    index = T.lscalar()  # index to a [mini]batch\n",
      "    x = T.fmatrix('x')   # input data from visual neurons\n",
      "    y = T.fmatrix('y')  # ground truth\n",
      "\n",
      "    rng = np.random.RandomState(1234)\n",
      "\n",
      "    # construct the MLP class\n",
      "    nn = MLP(rng=rng, input=x, n_in=n_in, n_hidden=n_hidden, n_out=n_out)\n",
      "    \n",
      "    if COM:\n",
      "        nn = COMMLP(rng=rng, input=x, n_in=n_in, n_hidden=n_hidden, n_out=n_out)\n",
      "    else:\n",
      "        nn = MLP(rng=rng, input=x, n_in=n_in, n_hidden=n_hidden, n_out=n_out)\n",
      "\n",
      "    cost = nn.mse(y)\n",
      "    \n",
      "    def RMSprop(cost, params, learning_rate=0.001, rho=0.9, epsilon=1e-6, mu=0, nesterov=False):\n",
      "        gparams = T.grad(cost, params)\n",
      "        updates = []\n",
      "        for p, g in zip(params, gparams):\n",
      "            v = theano.shared(p.get_value() * 0.)\n",
      "            ms = theano.shared(p.get_value() * 0.)\n",
      "            ms_new = rho * ms + (1 - rho) * g ** 2\n",
      "            gradient_scaling = T.sqrt(ms_new + epsilon)\n",
      "            g = g / gradient_scaling\n",
      "            \"\"\"\n",
      "            (1) v_t = mu * v_t-1 - lr * gradient_f(params_t)\n",
      "            or\n",
      "            classic\n",
      "            (2) params_t = params_t-1 + v_t\n",
      "            nesterov\n",
      "            (7) params_t = params_t-1 + mu * v_t - lr * gradient_f(params_t-1)\n",
      "            (8) params_t = params_t-1 + mu**2 * v_t-1 - (1+mu) * lr * gradient_f(params_t-1)\n",
      "            \"\"\"\n",
      "            v_new = mu * v - (1 - mu) * learning_rate * g\n",
      "            if nesterov:\n",
      "                p_new = p + mu * v_new - (1 - mu) * learning_rate * g\n",
      "            else:\n",
      "                p_new = p + v_new\n",
      "            updates.append((ms, ms_new))\n",
      "            updates.append((v, v_new))\n",
      "            updates.append((p, p_new))\n",
      "                \n",
      "        return updates\n",
      "    \n",
      "    if RMSProp:\n",
      "        updates = RMSprop(cost, nn.params, learning_rate=learning_rate, mu=momentum, nesterov=nesterov)\n",
      "    else:\n",
      "        # compute the gradient of cost with respect to theta (sotred in params)\n",
      "        # the resulting gradients will be stored in a list gparams\n",
      "        gparams = [T.grad(cost, param) for param in nn.params]\n",
      "\n",
      "        # specify how to update the parameters of the model as a list of\n",
      "        # (variable, update expression) pairs\n",
      "\n",
      "        updates = [\n",
      "            (param, param - learning_rate * gparam)\n",
      "            for param, gparam in zip(nn.params, gparams)\n",
      "        ]\n",
      "    \n",
      "    def inspect_inputs(i, node, fn):\n",
      "        print i, node, \"input(s) value(s):\", [input[0] for input in fn.inputs]\n",
      "\n",
      "    def inspect_outputs(i, node, fn):\n",
      "        print \"output(s) value(s):\", [output[0] for output in fn.outputs]\n",
      "\n",
      "    # compiling a Theano function `train_model` that returns the cost, but\n",
      "    # in the same time updates the parameter of the model based on the rules\n",
      "    # defined in `updates`\n",
      "    train_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=cost,\n",
      "        updates=updates,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "    \n",
      "    validate_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=nn.valid_mse(y),\n",
      "        givens={\n",
      "            x: valid_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "            y: valid_set_y[index * batch_size:(index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "\n",
      "    ###############\n",
      "    # TRAIN MODEL #\n",
      "    ###############\n",
      "    print '... training'\n",
      "\n",
      "    start_time = time.clock()\n",
      "\n",
      "    epoch = 0 \n",
      "    done_looping = False\n",
      "    \n",
      "    if valid_dataset:\n",
      "        valid_mse = np.zeros(n_epochs)\n",
      "\n",
      "    while (epoch < n_epochs) and (not done_looping):\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "            \n",
      "            minibatch_avg_cost = train_model(minibatch_index)\n",
      "            \n",
      "        if valid_dataset:\n",
      "            validation_losses = [validate_model(i) for i in xrange(n_valid_batches)]\n",
      "            this_validation_loss = np.mean(validation_losses)\n",
      "            valid_mse[epoch] = this_validation_loss \n",
      "\n",
      "            print(\n",
      "                'epoch %i, minibatch %i/%i, validation error %f' %\n",
      "                (\n",
      "                    epoch,\n",
      "                    minibatch_index + 1,\n",
      "                    n_train_batches,\n",
      "                    this_validation_loss\n",
      "                )\n",
      "            )\n",
      "            \n",
      "        epoch = epoch + 1\n",
      "\n",
      "    end_time = time.clock()\n",
      "    \n",
      "    if valid_dataset:\n",
      "        return nn, x, valid_mse\n",
      "    return nn, x\n",
      "\n",
      "def test_nn(nn, nnx, test_data):\n",
      "    print 'testing'\n",
      "    test_batch_size = 1\n",
      "    test_set_x, test_set_y = shared_dataset(test_data)\n",
      "    index = T.lscalar()  # index to a [mini]batch\n",
      "    x = nnx   # input data from visual neurons\n",
      "    test_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=nn.y_pred,\n",
      "        givens={\n",
      "            x: test_set_x[index * test_batch_size: (index + 1) * test_batch_size]\n",
      "        },\n",
      "    )\n",
      "    \n",
      "    true_ys = test_set_y.get_value()\n",
      "    pred_ys = np.zeros((len(true_ys), 2))\n",
      "    for i in range(len(true_ys)):\n",
      "        pred_ys[i] = test_model(i)\n",
      "        #print test_model(i)[0], true_ys[i]\n",
      "        #print test_model(i)[0] * 90, true_ys[i]\n",
      "    \n",
      "    #print nn.get_params()\n",
      "    return pred_ys, true_ys"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Setting up models\n",
      "sm = pystan.StanModel(model_code=neurons_code)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_statistics(s1, s2, preds):\n",
      "    mean_s1 = np.mean(preds[0])\n",
      "    mean_s2 = np.mean(preds[1])\n",
      "    bias_s1 = mean_s1 - s1\n",
      "    bias_s2 = mean_s2 - s2\n",
      "    covmat = np.cov(preds)\n",
      "    var_s1 = covmat[0, 0]\n",
      "    var_s2 = covmat[1, 1]\n",
      "    cov = covmat[0, 1]\n",
      "    corr = cov / (np.sqrt(var_s1) * np.sqrt(var_s2))\n",
      "    stats = {'mean_s1': mean_s1, 'mean_s2': mean_s2, 'bias_s1': bias_s1, 'bias_s2': bias_s2, 'var_s1': var_s1, 'var_s2': var_s2, 'cov': cov, 'corr': corr}\n",
      "    return stats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data_1 = generate_trainset(20000)\n",
      "valid_data_1 = generate_trainset(20000)\n",
      "nn_rms, nnx_rms, valid_mse_rms = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100, RMSProp=True)\n",
      "\"\"\"\n",
      "nn_rms_m9, nnx_rms_m9, valid_mse_rms_m9 = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100, RMSProp=True, nesterov=True, momentum=0.9)\n",
      "nn_rms_m9f, nnx_rms_m9f, valid_mse_rms_m9f = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100, RMSProp=True, nesterov=False, momentum=0.9)\n",
      "nn_rms_m95, nnx_rms_m95, valid_mse_rms_m95 = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100, RMSProp=True, momentum=0.95)\n",
      "nn_rms_m99, nnx_rms_m99, valid_mse_rms_m99 = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100, RMSProp=True, momentum=0.99)\n",
      "\"\"\"\n",
      "nn_plain, nnx_plain, valid_mse_plain = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "... building the model\n",
        "... training"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 0, minibatch 1000/1000, validation error 250.311201"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 1, minibatch 1000/1000, validation error 140.217723"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 2, minibatch 1000/1000, validation error 112.081866"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 3, minibatch 1000/1000, validation error 82.674228"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 4, minibatch 1000/1000, validation error 77.711757"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 5, minibatch 1000/1000, validation error 73.068357"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 6, minibatch 1000/1000, validation error 67.375080"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 7, minibatch 1000/1000, validation error 61.801940"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 8, minibatch 1000/1000, validation error 58.944289"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 9, minibatch 1000/1000, validation error 56.370403"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 10, minibatch 1000/1000, validation error 54.633444"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 11, minibatch 1000/1000, validation error 53.608263"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 12, minibatch 1000/1000, validation error 53.891287"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 13, minibatch 1000/1000, validation error 52.568722"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 14, minibatch 1000/1000, validation error 52.522395"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 15, minibatch 1000/1000, validation error 51.639672"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 16, minibatch 1000/1000, validation error 50.851221"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 17, minibatch 1000/1000, validation error 49.984910"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 18, minibatch 1000/1000, validation error 50.480746"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 19, minibatch 1000/1000, validation error 50.399922"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 20, minibatch 1000/1000, validation error 50.771018"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 21, minibatch 1000/1000, validation error 49.663078"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 22, minibatch 1000/1000, validation error 49.776202"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 23, minibatch 1000/1000, validation error 49.686257"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 24, minibatch 1000/1000, validation error 50.276070"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 25, minibatch 1000/1000, validation error 49.840241"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 26, minibatch 1000/1000, validation error 49.612653"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 27, minibatch 1000/1000, validation error 49.306336"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 28, minibatch 1000/1000, validation error 50.732891"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 29, minibatch 1000/1000, validation error 50.071811"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 30, minibatch 1000/1000, validation error 50.871505"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 31, minibatch 1000/1000, validation error 50.561225"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 32, minibatch 1000/1000, validation error 51.525009"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 33, minibatch 1000/1000, validation error 51.142491"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 34, minibatch 1000/1000, validation error 51.391288"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 35, minibatch 1000/1000, validation error 50.821797"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 36, minibatch 1000/1000, validation error 51.419374"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 37, minibatch 1000/1000, validation error 51.749832"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 38, minibatch 1000/1000, validation error 51.893857"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 39, minibatch 1000/1000, validation error 51.390250"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 40, minibatch 1000/1000, validation error 52.233440"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 41, minibatch 1000/1000, validation error 51.759835"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 42, minibatch 1000/1000, validation error 51.401449"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 43, minibatch 1000/1000, validation error 51.117266"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 44, minibatch 1000/1000, validation error 50.933394"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 45, minibatch 1000/1000, validation error 50.711283"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 46, minibatch 1000/1000, validation error 51.287401"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 47, minibatch 1000/1000, validation error 51.014491"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 48, minibatch 1000/1000, validation error 51.481817"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 49, minibatch 1000/1000, validation error 50.961857"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 50, minibatch 1000/1000, validation error 51.241396"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 51, minibatch 1000/1000, validation error 51.090947"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 52, minibatch 1000/1000, validation error 50.864176"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 53, minibatch 1000/1000, validation error 49.870369"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 54, minibatch 1000/1000, validation error 49.944774"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 55, minibatch 1000/1000, validation error 49.959919"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 56, minibatch 1000/1000, validation error 49.473931"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 57, minibatch 1000/1000, validation error 49.319531"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 58, minibatch 1000/1000, validation error 48.482910"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 59, minibatch 1000/1000, validation error 48.334190"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 60, minibatch 1000/1000, validation error 47.730081"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 61, minibatch 1000/1000, validation error 47.727035"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 62, minibatch 1000/1000, validation error 48.520687"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 63, minibatch 1000/1000, validation error 47.991678"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 64, minibatch 1000/1000, validation error 47.829853"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 65, minibatch 1000/1000, validation error 47.994625"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 66, minibatch 1000/1000, validation error 47.139164"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 67, minibatch 1000/1000, validation error 46.980559"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 68, minibatch 1000/1000, validation error 47.084835"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 69, minibatch 1000/1000, validation error 46.561153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 70, minibatch 1000/1000, validation error 46.467581"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 71, minibatch 1000/1000, validation error 45.822361"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 72, minibatch 1000/1000, validation error 46.134257"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 73, minibatch 1000/1000, validation error 45.949400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 74, minibatch 1000/1000, validation error 45.830456"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 75, minibatch 1000/1000, validation error 45.105967"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 76, minibatch 1000/1000, validation error 47.058082"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 77, minibatch 1000/1000, validation error 46.558477"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 78, minibatch 1000/1000, validation error 46.463252"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 79, minibatch 1000/1000, validation error 45.261663"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 80, minibatch 1000/1000, validation error 45.608548"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 81, minibatch 1000/1000, validation error 45.562239"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 82, minibatch 1000/1000, validation error 44.067482"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 83, minibatch 1000/1000, validation error 44.359301"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 84, minibatch 1000/1000, validation error 42.495238"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 85, minibatch 1000/1000, validation error 42.931547"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 86, minibatch 1000/1000, validation error 42.258558"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 87, minibatch 1000/1000, validation error 43.158978"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 88, minibatch 1000/1000, validation error 41.412842"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 89, minibatch 1000/1000, validation error 43.258433"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 90, minibatch 1000/1000, validation error 41.232949"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 91, minibatch 1000/1000, validation error 42.556286"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 92, minibatch 1000/1000, validation error 40.521703"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 93, minibatch 1000/1000, validation error 41.759944"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 94, minibatch 1000/1000, validation error 39.421062"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 95, minibatch 1000/1000, validation error 40.573681"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 96, minibatch 1000/1000, validation error 39.091380"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 97, minibatch 1000/1000, validation error 40.424168"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 98, minibatch 1000/1000, validation error 39.127969"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 99, minibatch 1000/1000, validation error 40.314451"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "... building the model\n",
        "... training"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 0, minibatch 1000/1000, validation error 679.203003"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 1, minibatch 1000/1000, validation error 411.898600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 2, minibatch 1000/1000, validation error 322.963638"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 3, minibatch 1000/1000, validation error 272.721647"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 4, minibatch 1000/1000, validation error 239.834030"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 5, minibatch 1000/1000, validation error 216.538899"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 6, minibatch 1000/1000, validation error 198.764621"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 7, minibatch 1000/1000, validation error 184.861713"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 8, minibatch 1000/1000, validation error 173.657136"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 9, minibatch 1000/1000, validation error 164.401528"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 10, minibatch 1000/1000, validation error 156.532370"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 11, minibatch 1000/1000, validation error 149.751992"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 12, minibatch 1000/1000, validation error 143.830895"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 13, minibatch 1000/1000, validation error 138.604813"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 14, minibatch 1000/1000, validation error 133.917853"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 15, minibatch 1000/1000, validation error 129.652454"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 16, minibatch 1000/1000, validation error 125.817895"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 17, minibatch 1000/1000, validation error 122.363665"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 18, minibatch 1000/1000, validation error 119.190314"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 19, minibatch 1000/1000, validation error 116.262196"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 20, minibatch 1000/1000, validation error 113.559768"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 21, minibatch 1000/1000, validation error 111.053205"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 22, minibatch 1000/1000, validation error 108.732010"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 23, minibatch 1000/1000, validation error 106.574612"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 24, minibatch 1000/1000, validation error 104.576348"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 25, minibatch 1000/1000, validation error 102.701769"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 26, minibatch 1000/1000, validation error 100.952339"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 27, minibatch 1000/1000, validation error 99.310502"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 28, minibatch 1000/1000, validation error 97.773923"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 29, minibatch 1000/1000, validation error 96.330503"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 30, minibatch 1000/1000, validation error 94.972270"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 31, minibatch 1000/1000, validation error 93.674079"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 32, minibatch 1000/1000, validation error 92.445626"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 33, minibatch 1000/1000, validation error 91.281130"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 34, minibatch 1000/1000, validation error 90.177479"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 35, minibatch 1000/1000, validation error 89.126571"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 36, minibatch 1000/1000, validation error 88.104502"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 37, minibatch 1000/1000, validation error 87.105164"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 38, minibatch 1000/1000, validation error 86.177914"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 39, minibatch 1000/1000, validation error 85.288178"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 40, minibatch 1000/1000, validation error 84.449817"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 41, minibatch 1000/1000, validation error 83.655406"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 42, minibatch 1000/1000, validation error 82.902074"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 43, minibatch 1000/1000, validation error 82.153378"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 44, minibatch 1000/1000, validation error 81.420409"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 45, minibatch 1000/1000, validation error 80.717565"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 46, minibatch 1000/1000, validation error 80.040881"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 47, minibatch 1000/1000, validation error 79.386856"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 48, minibatch 1000/1000, validation error 78.754685"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 49, minibatch 1000/1000, validation error 78.143826"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 50, minibatch 1000/1000, validation error 77.564199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 51, minibatch 1000/1000, validation error 77.011846"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 52, minibatch 1000/1000, validation error 76.470331"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 53, minibatch 1000/1000, validation error 75.956690"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 54, minibatch 1000/1000, validation error 75.468822"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 55, minibatch 1000/1000, validation error 74.981046"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 56, minibatch 1000/1000, validation error 74.515787"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 57, minibatch 1000/1000, validation error 74.065617"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 58, minibatch 1000/1000, validation error 73.624463"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 59, minibatch 1000/1000, validation error 73.195992"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 60, minibatch 1000/1000, validation error 72.779986"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 61, minibatch 1000/1000, validation error 72.379252"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 62, minibatch 1000/1000, validation error 71.986618"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 63, minibatch 1000/1000, validation error 71.601255"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 64, minibatch 1000/1000, validation error 71.227666"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 65, minibatch 1000/1000, validation error 70.863311"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 66, minibatch 1000/1000, validation error 70.513927"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 67, minibatch 1000/1000, validation error 70.168817"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 68, minibatch 1000/1000, validation error 69.833733"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 69, minibatch 1000/1000, validation error 69.501433"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 70, minibatch 1000/1000, validation error 69.179752"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 71, minibatch 1000/1000, validation error 68.862898"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 72, minibatch 1000/1000, validation error 68.555092"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 73, minibatch 1000/1000, validation error 68.247003"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 74, minibatch 1000/1000, validation error 67.947488"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 75, minibatch 1000/1000, validation error 67.656638"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 76, minibatch 1000/1000, validation error 67.372842"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 77, minibatch 1000/1000, validation error 67.093501"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 78, minibatch 1000/1000, validation error 66.805313"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 79, minibatch 1000/1000, validation error 66.527742"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 80, minibatch 1000/1000, validation error 66.257954"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 81, minibatch 1000/1000, validation error 65.996329"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 82, minibatch 1000/1000, validation error 65.740196"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 83, minibatch 1000/1000, validation error 65.482942"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 84, minibatch 1000/1000, validation error 65.234082"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 85, minibatch 1000/1000, validation error 64.985690"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 86, minibatch 1000/1000, validation error 64.743271"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 87, minibatch 1000/1000, validation error 64.504356"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 88, minibatch 1000/1000, validation error 64.267592"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 89, minibatch 1000/1000, validation error 64.040040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 90, minibatch 1000/1000, validation error 63.813944"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 91, minibatch 1000/1000, validation error 63.593409"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 92, minibatch 1000/1000, validation error 63.379183"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 93, minibatch 1000/1000, validation error 63.168188"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 94, minibatch 1000/1000, validation error 62.965470"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 95, minibatch 1000/1000, validation error 62.766480"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 96, minibatch 1000/1000, validation error 62.569405"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 97, minibatch 1000/1000, validation error 62.370870"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 98, minibatch 1000/1000, validation error 62.175143"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 99, minibatch 1000/1000, validation error 61.979767"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import poisson\n",
      "rv, s, _ = valid_data_1\n",
      "print rv[0]\n",
      "def lik_means(s_1, s_2, c_0=.5, c_1=.5, sprefs=sprefs, sigtc_sq=sigtc_sq):\n",
      "    c_0 = c_1 = .5\n",
      "    sprefs_data = np.tile(sprefs, (len(s1_grid), 1))\n",
      "    s_0t = np.exp(-np.square((np.transpose(np.tile(s_1, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_0 = c_0 * s_0t.T\n",
      "    s_1t = np.exp(-np.square((np.transpose(np.tile(s_2, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_1 = c_1 * s_1t.T\n",
      "    r = r_max * (stim_0 + stim_1)\n",
      "    return r.T\n",
      "def posterior_mean(r, num_s1):\n",
      "    grid = np.linspace(-60, 60, num_s1)\n",
      "    s1_grid = np.concatenate([[grid[i]]*(num_s1-i) for i in range(num_s1)])\n",
      "    s2_grid = np.concatenate([grid[i:num_s1+1] for i in range(num_s1)])\n",
      "    means = lik_means(s1_grid, s2_grid)\n",
      "    ns_liks = poisson.pmf(r, mu=means)\n",
      "    stim_liks = np.prod(ns_liks, axis=1)\n",
      "    #p_s = 2/14400\n",
      "    #logp_s = np.log(p_s)\n",
      "    logp_s = -3.8573325\n",
      "    loglik = np.sum(np.log(ns_liks), axis=1)\n",
      "    post1 = sum(s1_grid * np.exp(loglik + logp_s)/sum(np.exp(loglik + logp_s)))\n",
      "    post2 = sum(s2_grid * np.exp(loglik + logp_s)/sum(np.exp(loglik + logp_s)))\n",
      "    return post1, post2\n",
      "print s*90\n",
      "print posterior_mean(rv[0], 100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0.  0.  0.  0.  2.  0.  4.  1.  3.  5.  4.  1.  4.  4.  1.  1.\n",
        "  1.  2.  2.  0.  1.  6.  0.  3.  4.  8.  1.  3.  6.  3.  5.  1.  2.  1.\n",
        "  0.  0.  0.  0.  0.  0.  0.]\n",
        "[[  4.73531765  47.82741345]\n",
        " [-32.23526211 -20.11736552]\n",
        " [-51.77456905  54.49828449]\n",
        " ..., \n",
        " [-23.92455358  26.55224834]\n",
        " [-44.11258233  56.85774981]\n",
        " [-50.02939683  34.60282354]]\n",
        "(1.4695540418269606, 47.854583076390831)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 152
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s_arr = [-50, -30, -10, 0, 10, 30, 50]\n",
      "def test_combs(s_arr):\n",
      "    l_sarr = len(s_arr)\n",
      "    nn = [[None] * l_sarr for k in range(l_sarr)]\n",
      "    opt = [[None] * l_sarr for k in range(l_sarr)]\n",
      "    for i in range(l_sarr):\n",
      "        for j in range(i+1, l_sarr):\n",
      "            s1 = s_arr[i]\n",
      "            s2 = s_arr[j]\n",
      "            nn[i][j], opt[i][j] = test_models(s1, s2, nn2, nnx2, sm)\n",
      "    return nn, opt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rv, s, _ = valid_data_1\n",
      "s=s.T*90\n",
      "opt_preds = fit_optimal(rv, sm, s)\n",
      "mse_opt = np.mean((opt_preds[0] - s[0]) ** 2 + (opt_preds[1] - s[1]) ** 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "#plt.plot(range(20, 100), valid_mse_plain[20:100], c='b')\n",
      "#plt.plot(range(20, 100), valid_mse_rms[20:100], c='y')\n",
      "plt.plot(valid_mse_plain, c='b', label='Vanilla')\n",
      "plt.plot(valid_mse_rms, c='k', label='RMSProp')\n",
      "plt.plot(np.ones(100) * mse_opt, c='r', label='MAP')\n",
      "plt.legend()\n",
      "plt.xlabel('epoch')\n",
      "plt.ylabel('MSE')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s1 = -30\n",
      "num_deltas = 60\n",
      "#num_deltas = 5\n",
      "\n",
      "mean_s1_nn_rms = [None] * num_deltas\n",
      "mean_s2_nn_rms = [None] * num_deltas\n",
      "bias_s1_nn_rms = [None] * num_deltas\n",
      "bias_s2_nn_rms = [None] * num_deltas\n",
      "var_s1_nn_rms = [None] * num_deltas\n",
      "var_s2_nn_rms = [None] * num_deltas\n",
      "corr_nn_rms = [None] * num_deltas\n",
      "cov_nn_rms = [None] * num_deltas\n",
      "\n",
      "\"\"\"\n",
      "mean_s1_nn_rms_m9 = [None] * num_deltas\n",
      "mean_s2_nn_rms_m9 = [None] * num_deltas\n",
      "bias_s1_nn_rms_m9 = [None] * num_deltas\n",
      "bias_s2_nn_rms_m9 = [None] * num_deltas\n",
      "var_s1_nn_rms_m9 = [None] * num_deltas\n",
      "var_s2_nn_rms_m9 = [None] * num_deltas\n",
      "corr_nn_rms_m9 = [None] * num_deltas\n",
      "cov_nn_rms_m9 = [None] * num_deltas\n",
      "\n",
      "mean_s1_nn_rms_m9f = [None] * num_deltas\n",
      "mean_s2_nn_rms_m9f = [None] * num_deltas\n",
      "bias_s1_nn_rms_m9f = [None] * num_deltas\n",
      "bias_s2_nn_rms_m9f = [None] * num_deltas\n",
      "var_s1_nn_rms_m9f = [None] * num_deltas\n",
      "var_s2_nn_rms_m9f = [None] * num_deltas\n",
      "corr_nn_rms_m9f = [None] * num_deltas\n",
      "cov_nn_rms_m9f = [None] * num_deltas\n",
      "\n",
      "mean_s1_nn_rms_m95 = [None] * num_deltas\n",
      "mean_s2_nn_rms_m95 = [None] * num_deltas\n",
      "bias_s1_nn_rms_m95 = [None] * num_deltas\n",
      "bias_s2_nn_rms_m95 = [None] * num_deltas\n",
      "var_s1_nn_rms_m95 = [None] * num_deltas\n",
      "var_s2_nn_rms_m95 = [None] * num_deltas\n",
      "corr_nn_rms_m95 = [None] * num_deltas\n",
      "cov_nn_rms_m95 = [None] * num_deltas\n",
      "\n",
      "mean_s1_nn_rms_m99 = [None] * num_deltas\n",
      "mean_s2_nn_rms_m99 = [None] * num_deltas\n",
      "bias_s1_nn_rms_m99 = [None] * num_deltas\n",
      "bias_s2_nn_rms_m99 = [None] * num_deltas\n",
      "var_s1_nn_rms_m99 = [None] * num_deltas\n",
      "var_s2_nn_rms_m99 = [None] * num_deltas\n",
      "corr_nn_rms_m99 = [None] * num_deltas\n",
      "cov_nn_rms_m99 = [None] * num_deltas\n",
      "\n",
      "mean_s1_nn_plain = [None] * num_deltas\n",
      "mean_s2_nn_plain = [None] * num_deltas\n",
      "bias_s1_nn_plain = [None] * num_deltas\n",
      "bias_s2_nn_plain = [None] * num_deltas\n",
      "var_s1_nn_plain = [None] * num_deltas\n",
      "var_s2_nn_plain = [None] * num_deltas\n",
      "corr_nn_plain = [None] * num_deltas\n",
      "cov_nn_plain = [None] * num_deltas\n",
      "\"\"\"\n",
      "\n",
      "mean_s1_opt = [None] * num_deltas\n",
      "mean_s2_opt = [None] * num_deltas\n",
      "bias_s1_opt = [None] * num_deltas\n",
      "bias_s2_opt = [None] * num_deltas\n",
      "var_s1_opt = [None] * num_deltas\n",
      "var_s2_opt = [None] * num_deltas\n",
      "corr_opt = [None] * num_deltas\n",
      "cov_opt = [None] * num_deltas\n",
      "\n",
      "var_s1_fisher = [None] * num_deltas\n",
      "var_s2_fisher = [None] * num_deltas\n",
      "cov_fisher = [None] * num_deltas\n",
      "\n",
      "for delta_s in range(num_deltas):\n",
      "    #test_data = generate_s_data(s1, s1 + delta_s + 1, 3000)\n",
      "    test_data = generate_s_data(s1, s1 + delta_s, 300)\n",
      "    nn_preds_rms, _ = test_nn(nn_rms, nnx_rms, test_data)\n",
      "    \"\"\"\n",
      "    nn_preds_rms_m9, _ = test_nn(nn_rms_m9, nnx_rms_m9, test_data)\n",
      "    nn_preds_rms_m9f, _ = test_nn(nn_rms_m9f, nnx_rms_m9f, test_data)\n",
      "    nn_preds_rms_m95, _ = test_nn(nn_rms_m95, nnx_rms_m95, test_data)\n",
      "    nn_preds_rms_m99, _ = test_nn(nn_rms_m99, nnx_rms_m99, test_data)\n",
      "    nn_preds_plain, _ = test_nn(nn_plain, nnx_plain, test_data)\n",
      "    \"\"\"\n",
      "    r, _, _ = test_data\n",
      "    opt_preds = fit_optimal(r, sm, init={'s_1':s1, 's_2':s1 + delta_s + 1})\n",
      "    \n",
      "    nn_preds_rms = nn_preds_rms.T * 90\n",
      "    \"\"\"\n",
      "    nn_preds_rms_m9 = nn_preds_rms_m9.T * 90\n",
      "    nn_preds_rms_m9f = nn_preds_rms_m9f.T * 90\n",
      "    nn_preds_rms_m95 = nn_preds_rms_m95.T * 90\n",
      "    nn_preds_rms_m99 = nn_preds_rms_m99.T * 90\n",
      "    nn_preds_plain = nn_preds_plain.T * 90\n",
      "    \"\"\"\n",
      "\n",
      "    nn_stats_rms = get_statistics(s1, s1 + delta_s, nn_preds_rms)\n",
      "    \"\"\"\n",
      "    nn_stats_rms_m9 = get_statistics(s1, s1 + delta_s, nn_preds_rms_m9)\n",
      "    nn_stats_rms_m9f = get_statistics(s1, s1 + delta_s, nn_preds_rms_m9f)\n",
      "    nn_stats_rms_m95 = get_statistics(s1, s1 + delta_s, nn_preds_rms_m95)\n",
      "    nn_stats_rms_m99 = get_statistics(s1, s1 + delta_s, nn_preds_rms_m99)\n",
      "    nn_stats_plain = get_statistics(s1, s1 + delta_s, nn_preds_plain)\n",
      "    \"\"\"\n",
      "    opt_stats = get_statistics(s1, s1 + delta_s, opt_preds)\n",
      "    \n",
      "    if delta_s > 0:\n",
      "        FI = fisher_inf(s1, s1 + delta_s, .5, .5)\n",
      "        var_s1_fisher[delta_s] = FI[0, 0]\n",
      "        var_s2_fisher[delta_s] = FI[1, 1]\n",
      "        cov_fisher[delta_s] = FI[0, 1]\n",
      "    \n",
      "    mean_s1_nn_rms[delta_s] = nn_stats_rms['mean_s1']\n",
      "    mean_s2_nn_rms[delta_s] = nn_stats_rms['mean_s2']\n",
      "    bias_s1_nn_rms[delta_s] = nn_stats_rms['bias_s1']\n",
      "    bias_s2_nn_rms[delta_s] = nn_stats_rms['bias_s2']\n",
      "    var_s1_nn_rms[delta_s] = nn_stats_rms['var_s1']\n",
      "    var_s2_nn_rms[delta_s] = nn_stats_rms['var_s2']\n",
      "    corr_nn_rms[delta_s] = nn_stats_rms['corr']\n",
      "    cov_nn_rms[delta_s] = nn_stats_rms['cov']\n",
      "    \n",
      "    mean_s1_opt[delta_s] = opt_stats['mean_s1']\n",
      "    mean_s2_opt[delta_s] = opt_stats['mean_s2']\n",
      "    bias_s1_opt[delta_s] = opt_stats['bias_s1']\n",
      "    bias_s2_opt[delta_s] = opt_stats['bias_s2']\n",
      "    var_s1_opt[delta_s] = opt_stats['var_s1']\n",
      "    var_s2_opt[delta_s] = opt_stats['var_s2']\n",
      "    corr_opt[delta_s] = opt_stats['corr']\n",
      "    cov_opt[delta_s] = opt_stats['cov']\n",
      "    \n",
      "    \"\"\"\n",
      "    mean_s1_nn_rms_m9[delta_s] = nn_stats_rms_m9['mean_s1']\n",
      "    mean_s2_nn_rms_m9[delta_s] = nn_stats_rms_m9['mean_s2']\n",
      "    bias_s1_nn_rms_m9[delta_s] = nn_stats_rms_m9['bias_s1']\n",
      "    bias_s2_nn_rms_m9[delta_s] = nn_stats_rms_m9['bias_s2']\n",
      "    var_s1_nn_rms_m9[delta_s] = nn_stats_rms_m9['var_s1']\n",
      "    var_s2_nn_rms_m9[delta_s] = nn_stats_rms_m9['var_s2']\n",
      "    corr_nn_rms_m9[delta_s] = nn_stats_rms_m9['corr']\n",
      "    cov_nn_rms_m9[delta_s] = nn_stats_rms_m9['cov']\n",
      "    \n",
      "    mean_s1_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['mean_s1']\n",
      "    mean_s2_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['mean_s2']\n",
      "    bias_s1_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['bias_s1']\n",
      "    bias_s2_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['bias_s2']\n",
      "    var_s1_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['var_s1']\n",
      "    var_s2_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['var_s2']\n",
      "    corr_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['corr']\n",
      "    cov_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['cov']\n",
      "    \n",
      "    mean_s1_nn_rms_m95[delta_s] = nn_stats_rms_m95['mean_s1']\n",
      "    mean_s2_nn_rms_m95[delta_s] = nn_stats_rms_m95['mean_s2']\n",
      "    bias_s1_nn_rms_m95[delta_s] = nn_stats_rms_m95['bias_s1']\n",
      "    bias_s2_nn_rms_m95[delta_s] = nn_stats_rms_m95['bias_s2']\n",
      "    var_s1_nn_rms_m95[delta_s] = nn_stats_rms_m95['var_s1']\n",
      "    var_s2_nn_rms_m95[delta_s] = nn_stats_rms_m95['var_s2']\n",
      "    corr_nn_rms_m95[delta_s] = nn_stats_rms_m95['corr']\n",
      "    cov_nn_rms_m95[delta_s] = nn_stats_rms_m95['cov']\n",
      "    \n",
      "    mean_s1_nn_rms_m99[delta_s] = nn_stats_rms_m99['mean_s1']\n",
      "    mean_s2_nn_rms_m99[delta_s] = nn_stats_rms_m99['mean_s2']\n",
      "    bias_s1_nn_rms_m99[delta_s] = nn_stats_rms_m99['bias_s1']\n",
      "    bias_s2_nn_rms_m99[delta_s] = nn_stats_rms_m99['bias_s2']\n",
      "    var_s1_nn_rms_m99[delta_s] = nn_stats_rms_m99['var_s1']\n",
      "    var_s2_nn_rms_m99[delta_s] = nn_stats_rms_m99['var_s2']\n",
      "    corr_nn_rms_m99[delta_s] = nn_stats_rms_m99['corr']\n",
      "    cov_nn_rms_m99[delta_s] = nn_stats_rms_m99['cov']\n",
      "    \n",
      "    mean_s1_nn_plain[delta_s] = nn_stats_plain['mean_s1']\n",
      "    mean_s2_nn_plain[delta_s] = nn_stats_plain['mean_s2']\n",
      "    bias_s1_nn_plain[delta_s] = nn_stats_plain['bias_s1']\n",
      "    bias_s2_nn_plain[delta_s] = nn_stats_plain['bias_s2']\n",
      "    var_s1_nn_plain[delta_s] = nn_stats_plain['var_s1']\n",
      "    var_s2_nn_plain[delta_s] = nn_stats_plain['var_s2']\n",
      "    corr_nn_plain[delta_s] = nn_stats_plain['corr']\n",
      "    cov_nn_plain[delta_s] = nn_stats_plain['cov']\n",
      "    \"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.patches as mpatches\n",
      "s = range(-30, 30)\n",
      "neg_sd_nn_rms = mean_s1_nn_rms - np.sqrt(var_s1_nn_rms)\n",
      "pos_sd_nn_rms = mean_s1_nn_rms + np.sqrt(var_s1_nn_rms)\n",
      "neg_sd_opt = mean_s1_opt - np.sqrt(var_s1_opt)\n",
      "pos_sd_opt = mean_s1_opt + np.sqrt(var_s1_opt)\n",
      "plt.rc('text', usetex=True)\n",
      "plt.figure(figsize=(10,10))\n",
      "plt.fill_between(s, pos_sd_nn_rms, neg_sd_nn_rms, facecolor='k', alpha=0.5, edgecolor=\"None\", label=\"MLE\")\n",
      "plt.fill_between(s, pos_sd_opt, neg_sd_opt, facecolor='m', alpha=0.5, edgecolor=\"None\", label=\"NN\")\n",
      "plt.ylim([-40,40])\n",
      "plt.xlabel(r'$s_2$',fontsize=30)\n",
      "plt.ylabel(r'\\hat{s_1}',fontsize=30)\n",
      "nn_patch = mpatches.Patch(color='k', label='NN')\n",
      "mle_patch = mpatches.Patch(color='m', label='MLE')\n",
      "plt.legend(handles=[nn_patch, mle_patch])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = range(-30, 30)\n",
      "neg_sd_nn_rms = mean_s2_nn_rms - np.sqrt(var_s2_nn_rms)\n",
      "pos_sd_nn_rms = mean_s2_nn_rms + np.sqrt(var_s2_nn_rms)\n",
      "neg_sd_opt = mean_s2_opt - np.sqrt(var_s2_opt)\n",
      "pos_sd_opt = mean_s2_opt + np.sqrt(var_s2_opt)\n",
      "plt.rc('text', usetex=True)\n",
      "plt.figure(figsize=(10,10))\n",
      "plt.fill_between(s, pos_sd_nn_rms, neg_sd_nn_rms, facecolor='k', alpha=0.5, edgecolor=\"None\")\n",
      "plt.fill_between(s, pos_sd_opt, neg_sd_opt, facecolor='m', alpha=0.5, edgecolor=\"None\")\n",
      "plt.xlabel(r'$s_2$',fontsize=30)\n",
      "plt.ylabel(r'\\hat{s_2}',fontsize=30)\n",
      "plt.ylim([-40,40])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5)\n",
      "#RMS\n",
      "ax1.plot(range(1, 61), bias_s1_nn_rms, c='k', label='NN')\n",
      "ax2.plot(range(1, 61), bias_s2_nn_rms, c='k', label='NN')\n",
      "ax3.plot(range(1, 61), var_s1_nn_rms, c='k', label='NN')\n",
      "ax4.plot(range(1, 61), var_s2_nn_rms, c='k', label='NN')\n",
      "ax5.plot(range(1, 61), cov_nn_rms, c='k', label='NN')\n",
      "#optimal in red\n",
      "ax1.plot(range(1, 61), bias_s1_opt, c='m', label='MLE')\n",
      "ax2.plot(range(1, 61), bias_s2_opt, c='m', label='MLE')\n",
      "ax3.plot(range(1, 61), var_s1_opt, c='m', label='MLE')\n",
      "ax4.plot(range(1, 61), var_s2_opt, c='m', label='MLE')\n",
      "ax5.plot(range(1, 61), cov_opt, c='m', label='MLE')\n",
      "\"\"\"\n",
      "#RMS with momentum\n",
      "ax1.plot(range(1, 61), bias_s1_nn_rms_m9, c='m', label='NN')\n",
      "ax2.plot(range(1, 61), bias_s2_nn_rms_m9, c='m', label='NN')\n",
      "ax3.plot(range(1, 61), var_s1_nn_rms_m9, c='m', label='NN')\n",
      "ax4.plot(range(1, 61), var_s2_nn_rms_m9, c='m', label='NN')\n",
      "ax5.plot(range(1, 61), cov_nn_rms_m9, c='m', label='NN')\n",
      "#RMS with momentum\n",
      "ax1.plot(range(1, 61), bias_s1_nn_rms_m95, c='c', label='NN')\n",
      "ax2.plot(range(1, 61), bias_s2_nn_rms_m95, c='c', label='NN')\n",
      "ax3.plot(range(1, 61), var_s1_nn_rms_m95, c='c', label='NN')\n",
      "ax4.plot(range(1, 61), var_s2_nn_rms_m95, c='c', label='NN')\n",
      "ax5.plot(range(1, 61), cov_nn_rms_m95, c='c', label='NN')\n",
      "#RMS with momentum\n",
      "ax1.plot(range(1, 61), bias_s1_nn_rms_m99, c='k', label='NN')\n",
      "ax2.plot(range(1, 61), bias_s2_nn_rms_m99, c='k', label='NN')\n",
      "ax3.plot(range(1, 61), var_s1_nn_rms_m99, c='k', label='NN')\n",
      "ax4.plot(range(1, 61), var_s2_nn_rms_m99, c='k', label='NN')\n",
      "ax5.plot(range(1, 61), cov_nn_rms_m99, c='k', label='NN')\n",
      "#RMS\n",
      "ax1.plot(range(1, 61), bias_s1_nn_rms_m9f, c='r', label='NN')\n",
      "ax2.plot(range(1, 61), bias_s2_nn_rms_m9f, c='r', label='NN')\n",
      "ax3.plot(range(1, 61), var_s1_nn_rms_m9f, c='r', label='NN')\n",
      "ax4.plot(range(1, 61), var_s2_nn_rms_m9f, c='r', label='NN')\n",
      "ax5.plot(range(1, 61), cov_nn_rms_m9f, c='r', label='NN')\n",
      "#Plain\n",
      "ax1.plot(range(1, 61), bias_s1_nn_plain, c='b', label='NN')\n",
      "ax2.plot(range(1, 61), bias_s2_nn_plain, c='b', label='NN')\n",
      "ax3.plot(range(1, 61), var_s1_nn_plain, c='b', label='NN')\n",
      "ax4.plot(range(1, 61), var_s2_nn_plain, c='b', label='NN')\n",
      "ax5.plot(range(1, 61), cov_nn_plain, c='b', label='NN')\n",
      "\"\"\"\n",
      "#Fisher information in green\n",
      "ax3.plot(range(7, 61), var_s1_fisher[6:60], c='orange', label='Fisher')\n",
      "ax4.plot(range(7, 61), var_s2_fisher[6:60], c='orange', label='Fisher')\n",
      "ax5.plot(range(7, 61), cov_fisher[6:60], c='orange', label='Fisher')\n",
      "ax1.locator_params(axis = 'y', nbins = 4)\n",
      "ax2.locator_params(axis = 'y', nbins = 4)\n",
      "ax3.locator_params(axis = 'y', nbins = 4)\n",
      "ax4.locator_params(axis = 'y', nbins = 4)\n",
      "ax5.locator_params(axis = 'y', nbins = 4)\n",
      "#ax1.legend()\n",
      "ax1.set_title(\"Bias $s_1$\")\n",
      "ax2.set_title(\"Bias $s_2$\")\n",
      "ax3.set_title(r'Variance $s_1$')\n",
      "ax4.set_title(r'Variance $s_2$')\n",
      "ax5.set_title('Covariance')\n",
      "ax5.set_xlabel(r'$\\Delta$ s')\n",
      "f.set_size_inches(10,10)\n",
      "ax3.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}