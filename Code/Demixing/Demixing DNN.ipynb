{
 "metadata": {
  "name": "",
  "signature": "sha256:7a42c72131d74ec3d54add347f2a24a8f3c606cd95d605638bfa969f39de72b6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import math\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "import pystan\n",
      "import matplotlib.pyplot as plt\n",
      "import argparse\n",
      "\n",
      "nneuron = 61\n",
      "min_angle = -90\n",
      "max_angle = 90\n",
      "sprefs = np.linspace(min_angle, max_angle, nneuron)\n",
      "ndata = 3000\n",
      "eps = np.finfo(np.float64).eps\n",
      "\n",
      "r_max = 10\n",
      "sigtc_sq = float(10**2)\n",
      "sigtc = 10\n",
      "c_50 = 13.1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c1 = np.ones((2, ndata/2)) * .3\n",
      "c2 = np.ones((2, ndata/2)) * .7\n",
      "np.concatenate((np.ones((2, ndata/2)) * .3, np.ones((2, ndata/2)) * .7), axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def random_s(ndata, sort):\n",
      "    s = np.random.rand(2, ndata) * 120 - 60\n",
      "    if sort:\n",
      "        s = np.sort(s, axis=0)\n",
      "    return s[0], s[1]\n",
      "\n",
      "def generate_trainset(ndata, r_max=10):\n",
      "    s_0, s_1 = random_s(ndata, True)\n",
      "    c_0, c_1 = np.ones((2, ndata)) * .5\n",
      "    r, s, c = generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True, s_0, s_1, c_0, c_1)\n",
      "    return r, s, c\n",
      "\n",
      "def generate_trainset_c(ndata, low=.5, high=.5, r_max=10):\n",
      "    s_0, s_1 = random_s(ndata, True)\n",
      "    c_0, c_1 = np.concatenate((np.ones((2, ndata/2)) * low, np.ones((2, ndata/2)) * high), axis=1)\n",
      "    r, s, c = generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True, s_0, s_1, c_0, c_1)\n",
      "    return r, s, c\n",
      "\n",
      "def generate_testset_c(ndata, r_max=10):\n",
      "    s_0, s_1 = random_s(ndata, True)\n",
      "    c_0, c_1 = np.rand((2, ndata))\n",
      "    r, s, c = generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True, s_0, s_1, c_0, c_1)\n",
      "    return r, s, c\n",
      "\n",
      "def generate_s1set(ndata):\n",
      "    s_0, s_1 = random_s(ndata, True)\n",
      "    c_0 = np.ones(ndata)\n",
      "    c_1 = np.zeros(ndata)\n",
      "    r, s, c = generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True, s_0, s_1, c_0, c_1)\n",
      "    return r, s, c\n",
      "    \n",
      "def generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, noise, sort, s_0, s_1, c_0, c_1):\n",
      "    c_rms = np.sqrt(np.square(c_0) + np.square(c_1))\n",
      "    sprefs_data = np.tile(sprefs, (ndata, 1))\n",
      "    s_0t = np.exp(-np.square((np.transpose(np.tile(s_0, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_0 = c_0 * s_0t.T\n",
      "    s_1t = np.exp(-np.square((np.transpose(np.tile(s_1, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_1 = c_1 * s_1t.T\n",
      "    #r = r_max * (stim_0 + stim_1)/(c_50 + c_rms)\n",
      "    r = r_max * (stim_0 + stim_1)\n",
      "    r = r.T\n",
      "    s = np.array((s_0, s_1)).T\n",
      "    s = s/90\n",
      "    c = np.array((c_0, c_1)).T\n",
      "    if noise == \"poisson\":\n",
      "        r = np.random.poisson(r) + 0.0\n",
      "    return r, s, c\n",
      "\n",
      "def generate_s_data(stim_0, stim_1, ndata, con_0=.5, con_1=.5, r_max=10):\n",
      "    #c_0, c_1 = np.ones((2, ndata)) * .5\n",
      "    c_0 = np.ones(ndata) * con_0\n",
      "    c_1 = np.ones(ndata) * con_1\n",
      "    s_0, s_1 = np.ones((2, ndata))\n",
      "    s_0 = s_0 * stim_0\n",
      "    s_1 = s_1 * stim_1\n",
      "    r, s, c = generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True, s_0, s_1, c_0, c_1)\n",
      "    return r, s, c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "neurons_code = \"\"\"\n",
      "    data {\n",
      "        int<lower=0> N; // number of neurons\n",
      "        int r[N]; // neural response\n",
      "        real sprefs[N]; // preferred stimuli\n",
      "        real<lower=0> c_1;\n",
      "        real<lower=0> c_2;\n",
      "        int r_max;\n",
      "        //real c_rms;\n",
      "        //real c_50;\n",
      "        //real<lower=0> sig_tc;\n",
      "        real<lower=0> sigtc_sq;\n",
      "    }\n",
      "    parameters {\n",
      "        real s_1;\n",
      "        //real s_2;\n",
      "        real<lower=s_1> s_2;\n",
      "    }\n",
      "    transformed parameters {\n",
      "        real lambda[N];\n",
      "        for (n in 1:N)\n",
      "            // lambda[n] <- r_max * ((c_1 * exp(normal_log(s_1, sprefs[n], sig_tc)) + c_2 * exp(normal_log(s_2, sprefs[n], sig_tc)))/(c_rms + c_50));\n",
      "            // lambda[n] <- r_max * (c_1 * exp(normal_log(s_1, sprefs[n], sig_tc)) + c_2 * exp(normal_log(s_2, sprefs[n], sig_tc)));\n",
      "            lambda[n] <- r_max * (c_1 * exp(- square(s_1 - sprefs[n])/(2 * sigtc_sq)) + c_2 * exp(- square(s_2 - sprefs[n])/(2 * sigtc_sq)));\n",
      "    }\n",
      "    model {\n",
      "        s_1 ~ uniform(-60, 60);\n",
      "        //s_2 ~ uniform(-60, 60);\n",
      "        s_2 ~ uniform(s_1, 60);\n",
      "        r ~ poisson(lambda);\n",
      "    }\n",
      "    \"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fisher_inf(s_0, s_1, c_0, c_1, r_max=10):\n",
      "    fs_0 = np.exp(-np.square((np.transpose(np.tile(s_0, (nneuron, 1))) - sprefs))/(2 * sigtc_sq))[0]\n",
      "    qs_0 = r_max * c_0 * fs_0\n",
      "    df_s0 = ((-s_0 + sprefs)/sigtc_sq) * qs_0\n",
      "    fs_1 = np.exp(-np.square((np.transpose(np.tile(s_1, (nneuron, 1))) - sprefs))/(2 * sigtc_sq))[0]\n",
      "    qs_1 = r_max * c_1 * fs_1\n",
      "    df_s1 = ((-s_1 + sprefs)/sigtc_sq) * qs_1\n",
      "    Q = qs_0 + qs_1\n",
      "    Q_inv = 1/Q\n",
      "    J_11 = np.sum(np.square(df_s0) * Q_inv)\n",
      "    J_22 = np.sum(np.square(df_s1) * Q_inv)\n",
      "    J_12 = J_21 = np.sum(df_s0 * df_s1 * Q_inv)\n",
      "    fisher = np.linalg.inv([[J_11, J_12], [J_21, J_22]])\n",
      "    return fisher\n",
      "\n",
      "def fit_optimal(r, sm, s=None, N=61, init=None, sprefs=sprefs, sampling=False, sort=False, c_1=.5, c_2=.5, c_50=13.1, r_max=10, c_rms=0.707106781, sig_tc=10, sigtc_sq=10**2):\n",
      "    ndata = len(r)\n",
      "    neurons_dat = {'N': 61,\n",
      "                   'r': r[0].astype(int),\n",
      "                   'sprefs': sprefs,\n",
      "                   'c_1': .5,\n",
      "                   'c_2': .5,\n",
      "                   'c_50': 13.1,\n",
      "                   'r_max': r_max,\n",
      "                   'c_rms': 0.707106781,\n",
      "                   'sig_tc': 10,\n",
      "                   'sigtc_sq': sigtc_sq}\n",
      "\n",
      "    optimal = np.zeros((2, ndata))\n",
      "    print init\n",
      "    for i in range(len(r)):\n",
      "        neurons_dat['r'] = r[i].astype(int)\n",
      "        if sampling:\n",
      "            op = sm.sampling(data=neurons_dat)\n",
      "            optimal[0][i], optimal[1][i] = np.mean(op['s_1']), np.mean(op['s_2'])\n",
      "        else:\n",
      "            if s:\n",
      "                if s.any():\n",
      "                    init = {'s_1':max(s[0][i], -60 + eps), 's_2':min(s[1][i] + eps, 60 - eps)}\n",
      "                    print init\n",
      "                    op = sm.optimizing(data=neurons_dat, init=init)\n",
      "            elif not init:\n",
      "                op = sm.optimizing(data=neurons_dat)\n",
      "            else:     \n",
      "                op = sm.optimizing(data=neurons_dat, init=init)\n",
      "            optimal[0][i], optimal[1][i] = op['s_1'], op['s_2']\n",
      "        if sort:\n",
      "            optimal = np.sort(optimal, axis=0)\n",
      "    return optimal"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Multilayer ReLU net\n",
      "\"\"\"\n",
      "\n",
      "def relu(x):\n",
      "    return theano.tensor.switch(x<0, 0, x)\n",
      "\n",
      "class HiddenLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
      "                 activation=T.nnet.sigmoid):\n",
      "        \"\"\"\n",
      "        Typical hidden layer of a MLP: units are fully-connected and have\n",
      "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
      "        and the bias vector b is of shape (n_out,).\n",
      "\n",
      "        :type rng: np.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.dmatrix\n",
      "        :param input: a symbolic tensor of shape (n_examples, n_in)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: dimensionality of input\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of hidden units\n",
      "\n",
      "        :type activation: theano.Op or function\n",
      "        :param activation: Non linearity to be applied in the hidden\n",
      "                           layer\n",
      "        \"\"\"\n",
      "        self.input = input\n",
      "        if W is None:\n",
      "            W_values = (1/np.sqrt(n_in)) * np.random.randn(n_in, n_out)\n",
      "            \n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        if b is None:\n",
      "            b_values = np.zeros((n_out,), dtype=theano.config.floatX)\n",
      "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "\n",
      "        lin_output = T.dot(input, self.W) + self.b\n",
      "        self.output = (\n",
      "            lin_output if activation is None\n",
      "            else activation(lin_output)\n",
      "        )\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "class COMLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None):\n",
      "        \"\"\"\n",
      "        Layer with Center of Mass decoder\n",
      "        Params same as above\n",
      "        \"\"\"\n",
      "        self.input = input\n",
      "        if W is None:\n",
      "            W_values = (1/np.sqrt(n_in)) * np.random.randn(n_in, n_out)\n",
      "\n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        \n",
      "        self.ones = np.ones((n_in, n_out))\n",
      "        \n",
      "        self.output = T.dot(input, self.W)/T.dot(input, self.ones)\n",
      "        \n",
      "        # parameters of the model\n",
      "        self.params = [self.W]\n",
      "\n",
      "class MLP(object):\n",
      "\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
      "\n",
      "        :type rng: np.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: symbolic variable that describes the input of the\n",
      "        architecture (one minibatch)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: number of input units, the dimension of the space in\n",
      "        which the datapoints lie\n",
      "\n",
      "        :type n_hidden: int\n",
      "        :param n_hidden: number of hidden units\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "        which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        self.hiddenLayer1 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            #activation=T.nnet.sigmoid\n",
      "            activation=relu\n",
      "        )\n",
      "        \n",
      "        self.hiddenLayer2 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=self.hiddenLayer1.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out,\n",
      "            #activation=relu\n",
      "            activation=None\n",
      "        )\n",
      "        \n",
      "        self.y_pred = self.hiddenLayer2.output\n",
      "        \n",
      "        # the parameters of the model are the parameters of the two layers it is made out of\n",
      "        self.params = self.hiddenLayer1.params + self.hiddenLayer2.params\n",
      "    \n",
      "    def get_params(self):\n",
      "\n",
      "        params = {}\n",
      "        for param in self.params:\n",
      "            name = param.name\n",
      "            if name in params:\n",
      "                name = name, 2\n",
      "            params[name] = param.get_value()\n",
      "        return params\n",
      "    \n",
      "    def mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "    \n",
      "    def mse_s1(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean((self.y_pred[0] - y[0]) ** 2)\n",
      "    \n",
      "    def valid_mse(self, y):\n",
      "        return T.mean(((self.y_pred[0] * 90) - (y[0] * 90)) ** 2 + ((self.y_pred[1] * 90) - (y[1] * 90)) ** 2)\n",
      "    \n",
      "    def sym_mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean(((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "                      * ((self.y_pred[1] - y[0]) ** 2 + (self.y_pred[0] - y[1]) ** 2))\n",
      "        \n",
      "class COMMLP(object):\n",
      "\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"\n",
      "        Params same as above\n",
      "        \"\"\"\n",
      "\n",
      "        self.hiddenLayer1 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            activation=T.nnet.sigmoid\n",
      "        )\n",
      "        \n",
      "        self.hiddenLayer2 = COMLayer(\n",
      "            rng=rng,\n",
      "            input=self.hiddenLayer1.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out,\n",
      "        )\n",
      "        \n",
      "        self.y_pred = self.hiddenLayer2.output\n",
      "        \n",
      "        # the parameters of the model are the parameters of the two layers it is made out of\n",
      "        self.params = self.hiddenLayer1.params + self.hiddenLayer2.params\n",
      "    \n",
      "    def get_params(self):\n",
      "        params = {}\n",
      "        for param in self.params:\n",
      "            name = param.name\n",
      "            if name in params:\n",
      "                name = name, 2\n",
      "            params[name] = param.get_value()\n",
      "        return params\n",
      "    \n",
      "    def mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "    \n",
      "    def sym_mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean(((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "                      * ((self.y_pred[1] - y[0]) ** 2 + (self.y_pred[0] - y[1]) ** 2))\n",
      "        \n",
      "\n",
      "def shared_dataset(data_xy, borrow=True):\n",
      "        \"\"\" Function that loads the dataset into shared variables\n",
      "        \"\"\"\n",
      "        data_x, data_y, _ = data_xy\n",
      "        shared_x = theano.shared(np.asarray(data_x,\n",
      "                                               dtype='float32'),\n",
      "                                 borrow=borrow)\n",
      "        shared_y = theano.shared(np.asarray(data_y,\n",
      "                                               dtype='float32'),\n",
      "                                 borrow=borrow)\n",
      "        return shared_x, shared_y\n",
      "\n",
      "def train_nn(train_dataset, valid_dataset=None, n_hidden=20, learning_rate=0.01, n_epochs=10, batch_size=20, test_data=None, COM=False, RMSProp=False, nesterov=True, momentum=0, n_in=61, n_out=2):\n",
      "    \"\"\"\n",
      "    Demonstrate stochastic gradient descent optimization for a multilayer\n",
      "    perceptron\n",
      "\n",
      "    :type learning_rate: float\n",
      "    :param learning_rate: learning rate used (factor for the stochastic\n",
      "    gradient\n",
      "\n",
      "    :type n_epochs: int\n",
      "    :param n_epochs: maximal number of epochs to run the optimizer\n",
      "\n",
      "   \"\"\"\n",
      "    train_set_x, train_set_y = shared_dataset(train_dataset)\n",
      "    if valid_dataset:\n",
      "        valid_set_x, valid_set_y = shared_dataset(valid_dataset)\n",
      "\n",
      "    # compute number of minibatches for training, validation and testing\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    n_valid_batches = n_train_batches\n",
      "    \n",
      "    \n",
      "    ######################\n",
      "    # BUILD ACTUAL MODEL #\n",
      "    ######################\n",
      "    print '... building the model'\n",
      "\n",
      "    # allocate symbolic variables for the data\n",
      "    index = T.lscalar()  # index to a [mini]batch\n",
      "    x = T.fmatrix('x')   # input data from visual neurons\n",
      "    y = T.fmatrix('y')  # ground truth\n",
      "\n",
      "    rng = np.random.RandomState(1234)\n",
      "\n",
      "    # construct the MLP class\n",
      "    nn = MLP(rng=rng, input=x, n_in=n_in, n_hidden=n_hidden, n_out=n_out)\n",
      "    \n",
      "    if COM:\n",
      "        nn = COMMLP(rng=rng, input=x, n_in=n_in, n_hidden=n_hidden, n_out=n_out)\n",
      "    else:\n",
      "        nn = MLP(rng=rng, input=x, n_in=n_in, n_hidden=n_hidden, n_out=n_out)\n",
      "\n",
      "    cost = nn.mse(y)\n",
      "    \n",
      "    def RMSprop(cost, params, learning_rate=0.001, rho=0.9, epsilon=1e-6, mu=0, nesterov=False):\n",
      "        gparams = T.grad(cost, params)\n",
      "        updates = []\n",
      "        for p, g in zip(params, gparams):\n",
      "            v = theano.shared(p.get_value() * 0.)\n",
      "            ms = theano.shared(p.get_value() * 0.)\n",
      "            ms_new = rho * ms + (1 - rho) * g ** 2\n",
      "            gradient_scaling = T.sqrt(ms_new + epsilon)\n",
      "            g = g / gradient_scaling\n",
      "            \"\"\"\n",
      "            (1) v_t = mu * v_t-1 - lr * gradient_f(params_t)\n",
      "            or\n",
      "            classic\n",
      "            (2) params_t = params_t-1 + v_t\n",
      "            nesterov\n",
      "            (7) params_t = params_t-1 + mu * v_t - lr * gradient_f(params_t-1)\n",
      "            (8) params_t = params_t-1 + mu**2 * v_t-1 - (1+mu) * lr * gradient_f(params_t-1)\n",
      "            \"\"\"\n",
      "            v_new = mu * v - (1 - mu) * learning_rate * g\n",
      "            if nesterov:\n",
      "                p_new = p + mu * v_new - (1 - mu) * learning_rate * g\n",
      "            else:\n",
      "                p_new = p + v_new\n",
      "            updates.append((ms, ms_new))\n",
      "            updates.append((v, v_new))\n",
      "            updates.append((p, p_new))\n",
      "                \n",
      "        return updates\n",
      "    \n",
      "    if RMSProp:\n",
      "        updates = RMSprop(cost, nn.params, learning_rate=learning_rate, mu=momentum, nesterov=nesterov)\n",
      "    else:\n",
      "        # compute the gradient of cost with respect to theta (sotred in params)\n",
      "        # the resulting gradients will be stored in a list gparams\n",
      "        gparams = [T.grad(cost, param) for param in nn.params]\n",
      "\n",
      "        # specify how to update the parameters of the model as a list of\n",
      "        # (variable, update expression) pairs\n",
      "\n",
      "        updates = [\n",
      "            (param, param - learning_rate * gparam)\n",
      "            for param, gparam in zip(nn.params, gparams)\n",
      "        ]\n",
      "    \n",
      "    def inspect_inputs(i, node, fn):\n",
      "        print i, node, \"input(s) value(s):\", [input[0] for input in fn.inputs]\n",
      "\n",
      "    def inspect_outputs(i, node, fn):\n",
      "        print \"output(s) value(s):\", [output[0] for output in fn.outputs]\n",
      "\n",
      "    # compiling a Theano function `train_model` that returns the cost, but\n",
      "    # in the same time updates the parameter of the model based on the rules\n",
      "    # defined in `updates`\n",
      "    train_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=cost,\n",
      "        updates=updates,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "    \n",
      "    validate_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=nn.valid_mse(y),\n",
      "        givens={\n",
      "            x: valid_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "            y: valid_set_y[index * batch_size:(index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "\n",
      "    ###############\n",
      "    # TRAIN MODEL #\n",
      "    ###############\n",
      "    print '... training'\n",
      "\n",
      "    start_time = time.clock()\n",
      "\n",
      "    epoch = 0 \n",
      "    done_looping = False\n",
      "    \n",
      "    if valid_dataset:\n",
      "        valid_mse = np.zeros(n_epochs)\n",
      "\n",
      "    while (epoch < n_epochs) and (not done_looping):\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "            \n",
      "            minibatch_avg_cost = train_model(minibatch_index)\n",
      "            \n",
      "        if valid_dataset:\n",
      "            validation_losses = [validate_model(i) for i in xrange(n_valid_batches)]\n",
      "            this_validation_loss = np.mean(validation_losses)\n",
      "            valid_mse[epoch] = this_validation_loss \n",
      "\n",
      "            print(\n",
      "                'epoch %i, minibatch %i/%i, validation error %f' %\n",
      "                (\n",
      "                    epoch,\n",
      "                    minibatch_index + 1,\n",
      "                    n_train_batches,\n",
      "                    this_validation_loss\n",
      "                )\n",
      "            )\n",
      "            \n",
      "        epoch = epoch + 1\n",
      "\n",
      "    end_time = time.clock()\n",
      "    \n",
      "    if valid_dataset:\n",
      "        return nn, x, valid_mse\n",
      "    return nn, x\n",
      "\n",
      "def test_nn(nn, nnx, test_data):\n",
      "    print 'testing'\n",
      "    test_batch_size = 1\n",
      "    test_set_x, test_set_y = shared_dataset(test_data)\n",
      "    index = T.lscalar()  # index to a [mini]batch\n",
      "    x = nnx   # input data from visual neurons\n",
      "    test_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=nn.y_pred,\n",
      "        givens={\n",
      "            x: test_set_x[index * test_batch_size: (index + 1) * test_batch_size]\n",
      "        },\n",
      "    )\n",
      "    \n",
      "    true_ys = test_set_y.get_value()\n",
      "    pred_ys = np.zeros((len(true_ys), 2))\n",
      "    for i in range(len(true_ys)):\n",
      "        pred_ys[i] = test_model(i)\n",
      "        #print test_model(i)[0], true_ys[i]\n",
      "        #print test_model(i)[0] * 90, true_ys[i]\n",
      "    \n",
      "    #print nn.get_params()\n",
      "    return pred_ys, true_ys"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Setting up models\n",
      "sm = pystan.StanModel(model_code=neurons_code)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_statistics(s1, s2, preds):\n",
      "    mean_s1 = np.mean(preds[0])\n",
      "    mean_s2 = np.mean(preds[1])\n",
      "    bias_s1 = mean_s1 - s1\n",
      "    bias_s2 = mean_s2 - s2\n",
      "    covmat = np.cov(preds)\n",
      "    var_s1 = covmat[0, 0]\n",
      "    var_s2 = covmat[1, 1]\n",
      "    cov = covmat[0, 1]\n",
      "    corr = cov / (np.sqrt(var_s1) * np.sqrt(var_s2))\n",
      "    stats = {'mean_s1': mean_s1, 'mean_s2': mean_s2, 'bias_s1': bias_s1, 'bias_s2': bias_s2, 'var_s1': var_s1, 'var_s2': var_s2, 'cov': cov, 'corr': corr}\n",
      "    return stats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data_1 = generate_trainset(20000)\n",
      "valid_data_1 = generate_trainset(20000)\n",
      "nn_rms, nnx_rms, valid_mse_rms = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100, RMSProp=True)\n",
      "\"\"\"\n",
      "nn_rms_m9, nnx_rms_m9, valid_mse_rms_m9 = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100, RMSProp=True, nesterov=True, momentum=0.9)\n",
      "nn_rms_m9f, nnx_rms_m9f, valid_mse_rms_m9f = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100, RMSProp=True, nesterov=False, momentum=0.9)\n",
      "nn_rms_m95, nnx_rms_m95, valid_mse_rms_m95 = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100, RMSProp=True, momentum=0.95)\n",
      "nn_rms_m99, nnx_rms_m99, valid_mse_rms_m99 = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100, RMSProp=True, momentum=0.99)\n",
      "\"\"\"\n",
      "nn_plain, nnx_plain, valid_mse_plain = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import poisson\n",
      "def lik_means(s_1, s_2, c_0=.5, c_1=.5, sprefs=sprefs, sigtc_sq=sigtc_sq):\n",
      "    c_0 = c_1 = .5\n",
      "    sprefs_data = np.tile(sprefs, (len(s_1), 1))\n",
      "    s_0t = np.exp(-np.square((np.transpose(np.tile(s_1, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_0 = c_0 * s_0t.T\n",
      "    s_1t = np.exp(-np.square((np.transpose(np.tile(s_2, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_1 = c_1 * s_1t.T\n",
      "    r = r_max * (stim_0 + stim_1)\n",
      "    return r.T\n",
      "def posterior_mean(r, means, s1_grid, s2_grid):\n",
      "    ns_liks = poisson.pmf(r, mu=means)\n",
      "    stim_liks = np.prod(ns_liks, axis=1)\n",
      "    #p_s = 2/14400\n",
      "    #logp_s = np.log(p_s)\n",
      "    logp_s = -3.8573325\n",
      "    loglik = np.sum(np.log(ns_liks), axis=1)\n",
      "    post1 = sum(s1_grid * np.exp(loglik + logp_s)/sum(np.exp(loglik + logp_s)))\n",
      "    post2 = sum(s2_grid * np.exp(loglik + logp_s)/sum(np.exp(loglik + logp_s)))\n",
      "    return post1, post2\n",
      "def get_posteriors(r, num_s1=100):\n",
      "    ndata = len(r)\n",
      "    posteriors = np.zeros((2, ndata))\n",
      "    grid = np.linspace(-60, 60, num_s1)\n",
      "    s1_grid = np.concatenate([[grid[i]]*(num_s1-i) for i in range(num_s1)])\n",
      "    s2_grid = np.concatenate([grid[i:num_s1+1] for i in range(num_s1)])\n",
      "    means = lik_means(s1_grid, s2_grid)\n",
      "    for i in range(len(r)):\n",
      "        posteriors[0][i], posteriors[1][i] = posterior_mean(r[i], means, s1_grid, s2_grid)\n",
      "    return posteriors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rv, s, _ = valid_data_1\n",
      "s=s.T*90\n",
      "opt_preds = fit_optimal(rv, sm, s)\n",
      "post_preds = get_posteriors(rv)\n",
      "mse_opt = np.mean((opt_preds[0] - s[0]) ** 2 + (opt_preds[1] - s[1]) ** 2)\n",
      "mse_post = np.mean((post_preds[0] - s[0]) ** 2 + (post_preds[1] - s[1]) ** 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "post_preds = get_posteriors(rv)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mse_opt = np.mean((opt_preds[0] - s[0]) ** 2 + (opt_preds[1] - s[1]) ** 2)\n",
      "mse_post = np.mean((post_preds[0] - s[0]) ** 2 + (post_preds[1] - s[1]) ** 2)\n",
      "print mse_opt, mse_post"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "#plt.plot(range(20, 100), valid_mse_plain[20:100], c='b')\n",
      "#plt.plot(range(20, 100), valid_mse_rms[20:100], c='y')\n",
      "plt.plot(valid_mse_plain, c='b', label='Vanilla')\n",
      "plt.plot(valid_mse_rms, c='k', label='RMSProp')\n",
      "plt.plot(np.ones(100) * mse_opt, c='m', label='MAP')\n",
      "plt.plot(np.ones(100) * mse_post, c='orange', label='Posterior Mean')\n",
      "plt.legend()\n",
      "plt.xlabel('epoch')\n",
      "plt.ylabel('MSE')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s1 = -30\n",
      "num_deltas = 60\n",
      "#num_deltas = 5\n",
      "\n",
      "mean_s1_nn_rms = [None] * num_deltas\n",
      "mean_s2_nn_rms = [None] * num_deltas\n",
      "bias_s1_nn_rms = [None] * num_deltas\n",
      "bias_s2_nn_rms = [None] * num_deltas\n",
      "var_s1_nn_rms = [None] * num_deltas\n",
      "var_s2_nn_rms = [None] * num_deltas\n",
      "corr_nn_rms = [None] * num_deltas\n",
      "cov_nn_rms = [None] * num_deltas\n",
      "\n",
      "\"\"\"\n",
      "mean_s1_nn_rms_m9 = [None] * num_deltas\n",
      "mean_s2_nn_rms_m9 = [None] * num_deltas\n",
      "bias_s1_nn_rms_m9 = [None] * num_deltas\n",
      "bias_s2_nn_rms_m9 = [None] * num_deltas\n",
      "var_s1_nn_rms_m9 = [None] * num_deltas\n",
      "var_s2_nn_rms_m9 = [None] * num_deltas\n",
      "corr_nn_rms_m9 = [None] * num_deltas\n",
      "cov_nn_rms_m9 = [None] * num_deltas\n",
      "\n",
      "mean_s1_nn_rms_m9f = [None] * num_deltas\n",
      "mean_s2_nn_rms_m9f = [None] * num_deltas\n",
      "bias_s1_nn_rms_m9f = [None] * num_deltas\n",
      "bias_s2_nn_rms_m9f = [None] * num_deltas\n",
      "var_s1_nn_rms_m9f = [None] * num_deltas\n",
      "var_s2_nn_rms_m9f = [None] * num_deltas\n",
      "corr_nn_rms_m9f = [None] * num_deltas\n",
      "cov_nn_rms_m9f = [None] * num_deltas\n",
      "\n",
      "mean_s1_nn_rms_m95 = [None] * num_deltas\n",
      "mean_s2_nn_rms_m95 = [None] * num_deltas\n",
      "bias_s1_nn_rms_m95 = [None] * num_deltas\n",
      "bias_s2_nn_rms_m95 = [None] * num_deltas\n",
      "var_s1_nn_rms_m95 = [None] * num_deltas\n",
      "var_s2_nn_rms_m95 = [None] * num_deltas\n",
      "corr_nn_rms_m95 = [None] * num_deltas\n",
      "cov_nn_rms_m95 = [None] * num_deltas\n",
      "\n",
      "mean_s1_nn_rms_m99 = [None] * num_deltas\n",
      "mean_s2_nn_rms_m99 = [None] * num_deltas\n",
      "bias_s1_nn_rms_m99 = [None] * num_deltas\n",
      "bias_s2_nn_rms_m99 = [None] * num_deltas\n",
      "var_s1_nn_rms_m99 = [None] * num_deltas\n",
      "var_s2_nn_rms_m99 = [None] * num_deltas\n",
      "corr_nn_rms_m99 = [None] * num_deltas\n",
      "cov_nn_rms_m99 = [None] * num_deltas\n",
      "\n",
      "mean_s1_nn_plain = [None] * num_deltas\n",
      "mean_s2_nn_plain = [None] * num_deltas\n",
      "bias_s1_nn_plain = [None] * num_deltas\n",
      "bias_s2_nn_plain = [None] * num_deltas\n",
      "var_s1_nn_plain = [None] * num_deltas\n",
      "var_s2_nn_plain = [None] * num_deltas\n",
      "corr_nn_plain = [None] * num_deltas\n",
      "cov_nn_plain = [None] * num_deltas\n",
      "\"\"\"\n",
      "\n",
      "mean_s1_opt = [None] * num_deltas\n",
      "mean_s2_opt = [None] * num_deltas\n",
      "bias_s1_opt = [None] * num_deltas\n",
      "bias_s2_opt = [None] * num_deltas\n",
      "var_s1_opt = [None] * num_deltas\n",
      "var_s2_opt = [None] * num_deltas\n",
      "corr_opt = [None] * num_deltas\n",
      "cov_opt = [None] * num_deltas\n",
      "\n",
      "mean_s1_post = [None] * num_deltas\n",
      "mean_s2_post = [None] * num_deltas\n",
      "bias_s1_post = [None] * num_deltas\n",
      "bias_s2_post = [None] * num_deltas\n",
      "var_s1_post = [None] * num_deltas\n",
      "var_s2_post = [None] * num_deltas\n",
      "corr_post = [None] * num_deltas\n",
      "cov_post = [None] * num_deltas\n",
      "\n",
      "var_s1_fisher = [None] * num_deltas\n",
      "var_s2_fisher = [None] * num_deltas\n",
      "cov_fisher = [None] * num_deltas\n",
      "\n",
      "for delta_s in range(num_deltas):\n",
      "    #test_data = generate_s_data(s1, s1 + delta_s + 1, 3000)\n",
      "    test_data = generate_s_data(s1, s1 + delta_s, 300)\n",
      "    nn_preds_rms, _ = test_nn(nn_rms, nnx_rms, test_data)\n",
      "    \"\"\"\n",
      "    nn_preds_rms_m9, _ = test_nn(nn_rms_m9, nnx_rms_m9, test_data)\n",
      "    nn_preds_rms_m9f, _ = test_nn(nn_rms_m9f, nnx_rms_m9f, test_data)\n",
      "    nn_preds_rms_m95, _ = test_nn(nn_rms_m95, nnx_rms_m95, test_data)\n",
      "    nn_preds_rms_m99, _ = test_nn(nn_rms_m99, nnx_rms_m99, test_data)\n",
      "    nn_preds_plain, _ = test_nn(nn_plain, nnx_plain, test_data)\n",
      "    \"\"\"\n",
      "    r, _, _ = test_data\n",
      "    opt_preds = fit_optimal(r, sm, init={'s_1':s1, 's_2':s1 + delta_s + 1})\n",
      "    post_preds = get_posteriors(r)\n",
      "    \n",
      "    nn_preds_rms = nn_preds_rms.T * 90\n",
      "    \"\"\"\n",
      "    nn_preds_rms_m9 = nn_preds_rms_m9.T * 90\n",
      "    nn_preds_rms_m9f = nn_preds_rms_m9f.T * 90\n",
      "    nn_preds_rms_m95 = nn_preds_rms_m95.T * 90\n",
      "    nn_preds_rms_m99 = nn_preds_rms_m99.T * 90\n",
      "    nn_preds_plain = nn_preds_plain.T * 90\n",
      "    \"\"\"\n",
      "\n",
      "    nn_stats_rms = get_statistics(s1, s1 + delta_s, nn_preds_rms)\n",
      "    \"\"\"\n",
      "    nn_stats_rms_m9 = get_statistics(s1, s1 + delta_s, nn_preds_rms_m9)\n",
      "    nn_stats_rms_m9f = get_statistics(s1, s1 + delta_s, nn_preds_rms_m9f)\n",
      "    nn_stats_rms_m95 = get_statistics(s1, s1 + delta_s, nn_preds_rms_m95)\n",
      "    nn_stats_rms_m99 = get_statistics(s1, s1 + delta_s, nn_preds_rms_m99)\n",
      "    nn_stats_plain = get_statistics(s1, s1 + delta_s, nn_preds_plain)\n",
      "    \"\"\"\n",
      "    opt_stats = get_statistics(s1, s1 + delta_s, opt_preds)\n",
      "    post_stats = get_statistics(s1, s1 + delta_s, post_preds)\n",
      "    \n",
      "    if delta_s > 0:\n",
      "        FI = fisher_inf(s1, s1 + delta_s, .5, .5)\n",
      "        var_s1_fisher[delta_s] = FI[0, 0]\n",
      "        var_s2_fisher[delta_s] = FI[1, 1]\n",
      "        cov_fisher[delta_s] = FI[0, 1]\n",
      "    \n",
      "    mean_s1_nn_rms[delta_s] = nn_stats_rms['mean_s1']\n",
      "    mean_s2_nn_rms[delta_s] = nn_stats_rms['mean_s2']\n",
      "    bias_s1_nn_rms[delta_s] = nn_stats_rms['bias_s1']\n",
      "    bias_s2_nn_rms[delta_s] = nn_stats_rms['bias_s2']\n",
      "    var_s1_nn_rms[delta_s] = nn_stats_rms['var_s1']\n",
      "    var_s2_nn_rms[delta_s] = nn_stats_rms['var_s2']\n",
      "    corr_nn_rms[delta_s] = nn_stats_rms['corr']\n",
      "    cov_nn_rms[delta_s] = nn_stats_rms['cov']\n",
      "    \n",
      "    mean_s1_opt[delta_s] = opt_stats['mean_s1']\n",
      "    mean_s2_opt[delta_s] = opt_stats['mean_s2']\n",
      "    bias_s1_opt[delta_s] = opt_stats['bias_s1']\n",
      "    bias_s2_opt[delta_s] = opt_stats['bias_s2']\n",
      "    var_s1_opt[delta_s] = opt_stats['var_s1']\n",
      "    var_s2_opt[delta_s] = opt_stats['var_s2']\n",
      "    corr_opt[delta_s] = opt_stats['corr']\n",
      "    cov_opt[delta_s] = opt_stats['cov']\n",
      "    \n",
      "    mean_s1_post[delta_s] = post_stats['mean_s1']\n",
      "    mean_s2_post[delta_s] = post_stats['mean_s2']\n",
      "    bias_s1_post[delta_s] = post_stats['bias_s1']\n",
      "    bias_s2_post[delta_s] = post_stats['bias_s2']\n",
      "    var_s1_post[delta_s] = post_stats['var_s1']\n",
      "    var_s2_post[delta_s] = post_stats['var_s2']\n",
      "    corr_post[delta_s] = post_stats['corr']\n",
      "    cov_post[delta_s] = post_stats['cov']\n",
      "    \n",
      "    \"\"\"\n",
      "    mean_s1_nn_rms_m9[delta_s] = nn_stats_rms_m9['mean_s1']\n",
      "    mean_s2_nn_rms_m9[delta_s] = nn_stats_rms_m9['mean_s2']\n",
      "    bias_s1_nn_rms_m9[delta_s] = nn_stats_rms_m9['bias_s1']\n",
      "    bias_s2_nn_rms_m9[delta_s] = nn_stats_rms_m9['bias_s2']\n",
      "    var_s1_nn_rms_m9[delta_s] = nn_stats_rms_m9['var_s1']\n",
      "    var_s2_nn_rms_m9[delta_s] = nn_stats_rms_m9['var_s2']\n",
      "    corr_nn_rms_m9[delta_s] = nn_stats_rms_m9['corr']\n",
      "    cov_nn_rms_m9[delta_s] = nn_stats_rms_m9['cov']\n",
      "    \n",
      "    mean_s1_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['mean_s1']\n",
      "    mean_s2_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['mean_s2']\n",
      "    bias_s1_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['bias_s1']\n",
      "    bias_s2_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['bias_s2']\n",
      "    var_s1_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['var_s1']\n",
      "    var_s2_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['var_s2']\n",
      "    corr_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['corr']\n",
      "    cov_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['cov']\n",
      "    \n",
      "    mean_s1_nn_rms_m95[delta_s] = nn_stats_rms_m95['mean_s1']\n",
      "    mean_s2_nn_rms_m95[delta_s] = nn_stats_rms_m95['mean_s2']\n",
      "    bias_s1_nn_rms_m95[delta_s] = nn_stats_rms_m95['bias_s1']\n",
      "    bias_s2_nn_rms_m95[delta_s] = nn_stats_rms_m95['bias_s2']\n",
      "    var_s1_nn_rms_m95[delta_s] = nn_stats_rms_m95['var_s1']\n",
      "    var_s2_nn_rms_m95[delta_s] = nn_stats_rms_m95['var_s2']\n",
      "    corr_nn_rms_m95[delta_s] = nn_stats_rms_m95['corr']\n",
      "    cov_nn_rms_m95[delta_s] = nn_stats_rms_m95['cov']\n",
      "    \n",
      "    mean_s1_nn_rms_m99[delta_s] = nn_stats_rms_m99['mean_s1']\n",
      "    mean_s2_nn_rms_m99[delta_s] = nn_stats_rms_m99['mean_s2']\n",
      "    bias_s1_nn_rms_m99[delta_s] = nn_stats_rms_m99['bias_s1']\n",
      "    bias_s2_nn_rms_m99[delta_s] = nn_stats_rms_m99['bias_s2']\n",
      "    var_s1_nn_rms_m99[delta_s] = nn_stats_rms_m99['var_s1']\n",
      "    var_s2_nn_rms_m99[delta_s] = nn_stats_rms_m99['var_s2']\n",
      "    corr_nn_rms_m99[delta_s] = nn_stats_rms_m99['corr']\n",
      "    cov_nn_rms_m99[delta_s] = nn_stats_rms_m99['cov']\n",
      "    \n",
      "    mean_s1_nn_plain[delta_s] = nn_stats_plain['mean_s1']\n",
      "    mean_s2_nn_plain[delta_s] = nn_stats_plain['mean_s2']\n",
      "    bias_s1_nn_plain[delta_s] = nn_stats_plain['bias_s1']\n",
      "    bias_s2_nn_plain[delta_s] = nn_stats_plain['bias_s2']\n",
      "    var_s1_nn_plain[delta_s] = nn_stats_plain['var_s1']\n",
      "    var_s2_nn_plain[delta_s] = nn_stats_plain['var_s2']\n",
      "    corr_nn_plain[delta_s] = nn_stats_plain['corr']\n",
      "    cov_nn_plain[delta_s] = nn_stats_plain['cov']\n",
      "    \"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.patches as mpatches\n",
      "s = range(-30, 30)\n",
      "neg_sd_nn_rms = mean_s1_nn_rms - np.sqrt(var_s1_nn_rms)\n",
      "pos_sd_nn_rms = mean_s1_nn_rms + np.sqrt(var_s1_nn_rms)\n",
      "neg_sd_opt = mean_s1_opt - np.sqrt(var_s1_opt)\n",
      "pos_sd_opt = mean_s1_opt + np.sqrt(var_s1_opt)\n",
      "neg_sd_post = mean_s1_post - np.sqrt(var_s1_post)\n",
      "pos_sd_post = mean_s1_post + np.sqrt(var_s1_post)\n",
      "plt.rc('text', usetex=True)\n",
      "plt.figure(figsize=(10,10))\n",
      "plt.fill_between(s, pos_sd_nn_rms, neg_sd_nn_rms, facecolor='k', alpha=0.5, edgecolor=\"None\", label=\"NN\")\n",
      "plt.fill_between(s, pos_sd_opt, neg_sd_opt, facecolor='m', alpha=0.5, edgecolor=\"None\", label=\"MLE\")\n",
      "plt.fill_between(s, pos_sd_post, neg_sd_post, facecolor='orange', alpha=0.5, edgecolor=\"None\", label=\"Posterior\")\n",
      "plt.ylim([-40,40])\n",
      "plt.xlabel(r'$s_2$',fontsize=30)\n",
      "plt.ylabel(r'\\hat{s_1}',fontsize=30)\n",
      "nn_patch = mpatches.Patch(color='k', label='NN')\n",
      "mle_patch = mpatches.Patch(color='m', label='MAP')\n",
      "post_patch = mpatches.Patch(color='orange', label='Posterior')\n",
      "plt.legend(handles=[nn_patch, mle_patch, post_patch])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = range(-30, 30)\n",
      "neg_sd_nn_rms = mean_s2_nn_rms - np.sqrt(var_s2_nn_rms)\n",
      "pos_sd_nn_rms = mean_s2_nn_rms + np.sqrt(var_s2_nn_rms)\n",
      "neg_sd_opt = mean_s2_opt - np.sqrt(var_s2_opt)\n",
      "pos_sd_opt = mean_s2_opt + np.sqrt(var_s2_opt)\n",
      "neg_sd_post = mean_s2_post - np.sqrt(var_s2_post)\n",
      "pos_sd_post = mean_s2_post + np.sqrt(var_s2_post)\n",
      "plt.rc('text', usetex=True)\n",
      "plt.figure(figsize=(10,10))\n",
      "plt.fill_between(s, pos_sd_nn_rms, neg_sd_nn_rms, facecolor='k', alpha=0.5, edgecolor=\"None\")\n",
      "plt.fill_between(s, pos_sd_opt, neg_sd_opt, facecolor='m', alpha=0.5, edgecolor=\"None\")\n",
      "plt.fill_between(s, pos_sd_post, neg_sd_post, facecolor='orange', alpha=0.5, edgecolor=\"None\")\n",
      "plt.xlabel(r'$s_2$',fontsize=30)\n",
      "plt.ylabel(r'\\hat{s_2}',fontsize=30)\n",
      "plt.ylim([-40,40])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5)\n",
      "#RMS\n",
      "ax1.plot(range(1, 61), bias_s1_nn_rms, c='k', label='NN')\n",
      "ax2.plot(range(1, 61), bias_s2_nn_rms, c='k', label='NN')\n",
      "ax3.plot(range(1, 61), var_s1_nn_rms, c='k', label='NN')\n",
      "ax4.plot(range(1, 61), var_s2_nn_rms, c='k', label='NN')\n",
      "ax5.plot(range(1, 61), cov_nn_rms, c='k', label='NN')\n",
      "#optimal in red\n",
      "ax1.plot(range(1, 61), bias_s1_opt, c='m', label='MLE')\n",
      "ax2.plot(range(1, 61), bias_s2_opt, c='m', label='MLE')\n",
      "ax3.plot(range(1, 61), var_s1_opt, c='m', label='MLE')\n",
      "ax4.plot(range(1, 61), var_s2_opt, c='m', label='MLE')\n",
      "ax5.plot(range(1, 61), cov_opt, c='m', label='MLE')\n",
      "#optimal in red\n",
      "ax1.plot(range(1, 61), bias_s1_post, c='orange', label='MLE')\n",
      "ax2.plot(range(1, 61), bias_s2_post, c='orange', label='MLE')\n",
      "ax3.plot(range(1, 61), var_s1_post, c='orange', label='MLE')\n",
      "ax4.plot(range(1, 61), var_s2_post, c='orange', label='MLE')\n",
      "ax5.plot(range(1, 61), cov_post, c='orange', label='MLE')\n",
      "\"\"\"\n",
      "#RMS with momentum\n",
      "ax1.plot(range(1, 61), bias_s1_nn_rms_m9, c='m', label='NN')\n",
      "ax2.plot(range(1, 61), bias_s2_nn_rms_m9, c='m', label='NN')\n",
      "ax3.plot(range(1, 61), var_s1_nn_rms_m9, c='m', label='NN')\n",
      "ax4.plot(range(1, 61), var_s2_nn_rms_m9, c='m', label='NN')\n",
      "ax5.plot(range(1, 61), cov_nn_rms_m9, c='m', label='NN')\n",
      "#RMS with momentum\n",
      "ax1.plot(range(1, 61), bias_s1_nn_rms_m95, c='c', label='NN')\n",
      "ax2.plot(range(1, 61), bias_s2_nn_rms_m95, c='c', label='NN')\n",
      "ax3.plot(range(1, 61), var_s1_nn_rms_m95, c='c', label='NN')\n",
      "ax4.plot(range(1, 61), var_s2_nn_rms_m95, c='c', label='NN')\n",
      "ax5.plot(range(1, 61), cov_nn_rms_m95, c='c', label='NN')\n",
      "#RMS with momentum\n",
      "ax1.plot(range(1, 61), bias_s1_nn_rms_m99, c='k', label='NN')\n",
      "ax2.plot(range(1, 61), bias_s2_nn_rms_m99, c='k', label='NN')\n",
      "ax3.plot(range(1, 61), var_s1_nn_rms_m99, c='k', label='NN')\n",
      "ax4.plot(range(1, 61), var_s2_nn_rms_m99, c='k', label='NN')\n",
      "ax5.plot(range(1, 61), cov_nn_rms_m99, c='k', label='NN')\n",
      "#RMS\n",
      "ax1.plot(range(1, 61), bias_s1_nn_rms_m9f, c='r', label='NN')\n",
      "ax2.plot(range(1, 61), bias_s2_nn_rms_m9f, c='r', label='NN')\n",
      "ax3.plot(range(1, 61), var_s1_nn_rms_m9f, c='r', label='NN')\n",
      "ax4.plot(range(1, 61), var_s2_nn_rms_m9f, c='r', label='NN')\n",
      "ax5.plot(range(1, 61), cov_nn_rms_m9f, c='r', label='NN')\n",
      "#Plain\n",
      "ax1.plot(range(1, 61), bias_s1_nn_plain, c='b', label='NN')\n",
      "ax2.plot(range(1, 61), bias_s2_nn_plain, c='b', label='NN')\n",
      "ax3.plot(range(1, 61), var_s1_nn_plain, c='b', label='NN')\n",
      "ax4.plot(range(1, 61), var_s2_nn_plain, c='b', label='NN')\n",
      "ax5.plot(range(1, 61), cov_nn_plain, c='b', label='NN')\n",
      "\"\"\"\n",
      "#Fisher information in green\n",
      "ax3.plot(range(7, 61), var_s1_fisher[6:60], c='orange', label='Fisher')\n",
      "ax4.plot(range(7, 61), var_s2_fisher[6:60], c='orange', label='Fisher')\n",
      "ax5.plot(range(7, 61), cov_fisher[6:60], c='orange', label='Fisher')\n",
      "ax1.locator_params(axis = 'y', nbins = 4)\n",
      "ax2.locator_params(axis = 'y', nbins = 4)\n",
      "ax3.locator_params(axis = 'y', nbins = 4)\n",
      "ax4.locator_params(axis = 'y', nbins = 4)\n",
      "ax5.locator_params(axis = 'y', nbins = 4)\n",
      "#ax1.legend()\n",
      "ax1.set_title(\"Bias $s_1$\")\n",
      "ax2.set_title(\"Bias $s_2$\")\n",
      "ax3.set_title(r'Variance $s_1$')\n",
      "ax4.set_title(r'Variance $s_2$')\n",
      "ax5.set_title('Covariance')\n",
      "ax5.set_xlabel(r'$\\Delta$ s')\n",
      "f.set_size_inches(10,10)\n",
      "ax3.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "generate_trainset_c(20000, low=.3, high=.7, r_max=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}