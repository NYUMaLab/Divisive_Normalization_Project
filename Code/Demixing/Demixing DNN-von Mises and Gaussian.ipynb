{
 "metadata": {
  "name": "",
  "signature": "sha256:4434e10726a997cc49b4bd1bf7d0c71bcf395c44cb97a638c61bcb1909529100"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import math\n",
      "from scipy.stats import vonmises\n",
      "\n",
      "nneuron = 121\n",
      "t_pi = 2*math.pi\n",
      "min_angle = 0\n",
      "max_angle = t_pi\n",
      "sprefs = np.linspace(min_angle, max_angle, nneuron)\n",
      "\n",
      "kappa_tc = t_pi\n",
      "ndata = 3000\n",
      "r_max = 50\n",
      "\n",
      "c_50 = 13.1\n",
      "\n",
      "def generate_popcode_data(ndata, nneuron, kappa_tc, c_50, r_max, noise, sort):\n",
      "    s = np.random.rand(2, ndata)*t_pi\n",
      "    if sort:\n",
      "        s = np.sort(s, axis=0)\n",
      "    c = np.ones((2, ndata)) * .5\n",
      "    c_rms = np.sqrt(np.square(c[0]) + np.square(c[1]))\n",
      "    stim_0 = c[0] * np.asarray([vonmises.pdf(s[0], kappa_tc, loc=sprefs[sp]) for sp in range(len(sprefs))])\n",
      "    stim_1 = c[1] * np.asarray([vonmises.pdf(s[1], kappa_tc, loc=sprefs[sp]) for sp in range(len(sprefs))])\n",
      "    r = r_max * (stim_0 + stim_1)/(c_50 + c_rms)\n",
      "    r = r.T\n",
      "    s = s.T\n",
      "    c = c.T\n",
      "    if noise == \"poisson\":\n",
      "        r = np.random.poisson(r) + 0.0\n",
      "    return r, s, c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pystan\n",
      "neurons_code = \"\"\"\n",
      "data {\n",
      "    int<lower=0> N; // number of neurons\n",
      "    int r[N]; // neural response\n",
      "    real sprefs[N]; // preferred stimuli\n",
      "    real<lower=0> c_1;\n",
      "    real<lower=0> c_2;\n",
      "    real c_50;\n",
      "    int r_max;\n",
      "    real c_rms;\n",
      "}\n",
      "parameters {\n",
      "    real s_1;\n",
      "    real s_2;\n",
      "}\n",
      "transformed parameters {\n",
      "    real lambda[N];\n",
      "    for (n in 1:N)\n",
      "        lambda[n] <- r_max * ((c_1 * exp(von_mises_log(s_1, sprefs[n], 2 * pi())) + c_2 * exp(von_mises_log(s_2, sprefs[n], 2 * pi())))/(c_rms + c_50));\n",
      "}\n",
      "model {\n",
      "    s_1 ~ uniform(0, 2 * pi());\n",
      "    s_2 ~ uniform(0, 2 * pi());\n",
      "    r ~ poisson(lambda);\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "ndata = 1\n",
      "#r, s, _ = generate_popcode_data(ndata, nneuron, kappa_tc, c_50, r_max, \"poisson\", True)\n",
      "\n",
      "neurons_dat = {'N': 121,\n",
      "               'r': r[0].astype(int),\n",
      "               'sprefs': sprefs,\n",
      "               'c_1': .5,\n",
      "               'c_2': .5,\n",
      "               'c_50': 13.1,\n",
      "               'r_max': r_max,\n",
      "               'c_rms': 0.707106781}\n",
      "\n",
      "fit = pystan.stan(model_code=neurons_code, data=neurons_dat,\n",
      "                  iter=1000, chains=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "//anaconda/lib/python2.7/multiprocessing/queues.py:390: UserWarning: Pickling fit objects is an experimental feature!\n",
        "The relevant StanModel instance must be pickled along with this fit object.\n",
        "When unpickling the StanModel must be unpickled first.\n",
        "  return send(obj)\n",
        "//anaconda/lib/python2.7/multiprocessing/queues.py:390: UserWarning: Pickling fit objects is an experimental feature!\n",
        "The relevant StanModel instance must be pickled along with this fit object.\n",
        "When unpickling the StanModel must be unpickled first.\n",
        "  return send(obj)\n",
        "//anaconda/lib/python2.7/multiprocessing/queues.py:390: UserWarning: Pickling fit objects is an experimental feature!\n",
        "The relevant StanModel instance must be pickled along with this fit object.\n",
        "When unpickling the StanModel must be unpickled first.\n",
        "  return send(obj)\n",
        "//anaconda/lib/python2.7/multiprocessing/queues.py:390: UserWarning: Pickling fit objects is an experimental feature!\n",
        "The relevant StanModel instance must be pickled along with this fit object.\n",
        "When unpickling the StanModel must be unpickled first.\n",
        "  return send(obj)\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "print r[0]\n",
      "print s[0]\n",
      "print fit\n",
      "samps = fit.extract(['s_1', 's_2'])\n",
      "print len(samps['s_1'])\n",
      "print np.mean(samps['s_1'])\n",
      "print np.mean(samps['s_2'])\n",
      "plt.scatter(samps['s_1'], samps['s_2'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import math\n",
      "\n",
      "nneuron = 61\n",
      "min_angle = -90\n",
      "max_angle = 90\n",
      "sprefs = np.linspace(min_angle, max_angle, nneuron)\n",
      "\n",
      "r_max = 30\n",
      "sigtc_sq = float(10**2)\n",
      "sigtc = 10\n",
      "c_50 = 13.1\n",
      "\n",
      "def generate_popcode_data2(ndata, nneuron, sigtc_sq, c_50, r_max, noise, sort):\n",
      "    s = np.random.rand(2, ndata) * 120 - 60\n",
      "    if sort:\n",
      "        s = np.sort(s, axis=0)\n",
      "    c = np.ones((2, ndata)) * .5\n",
      "    c_rms = np.sqrt(np.square(c[0]) + np.square(c[1]))\n",
      "    sprefs_data = np.tile(sprefs, (ndata, 1))\n",
      "    s_0 = np.exp(-np.square((np.transpose(np.tile(s[0], (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_0 = c[0] * s_0.T\n",
      "    s_1 = np.exp(-np.square((np.transpose(np.tile(s[1], (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_1 = c[1] * s_1.T\n",
      "    r = r_max * (stim_0 + stim_1)/(c_50 + c_rms)\n",
      "    r = r.T\n",
      "    s = s.T\n",
      "    s = s/90\n",
      "    c = c.T\n",
      "    if noise == \"poisson\":\n",
      "        r = np.random.poisson(r) + 0.0\n",
      "    return r, s, c\n",
      "r1, s1, c1 = generate_popcode_data(ndata, nneuron, kappa_tc, c_50, r_max, \"poisson\", True)\n",
      "r2, s2, c2 = generate_popcode_data2(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True)\n",
      "print sprefs, r2[0], s2[0]\n",
      "print r1[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[-90. -87. -84. -81. -78. -75. -72. -69. -66. -63. -60. -57. -54. -51. -48.\n",
        " -45. -42. -39. -36. -33. -30. -27. -24. -21. -18. -15. -12.  -9.  -6.  -3.\n",
        "   0.   3.   6.   9.  12.  15.  18.  21.  24.  27.  30.  33.  36.  39.  42.\n",
        "  45.  48.  51.  54.  57.  60.  63.  66.  69.  72.  75.  78.  81.  84.  87.\n",
        "  90.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  1.  3.  0.  2.  1.  4.  3.  3.  2.  1.  0.  0.  2.  0.  1.  0.  0.  0.\n",
        "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0.  0.  0.  0.  0.] [-0.25045776 -0.195803  ]\n",
        "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
        "  0.  3.  1.  1.  1.  1.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  1.  0.  0.  2.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0.  0.  0.  0.  1.]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Multilayer ReLU net\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import numpy\n",
      "\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "def relu(x):\n",
      "    return theano.tensor.switch(x<0, 0, x)\n",
      "\n",
      "class HiddenLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
      "                 activation=T.nnet.relu):\n",
      "        \"\"\"\n",
      "        Typical hidden layer of a MLP: units are fully-connected and have\n",
      "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
      "        and the bias vector b is of shape (n_out,).\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.dmatrix\n",
      "        :param input: a symbolic tensor of shape (n_examples, n_in)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: dimensionality of input\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of hidden units\n",
      "\n",
      "        :type activation: theano.Op or function\n",
      "        :param activation: Non linearity to be applied in the hidden\n",
      "                           layer\n",
      "        \"\"\"\n",
      "        self.input = input\n",
      "        if W is None:\n",
      "            W_values = numpy.random.randn(n_in, n_out)\n",
      "            #W_values = .1 * numpy.random.randn(n_in, n_out)\n",
      "\n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        if b is None:\n",
      "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
      "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "\n",
      "        lin_output = T.dot(input, self.W) + self.b\n",
      "        self.output = (\n",
      "            lin_output if activation is None\n",
      "            else activation(lin_output)\n",
      "        )\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "class MLP(object):\n",
      "\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: symbolic variable that describes the input of the\n",
      "        architecture (one minibatch)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: number of input units, the dimension of the space in\n",
      "        which the datapoints lie\n",
      "\n",
      "        :type n_hidden: int\n",
      "        :param n_hidden: number of hidden units\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "        which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        # Since we are dealing with a one hidden layer MLP, this will translate\n",
      "        # into a HiddenLayer with a tanh activation function connected to the\n",
      "        # LogisticRegression layer; the activation function can be replaced by\n",
      "        # sigmoid or any other nonlinear function\n",
      "        self.hiddenLayer1 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            activation=relu\n",
      "        )\n",
      "        \n",
      "        self.hiddenLayer2 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=self.hiddenLayer1.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out,\n",
      "            activation=relu\n",
      "        )\n",
      "        \n",
      "        self.y_pred = self.hiddenLayer2.output\n",
      "        \n",
      "        # the parameters of the model are the parameters of the two layers it is made out of\n",
      "        self.params = self.hiddenLayer1.params + self.hiddenLayer2.params\n",
      "    \n",
      "    def get_params(self):\n",
      "\n",
      "        params = {}\n",
      "        for param in self.params:\n",
      "            name = param.name\n",
      "            if name in params:\n",
      "                name = name, 2\n",
      "            params[name] = param.get_value()\n",
      "        return params\n",
      "    \n",
      "    def mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "    \n",
      "    def sym_mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean(((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "                      * ((self.y_pred[1] - y[0]) ** 2 + (self.y_pred[0] - y[1]) ** 2))\n",
      "        \n",
      "\n",
      "def shared_dataset(data_xy, borrow=True):\n",
      "        \"\"\" Function that loads the dataset into shared variables\n",
      "        \"\"\"\n",
      "        data_x, data_y, _ = data_xy\n",
      "        shared_x = theano.shared(numpy.asarray(data_x,\n",
      "                                               dtype='float32'),\n",
      "                                 borrow=borrow)\n",
      "        shared_y = theano.shared(numpy.asarray(data_y,\n",
      "                                               dtype='float32'),\n",
      "                                 borrow=borrow)\n",
      "        return shared_x, shared_y\n",
      "\n",
      "def train_nn(dataset, n_hidden=20, learning_rate=0.01, n_epochs=10, batch_size=20, test_data=None):\n",
      "    \"\"\"\n",
      "    Demonstrate stochastic gradient descent optimization for a multilayer\n",
      "    perceptron\n",
      "\n",
      "    :type learning_rate: float\n",
      "    :param learning_rate: learning rate used (factor for the stochastic\n",
      "    gradient\n",
      "\n",
      "    :type n_epochs: int\n",
      "    :param n_epochs: maximal number of epochs to run the optimizer\n",
      "\n",
      "    :type dataset: string\n",
      "    :param dataset: the path of the MNIST dataset file from\n",
      "                 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
      "\n",
      "\n",
      "   \"\"\"\n",
      "    train_set_x, train_set_y = shared_dataset(dataset)\n",
      "\n",
      "    # compute number of minibatches for training, validation and testing\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    \n",
      "    test_batch_size = 1\n",
      "    \n",
      "    ######################\n",
      "    # BUILD ACTUAL MODEL #\n",
      "    ######################\n",
      "    print '... building the model'\n",
      "\n",
      "    # allocate symbolic variables for the data\n",
      "    index = T.lscalar()  # index to a [mini]batch\n",
      "    x = T.fmatrix('x')   # input data from visual neurons\n",
      "    y = T.fmatrix('y')  # posterior\n",
      "\n",
      "    rng = numpy.random.RandomState(1234)\n",
      "\n",
      "    # construct the MLP class\n",
      "    nn = MLP(\n",
      "        rng=rng,\n",
      "        input=x,\n",
      "        #von mises/circular\n",
      "        #n_in=121, \n",
      "        n_in=61,\n",
      "        n_hidden=n_hidden,\n",
      "        n_out=2\n",
      "    )\n",
      "\n",
      "    # the cost we minimize during training is the mean squared error; cost is expressed\n",
      "    # here symbolically\n",
      "    cost = nn.mse(y)\n",
      "\n",
      "    # compute the gradient of cost with respect to theta (sotred in params)\n",
      "    # the resulting gradients will be stored in a list gparams\n",
      "    gparams = [T.grad(cost, param) for param in nn.params]\n",
      "\n",
      "    # specify how to update the parameters of the model as a list of\n",
      "    # (variable, update expression) pairs\n",
      "\n",
      "    # given two list the zip A = [a1, a2, a3, a4] and B = [b1, b2, b3, b4] of\n",
      "    # same length, zip generates a list C of same size, where each element\n",
      "    # is a pair formed from the two lists :\n",
      "    #    C = [(a1, b1), (a2, b2), (a3, b3), (a4, b4)]\n",
      "    updates = [\n",
      "        (param, param - learning_rate * gparam)\n",
      "        for param, gparam in zip(nn.params, gparams)\n",
      "    ]\n",
      "\n",
      "    # compiling a Theano function `train_model` that returns the cost, but\n",
      "    # in the same time updates the parameter of the model based on the rules\n",
      "    # defined in `updates`\n",
      "    train_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=cost,\n",
      "        updates=updates,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "\n",
      "    ###############\n",
      "    # TRAIN MODEL #\n",
      "    ###############\n",
      "    print '... training'\n",
      "\n",
      "    start_time = time.clock()\n",
      "\n",
      "    epoch = 0\n",
      "    done_looping = False\n",
      "\n",
      "    while (epoch < n_epochs) and (not done_looping):\n",
      "        epoch = epoch + 1\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "            \n",
      "            minibatch_avg_cost = train_model(minibatch_index)\n",
      "            \n",
      "            #print \"epoch \" + repr(epoch)\n",
      "            #print \"minibatch \" + repr(minibatch_index + 1) + \"/\" + repr(n_train_batches)\n",
      "            #print \"average cost \" + repr(minibatch_avg_cost)\n",
      "\n",
      "    end_time = time.clock()\n",
      "\n",
      "    def inspect_inputs(i, node, fn):\n",
      "        print i, node, \"input(s) value(s):\", [input[0] for input in fn.inputs]\n",
      "\n",
      "    def inspect_outputs(i, node, fn):\n",
      "        print \"output(s) value(s):\", [output[0] for output in fn.outputs]\n",
      "    \n",
      "    print 'testing'\n",
      "    test_batch_size = 1\n",
      "    test_set_x, test_set_y = shared_dataset(test_data)\n",
      "    test_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=nn.y_pred,\n",
      "        #mode=theano.compile.MonitorMode(pre_func=inspect_inputs, post_func=inspect_outputs),\n",
      "        givens={\n",
      "            x: test_set_x[index * test_batch_size: (index + 1) * test_batch_size]\n",
      "        },\n",
      "    )\n",
      "    \n",
      "    true_ys = test_set_y.get_value()\n",
      "    pred_ys = numpy.zeros(len(true_ys))\n",
      "    for i in range(len(true_ys)):\n",
      "        #pred_ys[i] = test_model(i)\n",
      "        print test_model(i), true_ys[i]\n",
      "    \n",
      "    print nn.get_params()\n",
      "    return pred_ys, true_ys\n",
      "\n",
      "#train_data = generate_popcode_data(ndata, nneuron, kappa_tc, c_50, r_max, \"poisson\", True)\n",
      "#test_data = generate_popcode_data(ndata, nneuron, kappa_tc, c_50, r_max, \"poisson\", True)\n",
      "train_data = generate_popcode_data2(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True)\n",
      "test_data = generate_popcode_data2(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True)\n",
      "nn, _ = train_nn(train_data, n_hidden=10, learning_rate=.01, n_epochs=100, test_data=test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "... building the model\n",
        "... training"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "testing\n",
        "[[ 0.  0.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [-0.57912576  0.20294704]\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "T.nnet.softmax"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "<theano.tensor.nnet.nnet.Softmax at 0x107ab0150>"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}