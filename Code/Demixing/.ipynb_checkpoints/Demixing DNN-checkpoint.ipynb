{
 "metadata": {
  "name": "",
  "signature": "sha256:bd513787bbd0e27cc2aadc70f7be5331d9db348e782ae95cace725f9d31bf2f1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import math\n",
      "from scipy.stats import vonmises\n",
      "\n",
      "nneuron = 121\n",
      "t_pi = 2*math.pi\n",
      "min_angle = 0\n",
      "max_angle = t_pi\n",
      "sprefs = np.linspace(min_angle, max_angle, nneuron)\n",
      "\n",
      "kappa_tc = t_pi\n",
      "ndata = 3000\n",
      "r_max = 50\n",
      "\n",
      "c_50 = 13.1\n",
      "\n",
      "def generate_popcode_data(ndata, nneuron, kappa_tc, c_50, r_max, noise, sort):\n",
      "    s = np.random.rand(2, ndata)*t_pi\n",
      "    if sort:\n",
      "        s = np.sort(s, axis=0)\n",
      "    c = np.ones((2, ndata)) * .5\n",
      "    c_rms = np.sqrt(np.square(c[0]) + np.square(c[1]))\n",
      "    stim_0 = c[0] * np.asarray([vonmises.pdf(s[0], kappa_tc, loc=sprefs[sp]) for sp in range(len(sprefs))])\n",
      "    stim_1 = c[1] * np.asarray([vonmises.pdf(s[1], kappa_tc, loc=sprefs[sp]) for sp in range(len(sprefs))])\n",
      "    r = r_max * (stim_0 + stim_1)/(c_50 + c_rms)\n",
      "    r = r.T\n",
      "    s = s.T\n",
      "    c = c.T\n",
      "    if noise == \"poisson\":\n",
      "        r = np.random.poisson(r) + 0.0\n",
      "    return r, s, c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import math\n",
      "\n",
      "nneuron = 61\n",
      "min_angle = -90\n",
      "max_angle = 90\n",
      "sprefs = np.linspace(min_angle, max_angle, nneuron)\n",
      "\n",
      "kappa_tc = t_pi\n",
      "ndata = 3000\n",
      "r_max = 30\n",
      "sigtc_sq = float(10**2)\n",
      "c_50 = 13.1\n",
      "\n",
      "def generate_popcode_data2(ndata, nneuron, sigtc_sq, c_50, r_max, noise, sort):\n",
      "    s = np.random.rand(2, ndata) * 120 - 60\n",
      "    if sort:\n",
      "        s = np.sort(s, axis=0)\n",
      "    c = np.ones((2, ndata)) * .5\n",
      "    c_rms = np.sqrt(np.square(c[0]) + np.square(c[1]))\n",
      "    sprefs_data = np.tile(sprefs, (ndata, 1))\n",
      "    s_0 = np.exp(-np.square((np.transpose(np.tile(s[0], (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_0 = c[0] * s_0.T\n",
      "    s_1 = np.exp(-np.square((np.transpose(np.tile(s[1], (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_1 = c[1] * s_1.T\n",
      "    r = r_max * (stim_0 + stim_1)/(c_50 + c_rms)\n",
      "    r = r.T\n",
      "    s = s.T\n",
      "    s = s/90\n",
      "    c = c.T\n",
      "    if noise == \"poisson\":\n",
      "        r = np.random.poisson(r) + 0.0\n",
      "    return r, s, c\n",
      "r1, s1, c1 = generate_popcode_data(ndata, nneuron, kappa_tc, c_50, r_max, \"poisson\", True)\n",
      "r2, s2, c2 = generate_popcode_data2(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True)\n",
      "print sprefs, r2[0], s2[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[-90. -87. -84. -81. -78. -75. -72. -69. -66. -63. -60. -57. -54. -51. -48.\n",
        " -45. -42. -39. -36. -33. -30. -27. -24. -21. -18. -15. -12.  -9.  -6.  -3.\n",
        "   0.   3.   6.   9.  12.  15.  18.  21.  24.  27.  30.  33.  36.  39.  42.\n",
        "  45.  48.  51.  54.  57.  60.  63.  66.  69.  72.  75.  78.  81.  84.  87.\n",
        "  90.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
        "  2.  0.  0.  2.  2.  1.  1.  1.  0.  0.  2.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0.  0.  0.  0.  0.] [ 0.26839308  0.38262492]\n"
       ]
      }
     ],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pystan\n",
      "neurons_code = \"\"\"\n",
      "data {\n",
      "    int<lower=0> N; // number of neurons\n",
      "    int r[N]; // neural response\n",
      "    real sprefs[N]; // preferred stimuli\n",
      "    real<lower=0> c_1;\n",
      "    real<lower=0> c_2;\n",
      "    real c_50;\n",
      "    int r_max;\n",
      "    real c_rms;\n",
      "}\n",
      "parameters {\n",
      "    real s_1;\n",
      "    real s_2;\n",
      "}\n",
      "transformed parameters {\n",
      "    real lambda[N];\n",
      "    for (n in 1:N)\n",
      "        lambda[n] <- r_max * ((c_1 * exp(von_mises_log(s_1, sprefs[n], 2 * pi())) + c_2 * exp(von_mises_log(s_2, sprefs[n], 2 * pi())))/(c_rms + c_50));\n",
      "}\n",
      "model {\n",
      "    s_1 ~ uniform(0, 2 * pi());\n",
      "    s_2 ~ uniform(0, 2 * pi());\n",
      "    r ~ poisson(lambda);\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "ndata = 1\n",
      "#r, s, _ = generate_popcode_data(ndata, nneuron, kappa_tc, c_50, r_max, \"poisson\", True)\n",
      "\n",
      "neurons_dat = {'N': 121,\n",
      "               'r': r[0].astype(int),\n",
      "               'sprefs': sprefs,\n",
      "               'c_1': .5,\n",
      "               'c_2': .5,\n",
      "               'c_50': 13.1,\n",
      "               'r_max': r_max,\n",
      "               'c_rms': 0.707106781}\n",
      "\n",
      "fit = pystan.stan(model_code=neurons_code, data=neurons_dat,\n",
      "                  iter=1000, chains=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "//anaconda/lib/python2.7/multiprocessing/queues.py:390: UserWarning: Pickling fit objects is an experimental feature!\n",
        "The relevant StanModel instance must be pickled along with this fit object.\n",
        "When unpickling the StanModel must be unpickled first.\n",
        "  return send(obj)\n",
        "//anaconda/lib/python2.7/multiprocessing/queues.py:390: UserWarning: Pickling fit objects is an experimental feature!\n",
        "The relevant StanModel instance must be pickled along with this fit object.\n",
        "When unpickling the StanModel must be unpickled first.\n",
        "  return send(obj)\n",
        "//anaconda/lib/python2.7/multiprocessing/queues.py:390: UserWarning: Pickling fit objects is an experimental feature!\n",
        "The relevant StanModel instance must be pickled along with this fit object.\n",
        "When unpickling the StanModel must be unpickled first.\n",
        "  return send(obj)\n",
        "//anaconda/lib/python2.7/multiprocessing/queues.py:390: UserWarning: Pickling fit objects is an experimental feature!\n",
        "The relevant StanModel instance must be pickled along with this fit object.\n",
        "When unpickling the StanModel must be unpickled first.\n",
        "  return send(obj)\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "print r[0]\n",
      "print s[0]\n",
      "print fit\n",
      "samps = fit.extract(['s_1', 's_2'])\n",
      "print len(samps['s_1'])\n",
      "print np.mean(samps['s_1'])\n",
      "print np.mean(samps['s_2'])\n",
      "plt.scatter(samps['s_1'], samps['s_2'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 2.  1.  3.  4.  1.  2.  0.  2.  2.  0.  0.  1.  0.  0.  1.  0.  1.  0.\n",
        "  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
        "  1.  0.  2.  0.  0.  3.  0.  4.  2.  1.  0.  5.  0.  1.  2.  0.  1.  3.\n",
        "  1.  1.  0.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
        "  0.  0.  0.  3.  1.  1.  1.  1.  2.  3.  1.  4.  2.]\n",
        "[ 2.46014997  6.27724023]\n",
        "Inference for Stan model: anon_model_9f8abcf8ac801a2ee6e70040d94f7606.\n",
        "4 chains, each with iter=1000; warmup=500; thin=1; \n",
        "post-warmup draws per chain=500, total post-warmup draws=2000.\n",
        "\n",
        "              mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
        "s_1            2.5  3.5e-3   0.07   2.37   2.45    2.5   2.55   2.64  408.0   1.01\n",
        "s_2           0.07  2.4e-3   0.05 4.1e-3   0.04   0.07    0.1   0.19  392.0    1.0\n",
        "lambda[0]     1.73  2.2e-3   0.05   1.58   1.71   1.75   1.76   1.77  484.0    1.0\n",
        "lambda[1]     1.76  1.1e-3   0.03   1.67   1.76   1.77   1.77   1.77  547.0    1.0\n",
        "lambda[2]     1.75  1.1e-3   0.02   1.71   1.74   1.76   1.77   1.77  256.0   1.01\n",
        "lambda[3]     1.72  2.2e-3   0.04   1.65   1.69   1.73   1.75   1.77  307.0    1.0\n",
        "lambda[4]     1.66  3.3e-3   0.06   1.55   1.61   1.66   1.71   1.77  340.0    1.0\n",
        "lambda[5]     1.58  4.4e-3   0.08   1.44   1.51   1.57   1.64   1.74  357.0    1.0\n",
        "lambda[6]     1.47  5.2e-3    0.1   1.31   1.39   1.47   1.54   1.69  368.0    1.0\n",
        "lambda[7]     1.35  5.8e-3   0.11   1.18   1.26   1.34   1.43    1.6  377.0    1.0\n",
        "lambda[8]     1.22  6.2e-3   0.12   1.04   1.12   1.21    1.3    1.5  384.0    1.0\n",
        "lambda[9]     1.09  6.3e-3   0.13    0.9   0.99   1.07   1.16   1.38  390.0    1.0\n",
        "lambda[10]    0.95  6.3e-3   0.12   0.77   0.85   0.93   1.03   1.25  395.0    1.0\n",
        "lambda[11]    0.82  6.0e-3   0.12   0.65   0.73    0.8   0.89   1.11  400.0    1.0\n",
        "lambda[12]     0.7  5.6e-3   0.11   0.54   0.61   0.68   0.76   0.97  404.0    1.0\n",
        "lambda[13]    0.58  5.1e-3    0.1   0.44    0.5   0.56   0.64   0.84  408.0    1.0\n",
        "lambda[14]    0.48  4.5e-3   0.09   0.36   0.41   0.46   0.53   0.72  412.0    1.0\n",
        "lambda[15]    0.39  3.9e-3   0.08   0.29   0.33   0.38   0.44    0.6  416.0    1.0\n",
        "lambda[16]    0.32  3.4e-3   0.07   0.23   0.26    0.3   0.35    0.5  419.0    1.0\n",
        "lambda[17]    0.25  2.8e-3   0.06   0.18   0.21   0.24   0.28   0.41  422.0    1.0\n",
        "lambda[18]     0.2  2.3e-3   0.05   0.14   0.16   0.19   0.22   0.33  425.0    1.0\n",
        "lambda[19]    0.16  1.9e-3   0.04   0.11   0.13   0.15   0.18   0.26  428.0    1.0\n",
        "lambda[20]    0.12  1.5e-3   0.03   0.08    0.1   0.12   0.14   0.21  431.0    1.0\n",
        "lambda[21]     0.1  1.2e-3   0.03   0.07   0.08   0.09   0.11   0.17  433.0    1.0\n",
        "lambda[22]    0.08  9.8e-4   0.02   0.05   0.07   0.08   0.09   0.14  436.0    1.0\n",
        "lambda[23]    0.07  8.2e-4   0.02   0.05   0.06   0.07   0.08   0.11  451.0    1.0\n",
        "lambda[24]    0.06  7.7e-4   0.02   0.04   0.05   0.06   0.07    0.1  455.0    1.0\n",
        "lambda[25]    0.06  8.5e-4   0.02   0.04   0.05   0.06   0.07    0.1  442.0    1.0\n",
        "lambda[26]    0.07  1.0e-3   0.02   0.04   0.05   0.07   0.08   0.12  433.0   1.01\n",
        "lambda[27]    0.08  1.3e-3   0.03   0.04   0.06   0.08    0.1   0.14  424.0   1.01\n",
        "lambda[28]     0.1  1.7e-3   0.03   0.05   0.07   0.09   0.12   0.18  421.0   1.01\n",
        "lambda[29]    0.12  2.1e-3   0.04   0.06   0.09   0.12   0.15   0.22  420.0   1.01\n",
        "lambda[30]    0.16  2.6e-3   0.05   0.07   0.12   0.15   0.19   0.28  419.0   1.01\n",
        "lambda[31]     0.2  3.3e-3   0.07   0.09   0.15   0.19   0.23   0.34  418.0   1.01\n",
        "lambda[32]    0.25  3.9e-3   0.08   0.12   0.19   0.24    0.3   0.42  418.0   1.01\n",
        "lambda[33]    0.31  4.7e-3    0.1   0.16   0.24    0.3   0.37   0.52  418.0   1.01\n",
        "lambda[34]    0.38  5.5e-3   0.11    0.2    0.3   0.37   0.45   0.62  417.0   1.01\n",
        "lambda[35]    0.47  6.3e-3   0.13   0.26   0.38   0.46   0.55   0.74  416.0   1.01\n",
        "lambda[36]    0.57  7.1e-3   0.14   0.32   0.47   0.56   0.66   0.87  416.0   1.01\n",
        "lambda[37]    0.68  7.8e-3   0.16    0.4   0.57   0.67   0.78   1.01  415.0   1.01\n",
        "lambda[38]     0.8  8.4e-3   0.17   0.49   0.68   0.79   0.92   1.14  414.0   1.01\n",
        "lambda[39]    0.93  8.8e-3   0.18    0.6    0.8   0.93   1.05   1.28  412.0   1.01\n",
        "lambda[40]    1.06  9.0e-3   0.18   0.71   0.93   1.06   1.19   1.41  410.0   1.01\n",
        "lambda[41]     1.2  8.9e-3   0.18   0.84   1.07    1.2   1.32   1.53  408.0   1.01\n",
        "lambda[42]    1.33  8.5e-3   0.17   0.97   1.21   1.33   1.45   1.62  405.0   1.01\n",
        "lambda[43]    1.45  7.7e-3   0.16   1.11   1.34   1.46   1.56    1.7  402.0   1.01\n",
        "lambda[44]    1.55  6.7e-3   0.13   1.25   1.47   1.57   1.65   1.75  396.0   1.01\n",
        "lambda[45]    1.64  5.4e-3   0.11   1.38   1.58   1.66   1.72   1.77  387.0   1.01\n",
        "lambda[46]     1.7  3.9e-3   0.08    1.5   1.66   1.72   1.76   1.77  373.0   1.01\n",
        "lambda[47]    1.74  2.7e-3   0.05    1.6   1.72   1.75   1.77   1.77  323.0   1.01\n",
        "lambda[48]    1.74  2.1e-3   0.04   1.63   1.73   1.76   1.77   1.77  383.0    1.0\n",
        "lambda[49]    1.72  3.0e-3   0.06   1.56    1.7   1.75   1.77   1.77  427.0    1.0\n",
        "lambda[50]    1.67  4.4e-3   0.09   1.45   1.63    1.7   1.74   1.77  434.0   1.01\n",
        "lambda[51]     1.6  5.9e-3   0.12   1.33   1.53   1.62   1.69   1.76  426.0   1.01\n",
        "lambda[52]     1.5  7.1e-3   0.15   1.19   1.41   1.52   1.61   1.73  422.0   1.01\n",
        "lambda[53]    1.39  8.0e-3   0.16   1.05   1.28    1.4   1.51   1.67  418.0   1.01\n",
        "lambda[54]    1.26  8.7e-3   0.18   0.92   1.14   1.27   1.39   1.58  414.0   1.01\n",
        "lambda[55]    1.13  9.0e-3   0.18   0.79   1.01   1.13   1.26   1.48  411.0   1.01\n",
        "lambda[56]     1.0  9.0e-3   0.18   0.66   0.87    1.0   1.12   1.35  409.0   1.01\n",
        "lambda[57]    0.87  8.7e-3   0.18   0.55   0.74   0.86   0.99   1.22  406.0   1.01\n",
        "lambda[58]    0.74  8.2e-3   0.16   0.45   0.62   0.73   0.85   1.08  404.0   1.01\n",
        "lambda[59]    0.63  7.6e-3   0.15   0.37   0.52   0.62   0.73   0.95  401.0   1.01\n",
        "lambda[60]    0.52  6.8e-3   0.14   0.29   0.42   0.51   0.61   0.81  399.0   1.01\n",
        "lambda[61]    0.43  6.0e-3   0.12   0.23   0.34   0.42    0.5   0.69  397.0   1.01\n",
        "lambda[62]    0.35  5.2e-3    0.1   0.18   0.27   0.33   0.41   0.58  395.0   1.01\n",
        "lambda[63]    0.28  4.4e-3   0.09   0.14   0.21   0.27   0.33   0.47  393.0   1.01\n",
        "lambda[64]    0.22  3.7e-3   0.07   0.11   0.17   0.21   0.26   0.38  392.0   1.01\n",
        "lambda[65]    0.17  3.0e-3   0.06   0.08   0.13   0.16   0.21   0.31  390.0   1.01\n",
        "lambda[66]    0.13  2.4e-3   0.05   0.06    0.1   0.12   0.16   0.24  388.0   1.01\n",
        "lambda[67]     0.1  2.0e-3   0.04   0.04   0.07   0.09   0.12   0.19  387.0   1.01\n",
        "lambda[68]    0.08  1.5e-3   0.03   0.03   0.05   0.07   0.09   0.15  385.0   1.01\n",
        "lambda[69]    0.06  1.2e-3   0.02   0.02   0.04   0.05   0.07   0.11  384.0   1.01\n",
        "lambda[70]    0.04  9.2e-4   0.02   0.02   0.03   0.04   0.05   0.09  383.0   1.01\n",
        "lambda[71]    0.03  7.0e-4   0.01   0.01   0.02   0.03   0.04   0.06  382.0   1.01\n",
        "lambda[72]    0.02  5.3e-4   0.01 9.4e-3   0.02   0.02   0.03   0.05  380.0   1.01\n",
        "lambda[73]    0.02  3.9e-4 7.6e-3 6.8e-3   0.01   0.02   0.02   0.04  380.0   1.01\n",
        "lambda[74]    0.01  2.9e-4 5.6e-3 4.9e-3 8.4e-3   0.01   0.02   0.03  379.0   1.01\n",
        "lambda[75]  9.0e-3  2.1e-4 4.1e-3 3.5e-3 6.1e-3 8.2e-3   0.01   0.02  378.0   1.01\n",
        "lambda[76]  6.5e-3  1.6e-4 3.0e-3 2.6e-3 4.4e-3 5.9e-3 8.0e-3   0.01  377.0   1.01\n",
        "lambda[77]  4.7e-3  1.1e-4 2.2e-3 1.9e-3 3.2e-3 4.3e-3 5.8e-3   0.01  377.0   1.01\n",
        "lambda[78]  3.4e-3  8.2e-5 1.6e-3 1.4e-3 2.3e-3 3.1e-3 4.2e-3 7.3e-3  377.0   1.01\n",
        "lambda[79]  2.5e-3  5.9e-5 1.1e-3 1.0e-3 1.7e-3 2.3e-3 3.1e-3 5.3e-3  377.0   1.01\n",
        "lambda[80]  1.9e-3  4.2e-5 8.2e-4 7.8e-4 1.3e-3 1.7e-3 2.2e-3 3.9e-3  377.0    nan\n",
        "lambda[81]  1.4e-3  3.0e-5 5.9e-4 6.2e-4 9.8e-4 1.3e-3 1.7e-3 2.9e-3  378.0    nan\n",
        "lambda[82]  1.1e-3  2.2e-5 4.3e-4 5.3e-4 8.0e-4 1.0e-3 1.3e-3 2.1e-3  379.0    nan\n",
        "lambda[83]  9.0e-4  1.6e-5 3.1e-4 4.8e-4 6.9e-4 8.4e-4 1.1e-3 1.6e-3  382.0    nan\n",
        "lambda[84]  8.0e-4  1.2e-5 2.3e-4 4.6e-4 6.5e-4 7.7e-4 9.2e-4 1.4e-3  387.0    nan\n",
        "lambda[85]  7.9e-4  9.7e-6 1.9e-4 4.7e-4 6.6e-4 7.7e-4 9.0e-4 1.2e-3  390.0    nan\n",
        "lambda[86]  8.6e-4  1.0e-5 1.9e-4 5.0e-4 7.2e-4 8.5e-410.0e-4 1.2e-3  368.0    nan\n",
        "lambda[87]  1.0e-3  1.2e-5 2.3e-4 5.7e-4 8.5e-4 1.0e-3 1.2e-3 1.4e-3  359.0    nan\n",
        "lambda[88]  1.3e-3  1.7e-5 3.1e-4 6.8e-4 1.1e-3 1.3e-3 1.5e-3 1.8e-3  353.0    nan\n",
        "lambda[89]  1.7e-3  2.3e-5 4.3e-4 8.5e-4 1.4e-3 1.7e-3 2.0e-3 2.4e-3  350.0    nan\n",
        "lambda[90]  2.3e-3  3.2e-5 6.0e-4 1.1e-3 1.8e-3 2.2e-3 2.7e-3 3.3e-3  348.0    nan\n",
        "lambda[91]  3.1e-3  4.5e-5 8.3e-4 1.5e-3 2.5e-3 3.1e-3 3.7e-3 4.5e-3  348.0    nan\n",
        "lambda[92]  4.2e-3  6.2e-5 1.2e-3 2.0e-3 3.4e-3 4.2e-3 5.1e-3 6.3e-3  348.0    1.0\n",
        "lambda[93]  5.9e-3  8.6e-5 1.6e-3 2.7e-3 4.7e-3 5.8e-3 7.1e-3 8.7e-3  348.0    1.0\n",
        "lambda[94]  8.1e-3  1.2e-4 2.2e-3 3.8e-3 6.5e-3 8.1e-3 9.8e-3   0.01  349.0    1.0\n",
        "lambda[95]    0.01  1.6e-4 3.0e-3 5.3e-3 8.9e-3   0.01   0.01   0.02  349.0    1.0\n",
        "lambda[96]    0.02  2.2e-4 4.1e-3 7.3e-3   0.01   0.02   0.02   0.02  350.0    1.0\n",
        "lambda[97]    0.02  3.0e-4 5.6e-3   0.01   0.02   0.02   0.03   0.03  351.0    1.0\n",
        "lambda[98]    0.03  4.0e-4 7.5e-3   0.01   0.02   0.03   0.03   0.04  352.0    1.0\n",
        "lambda[99]    0.04  5.3e-410.0e-3   0.02   0.03   0.04   0.05   0.06  353.0    1.0\n",
        "lambda[100]   0.05  7.0e-4   0.01   0.03   0.04   0.05   0.06   0.07  354.0    1.0\n",
        "lambda[101]   0.07  9.1e-4   0.02   0.04   0.06   0.07   0.08    0.1  355.0    1.0\n",
        "lambda[102]   0.09  1.2e-3   0.02   0.05   0.08   0.09   0.11   0.13  357.0    1.0\n",
        "lambda[103]   0.12  1.5e-3   0.03   0.06    0.1   0.12   0.14   0.17  358.0    1.0\n",
        "lambda[104]   0.16  1.9e-3   0.04   0.09   0.13   0.16   0.19   0.22  360.0    1.0\n",
        "lambda[105]    0.2  2.3e-3   0.04   0.11   0.17   0.21   0.24   0.28  362.0    1.0\n",
        "lambda[106]   0.26  2.8e-3   0.05   0.15   0.22   0.26    0.3   0.35  364.0    1.0\n",
        "lambda[107]   0.33  3.3e-3   0.06   0.19   0.28   0.33   0.38   0.43  367.0    1.0\n",
        "lambda[108]   0.41  3.9e-3   0.07   0.24   0.35   0.41   0.47   0.53  369.0    1.0\n",
        "lambda[109]    0.5  4.5e-3   0.09   0.31   0.44    0.5   0.57   0.63  372.0    1.0\n",
        "lambda[110]    0.6  5.1e-3    0.1   0.38   0.54   0.61   0.68   0.75  375.0    1.0\n",
        "lambda[111]   0.72  5.6e-3   0.11   0.47   0.65   0.73    0.8   0.88  378.0    1.0\n",
        "lambda[112]   0.84  6.0e-3   0.12   0.58   0.77   0.85   0.93   1.02  382.0    1.0\n",
        "lambda[113]   0.98  6.3e-3   0.12   0.69    0.9   0.99   1.07   1.16  386.0    1.0\n",
        "lambda[114]   1.11  6.3e-3   0.13   0.81   1.03   1.13   1.21   1.29  391.0    1.0\n",
        "lambda[115]   1.25  6.2e-3   0.12   0.95   1.17   1.26   1.34   1.42  396.0    1.0\n",
        "lambda[116]   1.38  5.9e-3   0.12   1.08   1.31   1.39   1.47   1.54  403.0    1.0\n",
        "lambda[117]   1.49  5.3e-3   0.11   1.22   1.43   1.51   1.58   1.63  412.0    1.0\n",
        "lambda[118]   1.59  4.4e-3   0.09   1.35   1.55   1.61   1.66   1.71  425.0    1.0\n",
        "lambda[119]   1.67  3.4e-3   0.07   1.48   1.64   1.69   1.73   1.75  448.0    1.0\n",
        "lambda[120]   1.73  2.2e-3   0.05   1.58   1.71   1.75   1.76   1.77  484.0    1.0\n",
        "lp__        -61.55    0.06   1.12 -64.79 -61.98 -61.23 -60.75 -60.45  306.0   1.01\n",
        "\n",
        "Samples were drawn using NUTS(diag_e) at Fri Aug  7 16:09:23 2015.\n",
        "For each parameter, n_eff is a crude measure of effective sample size,\n",
        "and Rhat is the potential scale reduction factor on split chains (at \n",
        "convergence, Rhat=1)."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2000\n",
        "2.50057793734\n",
        "0.0736417259432\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "<matplotlib.collections.PathCollection at 0x10bd2f910>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEACAYAAABcXmojAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEARBqCFKQRAEQaghSkEQBEGo8f8DThzJZqju8jcAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10aa583d0>"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Multilayer ReLU net\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import numpy\n",
      "\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "def relu(x):\n",
      "    return theano.tensor.switch(x<0, 0, x)\n",
      "\n",
      "class HiddenLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
      "                 activation=relu):\n",
      "        \"\"\"\n",
      "        Typical hidden layer of a MLP: units are fully-connected and have\n",
      "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
      "        and the bias vector b is of shape (n_out,).\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.dmatrix\n",
      "        :param input: a symbolic tensor of shape (n_examples, n_in)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: dimensionality of input\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of hidden units\n",
      "\n",
      "        :type activation: theano.Op or function\n",
      "        :param activation: Non linearity to be applied in the hidden\n",
      "                           layer\n",
      "        \"\"\"\n",
      "        self.input = input\n",
      "        if W is None:\n",
      "            W_values = .5 * numpy.random.randn(n_in, n_out)\n",
      "            #W_values = .1 * numpy.random.randn(n_in, n_out)\n",
      "\n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        if b is None:\n",
      "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
      "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "\n",
      "        lin_output = T.dot(input, self.W) + self.b\n",
      "        self.output = (\n",
      "            lin_output if activation is None\n",
      "            else activation(lin_output)\n",
      "        )\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "class MLP(object):\n",
      "\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: symbolic variable that describes the input of the\n",
      "        architecture (one minibatch)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: number of input units, the dimension of the space in\n",
      "        which the datapoints lie\n",
      "\n",
      "        :type n_hidden: int\n",
      "        :param n_hidden: number of hidden units\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "        which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        # Since we are dealing with a one hidden layer MLP, this will translate\n",
      "        # into a HiddenLayer with a tanh activation function connected to the\n",
      "        # LogisticRegression layer; the activation function can be replaced by\n",
      "        # sigmoid or any other nonlinear function\n",
      "        self.hiddenLayer1 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            activation=relu\n",
      "        )\n",
      "        \n",
      "        self.hiddenLayer2 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=self.hiddenLayer1.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out,\n",
      "            activation=relu\n",
      "        )\n",
      "        \n",
      "        self.y_pred = self.hiddenLayer2.output\n",
      "        \n",
      "        # the parameters of the model are the parameters of the two layers it is made out of\n",
      "        self.params = self.hiddenLayer1.params + self.hiddenLayer2.params\n",
      "    \n",
      "    def mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "    \n",
      "    def sym_mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean(((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "                      * ((self.y_pred[1] - y[0]) ** 2 + (self.y_pred[0] - y[1]) ** 2))\n",
      "        \n",
      "\n",
      "def shared_dataset(data_xy, borrow=True):\n",
      "        \"\"\" Function that loads the dataset into shared variables\n",
      "        \"\"\"\n",
      "        data_x, data_y, _ = data_xy\n",
      "        shared_x = theano.shared(numpy.asarray(data_x,\n",
      "                                               dtype='float32'),\n",
      "                                 borrow=borrow)\n",
      "        shared_y = theano.shared(numpy.asarray(data_y,\n",
      "                                               dtype='float32'),\n",
      "                                 borrow=borrow)\n",
      "        return shared_x, shared_y\n",
      "\n",
      "def train_nn(dataset, n_hidden=20, learning_rate=0.01, n_epochs=10, batch_size=20, test_data=None):\n",
      "    \"\"\"\n",
      "    Demonstrate stochastic gradient descent optimization for a multilayer\n",
      "    perceptron\n",
      "\n",
      "    :type learning_rate: float\n",
      "    :param learning_rate: learning rate used (factor for the stochastic\n",
      "    gradient\n",
      "\n",
      "    :type n_epochs: int\n",
      "    :param n_epochs: maximal number of epochs to run the optimizer\n",
      "\n",
      "    :type dataset: string\n",
      "    :param dataset: the path of the MNIST dataset file from\n",
      "                 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
      "\n",
      "\n",
      "   \"\"\"\n",
      "    train_set_x, train_set_y = shared_dataset(dataset)\n",
      "\n",
      "    # compute number of minibatches for training, validation and testing\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    \n",
      "    test_batch_size = 1\n",
      "    \n",
      "    ######################\n",
      "    # BUILD ACTUAL MODEL #\n",
      "    ######################\n",
      "    print '... building the model'\n",
      "\n",
      "    # allocate symbolic variables for the data\n",
      "    index = T.lscalar()  # index to a [mini]batch\n",
      "    x = T.fmatrix('x')   # input data from visual neurons\n",
      "    y = T.fmatrix('y')  # posterior\n",
      "\n",
      "    rng = numpy.random.RandomState(1234)\n",
      "\n",
      "    # construct the MLP class\n",
      "    nn = MLP(\n",
      "        rng=rng,\n",
      "        input=x,\n",
      "        #von mises/circular\n",
      "        #n_in=121, \n",
      "        n_in=61,\n",
      "        n_hidden=n_hidden,\n",
      "        n_out=2\n",
      "    )\n",
      "\n",
      "    # the cost we minimize during training is the mean squared error; cost is expressed\n",
      "    # here symbolically\n",
      "    cost = nn.mse(y)\n",
      "\n",
      "    # compute the gradient of cost with respect to theta (sotred in params)\n",
      "    # the resulting gradients will be stored in a list gparams\n",
      "    gparams = [T.grad(cost, param) for param in nn.params]\n",
      "\n",
      "    # specify how to update the parameters of the model as a list of\n",
      "    # (variable, update expression) pairs\n",
      "\n",
      "    # given two list the zip A = [a1, a2, a3, a4] and B = [b1, b2, b3, b4] of\n",
      "    # same length, zip generates a list C of same size, where each element\n",
      "    # is a pair formed from the two lists :\n",
      "    #    C = [(a1, b1), (a2, b2), (a3, b3), (a4, b4)]\n",
      "    updates = [\n",
      "        (param, param - learning_rate * gparam)\n",
      "        for param, gparam in zip(nn.params, gparams)\n",
      "    ]\n",
      "\n",
      "    # compiling a Theano function `train_model` that returns the cost, but\n",
      "    # in the same time updates the parameter of the model based on the rules\n",
      "    # defined in `updates`\n",
      "    train_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=cost,\n",
      "        updates=updates,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "\n",
      "    ###############\n",
      "    # TRAIN MODEL #\n",
      "    ###############\n",
      "    print '... training'\n",
      "\n",
      "    start_time = time.clock()\n",
      "\n",
      "    epoch = 0\n",
      "    done_looping = False\n",
      "\n",
      "    while (epoch < n_epochs) and (not done_looping):\n",
      "        epoch = epoch + 1\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "            \n",
      "            minibatch_avg_cost = train_model(minibatch_index)\n",
      "            \n",
      "            #print \"epoch \" + repr(epoch)\n",
      "            #print \"minibatch \" + repr(minibatch_index + 1) + \"/\" + repr(n_train_batches)\n",
      "            #print \"average cost \" + repr(minibatch_avg_cost)\n",
      "\n",
      "    end_time = time.clock()\n",
      "\n",
      "    def inspect_inputs(i, node, fn):\n",
      "        print i, node, \"input(s) value(s):\", [input[0] for input in fn.inputs]\n",
      "\n",
      "    def inspect_outputs(i, node, fn):\n",
      "        print \"output(s) value(s):\", [output[0] for output in fn.outputs]\n",
      "    \n",
      "    print 'testing'\n",
      "    test_batch_size = 1\n",
      "    test_set_x, test_set_y = shared_dataset(test_data)\n",
      "    test_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=nn.y_pred,\n",
      "        mode=theano.compile.MonitorMode(\n",
      "                        pre_func=inspect_inputs,\n",
      "                        post_func=inspect_outputs),\n",
      "        givens={\n",
      "            x: test_set_x[index * test_batch_size: (index + 1) * test_batch_size]\n",
      "        },\n",
      "    )\n",
      "    \n",
      "    true_ys = test_set_y.get_value()\n",
      "    pred_ys = numpy.zeros(len(true_ys))\n",
      "    for i in range(len(true_ys)):\n",
      "        #pred_ys[i] = test_model(i)\n",
      "        print test_model(i), true_ys[i]\n",
      "    \n",
      "    return pred_ys, true_ys\n",
      "\n",
      "#train_data = generate_popcode_data(ndata, nneuron, kappa_tc, c_50, r_max, \"poisson\", True)\n",
      "#test_data = generate_popcode_data(ndata, nneuron, kappa_tc, c_50, r_max, \"poisson\", True)\n",
      "train_data = generate_popcode_data2(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True)\n",
      "test_data = generate_popcode_data2(1, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True)\n",
      "nn, _ = train_nn(train_data, n_hidden=10, learning_rate=.01, n_epochs=100, test_data=test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "cannot import name opt",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-3-47b152f14c7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m//anaconda/lib/python2.7/site-packages/theano/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprinting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_module\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfoldl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfoldr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedUpdates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m//anaconda/lib/python2.7/site-packages/theano/scan_module/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0m__contact__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Razvan Pascanu <r.pascanu@gmail>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_module\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscan_opt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_views\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfoldl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfoldr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m//anaconda/lib/python2.7/site-packages/theano/scan_module/scan_opt.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_scalar_constant_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgof\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m//anaconda/lib/python2.7/site-packages/theano/tensor/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     TensorConstantSignature, TensorConstant)\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopt_uncanonicalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mblas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mImportError\u001b[0m: cannot import name opt"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}