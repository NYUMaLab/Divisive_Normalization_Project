{
 "metadata": {
  "name": "",
  "signature": "sha256:d0c30e67f54d8d1ef59b832b82639cd33a5a91bd4df89ebe1f6f4cdb7828553e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import math\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "import pystan\n",
      "import matplotlib.pyplot as plt\n",
      "import argparse\n",
      "\n",
      "nneuron = 61\n",
      "min_angle = -90\n",
      "max_angle = 90\n",
      "sprefs = np.linspace(min_angle, max_angle, nneuron)\n",
      "ndata = 3000\n",
      "eps = np.finfo(np.float64).eps\n",
      "\n",
      "r_max = 10\n",
      "sigtc_sq = float(10**2)\n",
      "sigtc = 10\n",
      "c_50 = 13.1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def random_s(ndata, sort):\n",
      "    s = np.random.rand(2, ndata) * 120 - 60\n",
      "    if sort:\n",
      "        s = np.sort(s, axis=0)\n",
      "    return s[0], s[1]\n",
      "\n",
      "def generate_trainset(ndata, r_max=10):\n",
      "    s_0, s_1 = random_s(ndata, True)\n",
      "    c_0, c_1 = np.ones((2, ndata)) * .5\n",
      "    r, s, c = generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True, s_0, s_1, c_0, c_1)\n",
      "    return r, s, c\n",
      "\n",
      "def generate_s1set(ndata):\n",
      "    s_0, s_1 = random_s(ndata, True)\n",
      "    c_0 = np.ones(ndata)\n",
      "    c_1 = np.zeros(ndata)\n",
      "    r, s, c = generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True, s_0, s_1, c_0, c_1)\n",
      "    return r, s, c\n",
      "    \n",
      "def generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, noise, sort, s_0, s_1, c_0, c_1):\n",
      "    c_rms = np.sqrt(np.square(c_0) + np.square(c_1))\n",
      "    sprefs_data = np.tile(sprefs, (ndata, 1))\n",
      "    s_0t = np.exp(-np.square((np.transpose(np.tile(s_0, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_0 = c_0 * s_0t.T\n",
      "    s_1t = np.exp(-np.square((np.transpose(np.tile(s_1, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_1 = c_1 * s_1t.T\n",
      "    #r = r_max * (stim_0 + stim_1)/(c_50 + c_rms)\n",
      "    r = r_max * (stim_0 + stim_1)\n",
      "    r = r.T\n",
      "    s = np.array((s_0, s_1)).T\n",
      "    s = s/90\n",
      "    c = np.array((c_0, c_1)).T\n",
      "    if noise == \"poisson\":\n",
      "        r = np.random.poisson(r) + 0.0\n",
      "    return r, s, c\n",
      "\n",
      "def generate_s_data(stim_0, stim_1, ndata, con_0=.5, con_1=.5, r_max=10):\n",
      "    #c_0, c_1 = np.ones((2, ndata)) * .5\n",
      "    c_0 = np.ones(ndata) * con_0\n",
      "    c_1 = np.ones(ndata) * con_1\n",
      "    s_0, s_1 = np.ones((2, ndata))\n",
      "    s_0 = s_0 * stim_0\n",
      "    s_1 = s_1 * stim_1\n",
      "    r, s, c = generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True, s_0, s_1, c_0, c_1)\n",
      "    return r, s, c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "neurons_code = \"\"\"\n",
      "    data {\n",
      "        int<lower=0> N; // number of neurons\n",
      "        int r[N]; // neural response\n",
      "        real sprefs[N]; // preferred stimuli\n",
      "        real<lower=0> c_1;\n",
      "        real<lower=0> c_2;\n",
      "        int r_max;\n",
      "        //real c_rms;\n",
      "        //real c_50;\n",
      "        //real<lower=0> sig_tc;\n",
      "        real<lower=0> sigtc_sq;\n",
      "    }\n",
      "    parameters {\n",
      "        real s_1;\n",
      "        //real s_2;\n",
      "        real<lower=s_1> s_2;\n",
      "    }\n",
      "    transformed parameters {\n",
      "        real lambda[N];\n",
      "        for (n in 1:N)\n",
      "            // lambda[n] <- r_max * ((c_1 * exp(normal_log(s_1, sprefs[n], sig_tc)) + c_2 * exp(normal_log(s_2, sprefs[n], sig_tc)))/(c_rms + c_50));\n",
      "            // lambda[n] <- r_max * (c_1 * exp(normal_log(s_1, sprefs[n], sig_tc)) + c_2 * exp(normal_log(s_2, sprefs[n], sig_tc)));\n",
      "            lambda[n] <- r_max * (c_1 * exp(- square(s_1 - sprefs[n])/(2 * sigtc_sq)) + c_2 * exp(- square(s_2 - sprefs[n])/(2 * sigtc_sq)));\n",
      "    }\n",
      "    model {\n",
      "        s_1 ~ uniform(-60, 60);\n",
      "        //s_2 ~ uniform(-60, 60);\n",
      "        s_2 ~ uniform(s_1, 60);\n",
      "        r ~ poisson(lambda);\n",
      "    }\n",
      "    \"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fisher_inf(s_0, s_1, c_0, c_1, r_max=10):\n",
      "    fs_0 = np.exp(-np.square((np.transpose(np.tile(s_0, (nneuron, 1))) - sprefs))/(2 * sigtc_sq))[0]\n",
      "    qs_0 = r_max * c_0 * fs_0\n",
      "    df_s0 = ((-s_0 + sprefs)/sigtc_sq) * qs_0\n",
      "    fs_1 = np.exp(-np.square((np.transpose(np.tile(s_1, (nneuron, 1))) - sprefs))/(2 * sigtc_sq))[0]\n",
      "    qs_1 = r_max * c_1 * fs_1\n",
      "    df_s1 = ((-s_1 + sprefs)/sigtc_sq) * qs_1\n",
      "    Q = qs_0 + qs_1\n",
      "    Q_inv = 1/Q\n",
      "    J_11 = np.sum(np.square(df_s0) * Q_inv)\n",
      "    J_22 = np.sum(np.square(df_s1) * Q_inv)\n",
      "    J_12 = J_21 = np.sum(df_s0 * df_s1 * Q_inv)\n",
      "    fisher = np.linalg.inv([[J_11, J_12], [J_21, J_22]])\n",
      "    return fisher\n",
      "\n",
      "def fit_optimal(r, sm, s=None, N=61, init=None, sprefs=sprefs, sampling=False, sort=False, c_1=.5, c_2=.5, c_50=13.1, r_max=10, c_rms=0.707106781, sig_tc=10, sigtc_sq=10**2):\n",
      "    ndata = len(r)\n",
      "    neurons_dat = {'N': 61,\n",
      "                   'r': r[0].astype(int),\n",
      "                   'sprefs': sprefs,\n",
      "                   'c_1': .5,\n",
      "                   'c_2': .5,\n",
      "                   'c_50': 13.1,\n",
      "                   'r_max': r_max,\n",
      "                   'c_rms': 0.707106781,\n",
      "                   'sig_tc': 10,\n",
      "                   'sigtc_sq': sigtc_sq}\n",
      "\n",
      "    optimal = np.zeros((2, ndata))\n",
      "    print init\n",
      "    for i in range(len(r)):\n",
      "        neurons_dat['r'] = r[i].astype(int)\n",
      "        if sampling:\n",
      "            op = sm.sampling(data=neurons_dat)\n",
      "            optimal[0][i], optimal[1][i] = np.mean(op['s_1']), np.mean(op['s_2'])\n",
      "        else:\n",
      "            if s.any():\n",
      "                init = {'s_1':max(s[0][i], -60 + eps), 's_2':min(s[1][i] + eps, 60 - eps)}\n",
      "                print init\n",
      "                op = sm.optimizing(data=neurons_dat, init=init)\n",
      "            elif not init:\n",
      "                op = sm.optimizing(data=neurons_dat)\n",
      "            else:     \n",
      "                op = sm.optimizing(data=neurons_dat, init=init)\n",
      "            optimal[0][i], optimal[1][i] = op['s_1'], op['s_2']\n",
      "        if sort:\n",
      "            optimal = np.sort(optimal, axis=0)\n",
      "    return optimal"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Multilayer ReLU net\n",
      "\"\"\"\n",
      "\n",
      "def relu(x):\n",
      "    return theano.tensor.switch(x<0, 0, x)\n",
      "\n",
      "class HiddenLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
      "                 activation=T.nnet.sigmoid):\n",
      "        \"\"\"\n",
      "        Typical hidden layer of a MLP: units are fully-connected and have\n",
      "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
      "        and the bias vector b is of shape (n_out,).\n",
      "\n",
      "        :type rng: np.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.dmatrix\n",
      "        :param input: a symbolic tensor of shape (n_examples, n_in)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: dimensionality of input\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of hidden units\n",
      "\n",
      "        :type activation: theano.Op or function\n",
      "        :param activation: Non linearity to be applied in the hidden\n",
      "                           layer\n",
      "        \"\"\"\n",
      "        self.input = input\n",
      "        if W is None:\n",
      "            W_values = (1/np.sqrt(n_in)) * np.random.randn(n_in, n_out)\n",
      "            \n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        if b is None:\n",
      "            b_values = np.zeros((n_out,), dtype=theano.config.floatX)\n",
      "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "\n",
      "        lin_output = T.dot(input, self.W) + self.b\n",
      "        self.output = (\n",
      "            lin_output if activation is None\n",
      "            else activation(lin_output)\n",
      "        )\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "class COMLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None):\n",
      "        \"\"\"\n",
      "        Layer with Center of Mass decoder\n",
      "        Params same as above\n",
      "        \"\"\"\n",
      "        self.input = input\n",
      "        if W is None:\n",
      "            W_values = (1/np.sqrt(n_in)) * np.random.randn(n_in, n_out)\n",
      "\n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        \n",
      "        self.ones = np.ones((n_in, n_out))\n",
      "        \n",
      "        self.output = T.dot(input, self.W)/T.dot(input, self.ones)\n",
      "        \n",
      "        # parameters of the model\n",
      "        self.params = [self.W]\n",
      "\n",
      "class MLP(object):\n",
      "\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
      "\n",
      "        :type rng: np.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: symbolic variable that describes the input of the\n",
      "        architecture (one minibatch)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: number of input units, the dimension of the space in\n",
      "        which the datapoints lie\n",
      "\n",
      "        :type n_hidden: int\n",
      "        :param n_hidden: number of hidden units\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "        which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        self.hiddenLayer1 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            #activation=T.nnet.sigmoid\n",
      "            activation=relu\n",
      "        )\n",
      "        \n",
      "        self.hiddenLayer2 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=self.hiddenLayer1.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out,\n",
      "            #activation=relu\n",
      "            activation=None\n",
      "        )\n",
      "        \n",
      "        self.y_pred = self.hiddenLayer2.output\n",
      "        \n",
      "        # the parameters of the model are the parameters of the two layers it is made out of\n",
      "        self.params = self.hiddenLayer1.params + self.hiddenLayer2.params\n",
      "    \n",
      "    def get_params(self):\n",
      "\n",
      "        params = {}\n",
      "        for param in self.params:\n",
      "            name = param.name\n",
      "            if name in params:\n",
      "                name = name, 2\n",
      "            params[name] = param.get_value()\n",
      "        return params\n",
      "    \n",
      "    def mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "    \n",
      "    def mse_s1(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean((self.y_pred[0] - y[0]) ** 2)\n",
      "    \n",
      "    def valid_mse(self, y):\n",
      "        return T.mean(((self.y_pred[0] * 90) - (y[0] * 90)) ** 2 + ((self.y_pred[1] * 90) - (y[1] * 90)) ** 2)\n",
      "    \n",
      "    def sym_mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean(((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "                      * ((self.y_pred[1] - y[0]) ** 2 + (self.y_pred[0] - y[1]) ** 2))\n",
      "        \n",
      "class COMMLP(object):\n",
      "\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"\n",
      "        Params same as above\n",
      "        \"\"\"\n",
      "\n",
      "        self.hiddenLayer1 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            activation=T.nnet.sigmoid\n",
      "        )\n",
      "        \n",
      "        self.hiddenLayer2 = COMLayer(\n",
      "            rng=rng,\n",
      "            input=self.hiddenLayer1.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out,\n",
      "        )\n",
      "        \n",
      "        self.y_pred = self.hiddenLayer2.output\n",
      "        \n",
      "        # the parameters of the model are the parameters of the two layers it is made out of\n",
      "        self.params = self.hiddenLayer1.params + self.hiddenLayer2.params\n",
      "    \n",
      "    def get_params(self):\n",
      "        params = {}\n",
      "        for param in self.params:\n",
      "            name = param.name\n",
      "            if name in params:\n",
      "                name = name, 2\n",
      "            params[name] = param.get_value()\n",
      "        return params\n",
      "    \n",
      "    def mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "    \n",
      "    def sym_mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean(((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "                      * ((self.y_pred[1] - y[0]) ** 2 + (self.y_pred[0] - y[1]) ** 2))\n",
      "        \n",
      "\n",
      "def shared_dataset(data_xy, borrow=True):\n",
      "        \"\"\" Function that loads the dataset into shared variables\n",
      "        \"\"\"\n",
      "        data_x, data_y, _ = data_xy\n",
      "        shared_x = theano.shared(np.asarray(data_x,\n",
      "                                               dtype='float32'),\n",
      "                                 borrow=borrow)\n",
      "        shared_y = theano.shared(np.asarray(data_y,\n",
      "                                               dtype='float32'),\n",
      "                                 borrow=borrow)\n",
      "        return shared_x, shared_y\n",
      "\n",
      "def train_nn(train_dataset, valid_dataset=None, n_hidden=20, learning_rate=0.01, n_epochs=10, batch_size=20, test_data=None, COM=False, RMSProp=False, nesterov=True, momentum=0, n_in=61, n_out=2):\n",
      "    \"\"\"\n",
      "    Demonstrate stochastic gradient descent optimization for a multilayer\n",
      "    perceptron\n",
      "\n",
      "    :type learning_rate: float\n",
      "    :param learning_rate: learning rate used (factor for the stochastic\n",
      "    gradient\n",
      "\n",
      "    :type n_epochs: int\n",
      "    :param n_epochs: maximal number of epochs to run the optimizer\n",
      "\n",
      "   \"\"\"\n",
      "    train_set_x, train_set_y = shared_dataset(train_dataset)\n",
      "    if valid_dataset:\n",
      "        valid_set_x, valid_set_y = shared_dataset(valid_dataset)\n",
      "\n",
      "    # compute number of minibatches for training, validation and testing\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    n_valid_batches = n_train_batches\n",
      "    \n",
      "    \n",
      "    ######################\n",
      "    # BUILD ACTUAL MODEL #\n",
      "    ######################\n",
      "    print '... building the model'\n",
      "\n",
      "    # allocate symbolic variables for the data\n",
      "    index = T.lscalar()  # index to a [mini]batch\n",
      "    x = T.fmatrix('x')   # input data from visual neurons\n",
      "    y = T.fmatrix('y')  # posterior\n",
      "\n",
      "    rng = np.random.RandomState(1234)\n",
      "\n",
      "    # construct the MLP class\n",
      "    nn = MLP(rng=rng, input=x, n_in=n_in, n_hidden=n_hidden, n_out=n_out)\n",
      "    \n",
      "    if COM:\n",
      "        nn = COMMLP(rng=rng, input=x, n_in=n_in, n_hidden=n_hidden, n_out=n_out)\n",
      "    else:\n",
      "        nn = MLP(rng=rng, input=x, n_in=n_in, n_hidden=n_hidden, n_out=n_out)\n",
      "\n",
      "    cost = nn.mse(y)\n",
      "    \n",
      "    def RMSprop(cost, params, learning_rate=0.001, rho=0.9, epsilon=1e-6, mu=0, nesterov=False):\n",
      "        gparams = T.grad(cost, params)\n",
      "        updates = []\n",
      "        for p, g in zip(params, gparams):\n",
      "            v = theano.shared(p.get_value() * 0.)\n",
      "            ms = theano.shared(p.get_value() * 0.)\n",
      "            ms_new = rho * ms + (1 - rho) * g ** 2\n",
      "            gradient_scaling = T.sqrt(ms_new + epsilon)\n",
      "            g = g / gradient_scaling\n",
      "            \"\"\"\n",
      "            (1) v_t = mu * v_t-1 - lr * gradient_f(params_t)\n",
      "            or\n",
      "            classic\n",
      "            (2) params_t = params_t-1 + v_t\n",
      "            nesterov\n",
      "            (7) params_t = params_t-1 + mu * v_t - lr * gradient_f(params_t-1)\n",
      "            (8) params_t = params_t-1 + mu**2 * v_t-1 - (1+mu) * lr * gradient_f(params_t-1)\n",
      "            \"\"\"\n",
      "            v_new = mu * v - (1 - mu) * learning_rate * g\n",
      "            if nesterov:\n",
      "                p_new = p + mu * v_new - (1 - mu) * learning_rate * g\n",
      "            else:\n",
      "                p_new = p + v_new\n",
      "            updates.append((ms, ms_new))\n",
      "            updates.append((v, v_new))\n",
      "            updates.append((p, p_new))\n",
      "                \n",
      "        return updates\n",
      "    \n",
      "    if RMSProp:\n",
      "        updates = RMSprop(cost, nn.params, learning_rate=learning_rate, mu=momentum, nesterov=nesterov)\n",
      "    else:\n",
      "        # compute the gradient of cost with respect to theta (sotred in params)\n",
      "        # the resulting gradients will be stored in a list gparams\n",
      "        gparams = [T.grad(cost, param) for param in nn.params]\n",
      "\n",
      "        # specify how to update the parameters of the model as a list of\n",
      "        # (variable, update expression) pairs\n",
      "\n",
      "        updates = [\n",
      "            (param, param - learning_rate * gparam)\n",
      "            for param, gparam in zip(nn.params, gparams)\n",
      "        ]\n",
      "    \n",
      "    def inspect_inputs(i, node, fn):\n",
      "        print i, node, \"input(s) value(s):\", [input[0] for input in fn.inputs]\n",
      "\n",
      "    def inspect_outputs(i, node, fn):\n",
      "        print \"output(s) value(s):\", [output[0] for output in fn.outputs]\n",
      "\n",
      "    # compiling a Theano function `train_model` that returns the cost, but\n",
      "    # in the same time updates the parameter of the model based on the rules\n",
      "    # defined in `updates`\n",
      "    train_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=cost,\n",
      "        updates=updates,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "    \n",
      "    validate_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=nn.valid_mse(y),\n",
      "        givens={\n",
      "            x: valid_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "            y: valid_set_y[index * batch_size:(index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "\n",
      "    ###############\n",
      "    # TRAIN MODEL #\n",
      "    ###############\n",
      "    print '... training'\n",
      "\n",
      "    start_time = time.clock()\n",
      "\n",
      "    epoch = 0 \n",
      "    done_looping = False\n",
      "    \n",
      "    if valid_dataset:\n",
      "        valid_mse = np.zeros(n_epochs)\n",
      "\n",
      "    while (epoch < n_epochs) and (not done_looping):\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "            \n",
      "            minibatch_avg_cost = train_model(minibatch_index)\n",
      "            \n",
      "        if valid_dataset:\n",
      "            validation_losses = [validate_model(i) for i in xrange(n_valid_batches)]\n",
      "            this_validation_loss = np.mean(validation_losses)\n",
      "            valid_mse[epoch] = this_validation_loss \n",
      "\n",
      "            print(\n",
      "                'epoch %i, minibatch %i/%i, validation error %f' %\n",
      "                (\n",
      "                    epoch,\n",
      "                    minibatch_index + 1,\n",
      "                    n_train_batches,\n",
      "                    this_validation_loss\n",
      "                )\n",
      "            )\n",
      "            \n",
      "        epoch = epoch + 1\n",
      "\n",
      "    end_time = time.clock()\n",
      "    \n",
      "    if valid_dataset:\n",
      "        return nn, x, valid_mse\n",
      "    return nn, x\n",
      "\n",
      "def test_nn(nn, nnx, test_data):\n",
      "    print 'testing'\n",
      "    test_batch_size = 1\n",
      "    test_set_x, test_set_y = shared_dataset(test_data)\n",
      "    index = T.lscalar()  # index to a [mini]batch\n",
      "    x = nnx   # input data from visual neurons\n",
      "    test_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=nn.y_pred,\n",
      "        givens={\n",
      "            x: test_set_x[index * test_batch_size: (index + 1) * test_batch_size]\n",
      "        },\n",
      "    )\n",
      "    \n",
      "    true_ys = test_set_y.get_value()\n",
      "    pred_ys = np.zeros((len(true_ys), 2))\n",
      "    for i in range(len(true_ys)):\n",
      "        pred_ys[i] = test_model(i)\n",
      "        #print test_model(i)[0], true_ys[i]\n",
      "        #print test_model(i)[0] * 90, true_ys[i]\n",
      "    \n",
      "    #print nn.get_params()\n",
      "    return pred_ys, true_ys"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Setting up models\n",
      "sm = pystan.StanModel(model_code=neurons_code)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_statistics(s1, s2, preds):\n",
      "    mean_s1 = np.mean(preds[0])\n",
      "    mean_s2 = np.mean(preds[1])\n",
      "    bias_s1 = mean_s1 - s1\n",
      "    bias_s2 = mean_s2 - s2\n",
      "    covmat = np.cov(preds)\n",
      "    var_s1 = covmat[0, 0]\n",
      "    var_s2 = covmat[1, 1]\n",
      "    cov = covmat[0, 1]\n",
      "    corr = cov / (np.sqrt(var_s1) * np.sqrt(var_s2))\n",
      "    stats = {'mean_s1': mean_s1, 'mean_s2': mean_s2, 'bias_s1': bias_s1, 'bias_s2': bias_s2, 'var_s1': var_s1, 'var_s2': var_s2, 'cov': cov, 'corr': corr}\n",
      "    return stats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_models(s_0, s_1, nn, nnx, sm):\n",
      "    init = {'s_1':s_0,\n",
      "            's_2':s_1}\n",
      "    test_data = generate_s_data(s_0, s_1, 3000)\n",
      "    #print test_data\n",
      "    nn_preds, _ = test_nn(nn, nnx, test_data)\n",
      "    nn_preds = nn_preds.T * 90\n",
      "    r, s, c = test_data\n",
      "    opt_preds = fit_optimal(r, sm)\n",
      "    #opt_preds = fit_optimal(r, sm, init=init)\n",
      "    return nn_preds, opt_preds"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_trials(nn, optimal, s_1, s_2, ntraindata):\n",
      "    plt.rc('text', usetex=True)\n",
      "    fig, ax = plt.subplots(1, 1)\n",
      "    ax.scatter(nn[0], nn[1], c='b', label='Neural Net')\n",
      "    ax.scatter(optimal[0], optimal[1], c='r', label='MLE')\n",
      "    ax.set_xlabel(r'\\hat{s_1}',fontsize=16)\n",
      "    ax.set_ylabel(r'\\hat{s_2}',fontsize=16)\n",
      "    ax.legend()\n",
      "    name = \"{s_1}_{s_2}_{ntraindata}.pdf\".format(s_1=s_1, s_2=s_2, ntraindata=ntraindata)\n",
      "    plt.show()\n",
      "    #fig.savefig(name)\n",
      "\n",
      "def get_contours(nn, optimal):\n",
      "    nn1 = nn[0]\n",
      "    nn2 = nn[1]\n",
      "    opt1 = optimal[0]\n",
      "    opt2 = optimal[1]\n",
      "    xmin = nn1.min()\n",
      "    xmax = nn1.max()\n",
      "    ymin = nn2.min()\n",
      "    ymax = nn2.max()\n",
      "    X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
      "    positions = np.vstack([X.ravel(), Y.ravel()])\n",
      "    values_nn = np.vstack([nn1, nn2])\n",
      "    kernel_nn = stats.gaussian_kde(values_nn)\n",
      "    Z_nn = np.reshape(kernel_nn(positions).T, X.shape)\n",
      "    values_opt = np.vstack([opt1, opt2])\n",
      "    kernel_opt = stats.gaussian_kde(values_opt)\n",
      "    Z_opt = np.reshape(kernel_opt(positions).T, X.shape)\n",
      "    \"\"\"\n",
      "    plt.contour(X, Y, Z_nn, colors='b')\n",
      "    plt.contour(X, Y, Z_opt, colors='r')\n",
      "    \"\"\"\n",
      "    return X, Y, Z_nn, Z_opt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s_arr = [-50, -30, -10, 0, 10, 30, 50]\n",
      "def test_combs(s_arr):\n",
      "    l_sarr = len(s_arr)\n",
      "    nn = [[None] * l_sarr for k in range(l_sarr)]\n",
      "    opt = [[None] * l_sarr for k in range(l_sarr)]\n",
      "    for i in range(l_sarr):\n",
      "        for j in range(i+1, l_sarr):\n",
      "            s1 = s_arr[i]\n",
      "            s2 = s_arr[j]\n",
      "            nn[i][j], opt[i][j] = test_models(s1, s2, nn2, nnx2, sm)\n",
      "    return nn, opt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_contours(nn, opt):\n",
      "    axes().set_aspect('equal')\n",
      "    plt.xlim(-60, 60)\n",
      "    plt.ylim(-60, 60)\n",
      "    axes().set_xlabel(r'\\hat{s_1}',fontsize=16)\n",
      "    axes().set_ylabel(r'\\hat{s_2}',fontsize=16)\n",
      "    for i in range(l_sarr):\n",
      "        for j in range(i+1, l_sarr):\n",
      "            X, Y, Z_nn, Z_opt = get_contours(nn[i][j], opt[i][j])\n",
      "            plt.contour(X, Y, Z_nn, colors='b', levels=[0.003])\n",
      "            plt.contour(X, Y, Z_opt, colors='r', levels=[0.003])\n",
      "            #plt.scatter(nn[i][j][0], nn[i][j][1], c='g', label='Neural Net')\n",
      "            #plt.scatter(opt[i][j][0], opt[i][j][1], c='y', label='MLE')\n",
      "    fig.savefig(\"stimplot.pdf\")\n",
      "plt.figure(figsize=(10,10))\n",
      "plot_contours(nn, opt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def python_relu(x):\n",
      "    return x * (x > 0)\n",
      "nn_params = nn2.get_params()\n",
      "hl_outW = nn_params[('W', 2)]\n",
      "b = nn_params['b']\n",
      "W = nn_params['W']\n",
      "r, _, _ = generate_s_data(-10, 0, 3000)\n",
      "trials = python_relu(np.dot(r, W) + b)\n",
      "hl_cov = np.cov(trials.T)\n",
      "print hl_cov\n",
      "WTQ = np.dot(hl_outW.T, hl_cov)\n",
      "cov = np.dot(WTQ, hl_outW)\n",
      "approx_cov = np.dot(hl_outW.T, hl_outW)\n",
      "imgplot = plt.imshow(cov, extent=[0, 1, 0, 1])\n",
      "plt.colorbar()\n",
      "print cov\n",
      "print approx_cov"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn1_1 = np.mean(np.square(nn_preds1[0] - s1))\n",
      "nn1_2 = np.mean(np.square(nn_preds1[1] - s2))\n",
      "nn2_1 = np.mean(np.square(nn_preds2[0] - s1))\n",
      "nn2_2 = np.mean(np.square(nn_preds2[1] - s2))\n",
      "nn3_1 = np.mean(np.square(nn_preds3[0] - s1))\n",
      "nn3_2 = np.mean(np.square(nn_preds3[1] - s2))\n",
      "opt1_1 = np.mean(np.square(opt_preds1[0] - s1))\n",
      "opt1_2 = np.mean(np.square(opt_preds1[1] - s2))\n",
      "opt2_1 = np.mean(np.square(opt_preds2[0] - s1))\n",
      "opt2_2 = np.mean(np.square(opt_preds2[1] - s2))\n",
      "opt3_1 = np.mean(np.square(opt_preds3[0] - s1))\n",
      "opt3_2 = np.mean(np.square(opt_preds3[1] - s2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "plot1 = [nn1_1, nn2_1, nn3_1, opt1_1]\n",
      "plot2 = [nn1_2, nn2_2, nn3_2, opt1_2]\n",
      "def plot_MSEs()\n",
      "ind = np.arange(4)\n",
      "width = 0.35\n",
      "fig, ax = plt.subplots()\n",
      "rects1 = ax.bar(ind, plot1, width)\n",
      "rects2 = ax.bar(ind+width, plot2, width, color='y')\n",
      "ax.set_xticks(ind+width)\n",
      "ax.set_xticklabels(('NN 3000', 'NN 20000', 'NN 100000', 'Optimal'))\n",
      "ax.legend((rects1[0], rects2[0]), ('s 1', 's 2'))\n",
      "fig.savefig('mse.pdf')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data1 = generate_trainset(20000)\n",
      "train_data2 = generate_trainset(60000)\n",
      "train_data3 = generate_trainset(100000)\n",
      "nn1, nnx1 = train_nn(train_data1, n_hidden=20, learning_rate=.001, n_epochs=100)\n",
      "nn2, nnx2 = train_nn(train_data2, n_hidden=20, learning_rate=.001, n_epochs=100)\n",
      "nn3, nnx3 = train_nn(train_data3, n_hidden=20, learning_rate=.001, n_epochs=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s1 = -30\n",
      "num_deltas = 60\n",
      "mean_s1_nn1 = [None] * num_deltas\n",
      "mean_s2_nn1 = [None] * num_deltas\n",
      "bias_s1_nn1 = [None] * num_deltas\n",
      "bias_s2_nn1 = [None] * num_deltas\n",
      "var_s1_nn1 = [None] * num_deltas\n",
      "var_s2_nn1 = [None] * num_deltas\n",
      "corr_nn1 = [None] * num_deltas\n",
      "cov_nn1 = [None] * num_deltas\n",
      "\n",
      "mean_s1_nn2 = [None] * num_deltas\n",
      "mean_s2_nn2 = [None] * num_deltas\n",
      "bias_s1_nn2 = [None] * num_deltas\n",
      "bias_s2_nn2 = [None] * num_deltas\n",
      "var_s1_nn2 = [None] * num_deltas\n",
      "var_s2_nn2 = [None] * num_deltas\n",
      "corr_nn2 = [None] * num_deltas\n",
      "cov_nn2 = [None] * num_deltas\n",
      "\n",
      "mean_s1_nn3 = [None] * num_deltas\n",
      "mean_s2_nn3 = [None] * num_deltas\n",
      "bias_s1_nn3 = [None] * num_deltas\n",
      "bias_s2_nn3 = [None] * num_deltas\n",
      "var_s1_nn3 = [None] * num_deltas\n",
      "var_s2_nn3 = [None] * num_deltas\n",
      "corr_nn3 = [None] * num_deltas\n",
      "cov_nn3 = [None] * num_deltas\n",
      "\n",
      "mean_s1_opt = [None] * num_deltas\n",
      "mean_s2_opt = [None] * num_deltas\n",
      "bias_s1_opt = [None] * num_deltas\n",
      "bias_s2_opt = [None] * num_deltas\n",
      "var_s1_opt = [None] * num_deltas\n",
      "var_s2_opt = [None] * num_deltas\n",
      "corr_opt = [None] * num_deltas\n",
      "cov_opt = [None] * num_deltas\n",
      "\n",
      "var_s1_fisher = [None] * num_deltas\n",
      "var_s2_fisher = [None] * num_deltas\n",
      "corr_fisher = [None] * num_deltas\n",
      "\n",
      "for delta_s in range(num_deltas):\n",
      "    test_data = generate_s_data(s1, s1 + delta_s, 3000)\n",
      "    nn_preds1, _ = test_nn(nn1, nnx1, test_data)\n",
      "    nn_preds2, _ = test_nn(nn2, nnx2, test_data)\n",
      "    nn_preds3, _ = test_nn(nn3, nnx3, test_data)\n",
      "    \n",
      "    nn_preds1 = nn_preds1.T * 90\n",
      "    nn_preds2 = nn_preds2.T * 90\n",
      "    nn_preds3 = nn_preds3.T * 90\n",
      "    r, _, _ = test_data\n",
      "    opt_preds = fit_optimal(r, sm)\n",
      "    \n",
      "    nn_stats1 = get_statistics(s1, s1 + delta_s, nn_preds1)\n",
      "    nn_stats2 = get_statistics(s1, s1 + delta_s, nn_preds2)\n",
      "    nn_stats3 = get_statistics(s1, s1 + delta_s, nn_preds3)\n",
      "    opt_stats = get_statistics(s1, s1 + delta_s, opt_preds)\n",
      "    \n",
      "    if delta_s > 0:\n",
      "        FI = fisher_inf(s1, s1 + delta_s, .5, .5)\n",
      "        var_s1_fisher[delta_s] = FI[0, 0]\n",
      "        var_s2_fisher[delta_s] = FI[1, 1]\n",
      "        cov_fisher[delta_s] = FI[0, 1]\n",
      "        \n",
      "    mean_s1_nn1[delta_s] = nn_stats1['mean_s1']\n",
      "    mean_s2_nn1[delta_s] = nn_stats1['mean_s2']\n",
      "    bias_s1_nn1[delta_s] = nn_stats1['bias_s1']\n",
      "    bias_s2_nn1[delta_s] = nn_stats1['bias_s2']\n",
      "    var_s1_nn1[delta_s] = nn_stats1['var_s1']\n",
      "    var_s2_nn1[delta_s] = nn_stats1['var_s2']\n",
      "    corr_nn1[delta_s] = nn_stats1['corr']\n",
      "    cov_nn1[delta_s] = nn_stats1['cov']\n",
      "    \n",
      "    mean_s1_nn2[delta_s] = nn_stats2['mean_s1']\n",
      "    mean_s2_nn2[delta_s] = nn_stats2['mean_s2']\n",
      "    bias_s1_nn2[delta_s] = nn_stats2['bias_s1']\n",
      "    bias_s2_nn2[delta_s] = nn_stats2['bias_s2']\n",
      "    var_s1_nn2[delta_s] = nn_stats2['var_s1']\n",
      "    var_s2_nn2[delta_s] = nn_stats2['var_s2']\n",
      "    corr_nn2[delta_s] = nn_stats2['corr']\n",
      "    cov_nn2[delta_s] = nn_stats2['cov']\n",
      "    \n",
      "    mean_s1_nn3[delta_s] = nn_stats3['mean_s1']\n",
      "    mean_s2_nn3[delta_s] = nn_stats3['mean_s2']\n",
      "    bias_s1_nn3[delta_s] = nn_stats3['bias_s1']\n",
      "    bias_s2_nn3[delta_s] = nn_stats3['bias_s2']\n",
      "    var_s1_nn3[delta_s] = nn_stats3['var_s1']\n",
      "    var_s2_nn3[delta_s] = nn_stats3['var_s2']\n",
      "    corr_nn3[delta_s] = nn_stats3['corr']\n",
      "    cov_nn3[delta_s] = nn_stats3['cov']\n",
      "    \n",
      "    mean_s1_opt[delta_s] = opt_stats['mean_s1']\n",
      "    mean_s2_opt[delta_s] = opt_stats['mean_s2']\n",
      "    bias_s1_opt[delta_s] = opt_stats['bias_s1']\n",
      "    bias_s2_opt[delta_s] = opt_stats['bias_s2']\n",
      "    var_s1_opt[delta_s] = opt_stats['var_s1']\n",
      "    var_s2_opt[delta_s] = opt_stats['var_s2']\n",
      "    corr_opt[delta_s] = opt_stats['corr']\n",
      "    cov_opt[delta_s] = opt_stats['cov']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5)\n",
      "ax1.plot(range(1, 61), bias_s1_nn1, c='y', label='Neural Net 1')\n",
      "ax2.plot(range(1, 61), bias_s2_nn1, c='y', label='Neural Net 1')\n",
      "ax3.plot(range(1, 61), var_s1_nn1, c='y', label='Neural Net 1')\n",
      "ax4.plot(range(1, 61), var_s2_nn1, c='y', label='Neural Net 1')\n",
      "ax5.plot(range(1, 61), cov_nn1, c='y', label='Neural Net 1')\n",
      "#20,000 training/50 units in blue\n",
      "ax1.plot(range(1, 61), bias_s1_nn2, c='b', label='Neural Net 2')\n",
      "ax2.plot(range(1, 61), bias_s2_nn2, c='b', label='Neural Net 2')\n",
      "ax3.plot(range(1, 61), var_s1_nn2, c='b', label='Neural Net 2')\n",
      "ax4.plot(range(1, 61), var_s2_nn2, c='b', label='Neural Net 2')\n",
      "ax5.plot(range(1, 61), cov_nn2, c='b', label='Neural Net 2')\n",
      "#100,000 training/50 units in magenta\n",
      "ax1.plot(range(1, 61), bias_s1_nn3, c='m', label='Neural Net 3')\n",
      "ax2.plot(range(1, 61), bias_s2_nn3, c='m', label='Neural Net 3')\n",
      "ax3.plot(range(1, 61), var_s1_nn3, c='m', label='Neural Net 3')\n",
      "ax4.plot(range(1, 61), var_s2_nn3, c='m', label='Neural Net 3')\n",
      "ax5.plot(range(1, 61), cov_nn3, c='m', label='Neural Net 3') \n",
      "#optimal in red\n",
      "ax1.plot(range(1, 61), bias_s1_opt, c='r', label='MLE')\n",
      "ax2.plot(range(1, 61), bias_s2_opt, c='r', label='MLE')\n",
      "ax3.plot(range(1, 61), var_s1_opt, c='r', label='MLE')\n",
      "ax4.plot(range(1, 61), var_s2_opt, c='r', label='MLE')\n",
      "ax5.plot(range(1, 61), cov_opt, c='r', label='MLE')\n",
      "#Fisher information in green\n",
      "ax3.plot(range(9, 61), var_s1_fisher[8:60], c='g', label='Fisher')\n",
      "ax4.plot(range(9, 61), var_s2_fisher[8:60], c='g', label='Fisher')\n",
      "ax5.plot(range(9, 61), cov_fisher[8:60], c='g', label='Fisher')\n",
      "ax1.locator_params(axis = 'y', nbins = 4)\n",
      "ax2.locator_params(axis = 'y', nbins = 4)\n",
      "ax3.locator_params(axis = 'y', nbins = 4)\n",
      "ax4.locator_params(axis = 'y', nbins = 4)\n",
      "ax5.locator_params(axis = 'y', nbins = 4)\n",
      "#ax1.legend()\n",
      "ax1.set_title(\"Bias $s_1$\")\n",
      "ax2.set_title(\"Bias $s_2$\")\n",
      "ax3.set_title(r'Variance $s_1$')\n",
      "ax4.set_title(r'Variance $s_2$')\n",
      "ax5.set_title('Covariance')\n",
      "ax5.set_xlabel(r'$\\Delta$ s')\n",
      "f.set_size_inches(10,10)\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = range(-30, 30)\n",
      "neg_sd_nn1 = mean_s1_nn1 - np.sqrt(var_s1_nn1)\n",
      "pos_sd_nn1 = mean_s1_nn1 + np.sqrt(var_s1_nn1)\n",
      "neg_sd_nn2 = mean_s1_nn2 - np.sqrt(var_s1_nn2)\n",
      "pos_sd_nn2 = mean_s1_nn2 + np.sqrt(var_s1_nn2)\n",
      "neg_sd_nn3 = mean_s1_nn3 - np.sqrt(var_s1_nn3)\n",
      "pos_sd_nn3 = mean_s1_nn3 + np.sqrt(var_s1_nn3)\n",
      "neg_sd_opt = mean_s1_opt - np.sqrt(var_s1_opt)\n",
      "pos_sd_opt = mean_s1_opt + np.sqrt(var_s1_opt)\n",
      "plt.rc('text', usetex=True)\n",
      "plt.plot(s, mean_s1_nn1, c='y', label='Mean')\n",
      "plt.plot(s, neg_sd_nn1, c='y', alpha=.25, label='SD')\n",
      "plt.plot(s, pos_sd_nn1, c='y', alpha=.25, label='SD')\n",
      "plt.plot(s, mean_s1_nn2, c='b', label='Mean')\n",
      "plt.plot(s, neg_sd_nn2, c='b', alpha=.25, label='SD')\n",
      "plt.plot(s, pos_sd_nn2, c='b', alpha=.25, label='SD')\n",
      "plt.plot(s, mean_s1_nn3, c='m', label='Mean')\n",
      "plt.plot(s, neg_sd_nn3, c='m', alpha=.25, label='SD')\n",
      "plt.plot(s, pos_sd_nn3, c='m', alpha=.25, label='SD')\n",
      "plt.plot(s, mean_s1_opt, c='r', label='Mean')\n",
      "plt.plot(s, neg_sd_opt, c='r', alpha=.25, label='SD')\n",
      "plt.plot(s, pos_sd_opt, c='r', alpha=.25, label='SD')\n",
      "plt.xlabel(r'$s_2$',fontsize=16)\n",
      "plt.ylabel(r'\\hat{s_1}',fontsize=16)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = range(-30, 30)\n",
      "neg_sd_nn1 = mean_s2_nn1 - np.sqrt(var_s2_nn1)\n",
      "pos_sd_nn1 = mean_s2_nn1 + np.sqrt(var_s2_nn1)\n",
      "neg_sd_nn2 = mean_s2_nn2 - np.sqrt(var_s2_nn2)\n",
      "pos_sd_nn2 = mean_s2_nn2 + np.sqrt(var_s2_nn2)\n",
      "neg_sd_nn3 = mean_s2_nn3 - np.sqrt(var_s2_nn3)\n",
      "pos_sd_nn3 = mean_s2_nn3 + np.sqrt(var_s2_nn3)\n",
      "neg_sd_opt = mean_s2_opt - np.sqrt(var_s2_opt)\n",
      "pos_sd_opt = mean_s2_opt + np.sqrt(var_s2_opt)\n",
      "plt.rc('text', usetex=True)\n",
      "plt.plot(s, mean_s2_nn1, c='y', label='Mean')\n",
      "plt.plot(s, neg_sd_nn1, c='y', alpha=.5, label='SD')\n",
      "plt.plot(s, pos_sd_nn1, c='y', alpha=.5, label='SD')\n",
      "plt.plot(s, mean_s2_nn2, c='b', label='Mean')\n",
      "plt.plot(s, neg_sd_nn2, c='b', alpha=.5, label='SD')\n",
      "plt.plot(s, pos_sd_nn2, c='b', alpha=.5, label='SD')\n",
      "plt.plot(s, mean_s2_nn3, c='m', label='Mean')\n",
      "plt.plot(s, neg_sd_nn3, c='m', alpha=.5, label='SD')\n",
      "plt.plot(s, pos_sd_nn3, c='m', alpha=.5, label='SD')\n",
      "plt.plot(s, mean_s2_opt, c='r', label='Mean')\n",
      "plt.plot(s, neg_sd_opt, c='r', alpha=.5, label='SD')\n",
      "plt.plot(s, pos_sd_opt, c='r', alpha=.5, label='SD')\n",
      "plt.xlabel(r'$s_2$',fontsize=16)\n",
      "plt.ylabel(r'\\hat{s_2}',fontsize=16)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s1 = -29\n",
      "s2 = 30\n",
      "num_deltas = 60\n",
      "mean_s1_nn1_opp = [None] * num_deltas\n",
      "mean_s2_nn1_opp = [None] * num_deltas\n",
      "bias_s1_nn1_opp = [None] * num_deltas\n",
      "bias_s2_nn1_opp = [None] * num_deltas\n",
      "var_s1_nn1_opp = [None] * num_deltas\n",
      "var_s2_nn1_opp = [None] * num_deltas\n",
      "corr_nn1_opp = [None] * num_deltas\n",
      "cov_nn1_opp = [None] * num_deltas\n",
      "\n",
      "mean_s1_nn2_opp = [None] * num_deltas\n",
      "mean_s2_nn2_opp = [None] * num_deltas\n",
      "bias_s1_nn2_opp = [None] * num_deltas\n",
      "bias_s2_nn2_opp = [None] * num_deltas\n",
      "var_s1_nn2_opp = [None] * num_deltas\n",
      "var_s2_nn2_opp = [None] * num_deltas\n",
      "corr_nn2_opp = [None] * num_deltas\n",
      "cov_nn2_opp = [None] * num_deltas\n",
      "\n",
      "mean_s1_nn3_opp = [None] * num_deltas\n",
      "mean_s2_nn3_opp = [None] * num_deltas\n",
      "bias_s1_nn3_opp = [None] * num_deltas\n",
      "bias_s2_nn3_opp = [None] * num_deltas\n",
      "var_s1_nn3_opp = [None] * num_deltas\n",
      "var_s2_nn3_opp = [None] * num_deltas\n",
      "corr_nn3_opp = [None] * num_deltas\n",
      "cov_nn3_opp = [None] * num_deltas\n",
      "\n",
      "mean_s1_opt_opp = [None] * num_deltas\n",
      "mean_s2_opt_opp = [None] * num_deltas\n",
      "bias_s1_opt_opp = [None] * num_deltas\n",
      "bias_s2_opt_opp = [None] * num_deltas\n",
      "var_s1_opt_opp = [None] * num_deltas\n",
      "var_s2_opt_opp = [None] * num_deltas\n",
      "corr_opt_opp = [None] * num_deltas\n",
      "cov_opt_opp = [None] * num_deltas\n",
      "\n",
      "var_s1_fisher_opp = [None] * num_deltas\n",
      "var_s2_fisher_opp = [None] * num_deltas\n",
      "corr_fisher_opp = [None] * num_deltas\n",
      "\n",
      "for delta_s in range(num_deltas):\n",
      "    test_data = generate_s_data(s1 + delta_s, s2, 3000)\n",
      "    nn_preds1, _ = test_nn(nn1, nnx1, test_data)\n",
      "    nn_preds2, _ = test_nn(nn2, nnx2, test_data)\n",
      "    nn_preds3, _ = test_nn(nn3, nnx3, test_data)\n",
      "    \n",
      "    nn_preds1 = nn_preds1.T * 90\n",
      "    nn_preds2 = nn_preds2.T * 90\n",
      "    nn_preds3 = nn_preds3.T * 90\n",
      "    r, _, _ = test_data\n",
      "    opt_preds = fit_optimal(r, sm)\n",
      "    \n",
      "    nn_stats1 = get_statistics(s1 + delta_s, s2, nn_preds1)\n",
      "    nn_stats2 = get_statistics(s1 + delta_s, s2, nn_preds2)\n",
      "    nn_stats3 = get_statistics(s1 + delta_s, s2, nn_preds3)\n",
      "    opt_stats = get_statistics(s1 + delta_s, s2, opt_preds)\n",
      "    \n",
      "    if delta_s > 0:\n",
      "        FI = fisher_inf(s1 + delta_s, s2, .5, .5)\n",
      "        var_s1_fisher[delta_s] = FI[0, 0]\n",
      "        var_s2_fisher[delta_s] = FI[1, 1]\n",
      "        cov_fisher[delta_s] = FI[0, 1]\n",
      "        \n",
      "    mean_s1_nn1_opp[delta_s] = nn_stats1['mean_s1']\n",
      "    mean_s2_nn1_opp[delta_s] = nn_stats1['mean_s2']\n",
      "    bias_s1_nn1_opp[delta_s] = nn_stats1['bias_s1']\n",
      "    bias_s2_nn1_opp[delta_s] = nn_stats1['bias_s2']\n",
      "    var_s1_nn1_opp[delta_s] = nn_stats1['var_s1']\n",
      "    var_s2_nn1_opp[delta_s] = nn_stats1['var_s2']\n",
      "    corr_nn1_opp[delta_s] = nn_stats1['corr']\n",
      "    cov_nn1_opp[delta_s] = nn_stats1['cov']\n",
      "    \n",
      "    mean_s1_nn2_opp[delta_s] = nn_stats2['mean_s1']\n",
      "    mean_s2_nn2_opp[delta_s] = nn_stats2['mean_s2']\n",
      "    bias_s1_nn2_opp[delta_s] = nn_stats2['bias_s1']\n",
      "    bias_s2_nn2_opp[delta_s] = nn_stats2['bias_s2']\n",
      "    var_s1_nn2_opp[delta_s] = nn_stats2['var_s1']\n",
      "    var_s2_nn2_opp[delta_s] = nn_stats2['var_s2']\n",
      "    corr_nn2_opp[delta_s] = nn_stats2['corr']\n",
      "    cov_nn2_opp[delta_s] = nn_stats2['cov']\n",
      "    \n",
      "    mean_s1_nn3_opp[delta_s] = nn_stats3['mean_s1']\n",
      "    mean_s2_nn3_opp[delta_s] = nn_stats3['mean_s2']\n",
      "    bias_s1_nn3_opp[delta_s] = nn_stats3['bias_s1']\n",
      "    bias_s2_nn3_opp[delta_s] = nn_stats3['bias_s2']\n",
      "    var_s1_nn3_opp[delta_s] = nn_stats3['var_s1']\n",
      "    var_s2_nn3_opp[delta_s] = nn_stats3['var_s2']\n",
      "    corr_nn3_opp[delta_s] = nn_stats3['corr']\n",
      "    cov_nn3_opp[delta_s] = nn_stats3['cov']\n",
      "    \n",
      "    mean_s1_opt_opp[delta_s] = opt_stats['mean_s1']\n",
      "    mean_s2_opt_opp[delta_s] = opt_stats['mean_s2']\n",
      "    bias_s1_opt_opp[delta_s] = opt_stats['bias_s1']\n",
      "    bias_s2_opt_opp[delta_s] = opt_stats['bias_s2']\n",
      "    var_s1_opt_opp[delta_s] = opt_stats['var_s1']\n",
      "    var_s2_opt_opp[delta_s] = opt_stats['var_s2']\n",
      "    corr_opt_opp[delta_s] = opt_stats['corr']\n",
      "    cov_opt_opp[delta_s] = opt_stats['cov']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ds = range(59, -1, -1)\n",
      "f, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5)\n",
      "#20,000 training in blue\n",
      "ax1.plot(ds, bias_s1_nn1_opp, c='y', label='Neural Net 1')\n",
      "ax2.plot(ds, bias_s2_nn1_opp, c='y', label='Neural Net 1')\n",
      "ax3.plot(ds, var_s1_nn1_opp, c='y', label='Neural Net 1')\n",
      "ax4.plot(ds, var_s2_nn1_opp, c='y', label='Neural Net 1')\n",
      "ax5.plot(ds, cov_nn1_opp, c='y', label='Neural Net 1')\n",
      "#60,000 training in blue\n",
      "ax1.plot(ds, bias_s1_nn2_opp, c='b', label='Neural Net 2')\n",
      "ax2.plot(ds, bias_s2_nn2_opp, c='b', label='Neural Net 2')\n",
      "ax3.plot(ds, var_s1_nn2_opp, c='b', label='Neural Net 2')\n",
      "ax4.plot(ds, var_s2_nn2_opp, c='b', label='Neural Net 2')\n",
      "ax5.plot(ds, cov_nn2_opp, c='b', label='Neural Net 2')\n",
      "#100,000 training in magenta\n",
      "ax1.plot(ds, bias_s1_nn3_opp, c='m', label='Neural Net 3')\n",
      "ax2.plot(ds, bias_s2_nn3_opp, c='m', label='Neural Net 3')\n",
      "ax3.plot(ds, var_s1_nn3_opp, c='m', label='Neural Net 3')\n",
      "ax4.plot(ds, var_s2_nn3_opp, c='m', label='Neural Net 3')\n",
      "ax5.plot(ds, cov_nn3_opp, c='m', label='Neural Net 3') \n",
      "#optimal in red\n",
      "ax1.plot(ds, bias_s1_opt_opp, c='r', label='MLE')\n",
      "ax2.plot(ds, bias_s2_opt_opp, c='r', label='MLE')\n",
      "ax3.plot(ds, var_s1_opt_opp, c='r', label='MLE')\n",
      "ax4.plot(ds, var_s2_opt_opp, c='r', label='MLE')\n",
      "ax5.plot(ds, cov_opt_opp, c='r', label='MLE')\n",
      "#Fisher information in green\n",
      "ax3.plot(range(60, 8, -1), var_s1_fisher[0:52], c='g', label='Fisher')\n",
      "ax4.plot(range(60, 8, -1), var_s2_fisher[0:52], c='g', label='Fisher')\n",
      "ax5.plot(range(60, 8, -1), cov_fisher[0:52], c='g', label='Fisher')\n",
      "ax1.locator_params(axis = 'y', nbins = 4)\n",
      "ax2.locator_params(axis = 'y', nbins = 4)\n",
      "ax3.locator_params(axis = 'y', nbins = 4)\n",
      "ax4.locator_params(axis = 'y', nbins = 4)\n",
      "ax5.locator_params(axis = 'y', nbins = 4)\n",
      "#ax1.legend()\n",
      "ax1.set_title(\"Bias $s_1$\")\n",
      "ax2.set_title(\"Bias $s_2$\")\n",
      "ax3.set_title(r'Variance $s_1$')\n",
      "ax4.set_title(r'Variance $s_2$')\n",
      "ax5.set_title('Covariance')\n",
      "ax5.set_xlabel(r'$\\Delta$ s')\n",
      "f.set_size_inches(10,10)\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = range(-30, 29)\n",
      "neg_sd_nn1_opp = mean_s1_nn1_opp[0:59] - np.sqrt(var_s1_nn1_opp[0:59])\n",
      "pos_sd_nn1_opp = mean_s1_nn1_opp[0:59] + np.sqrt(var_s1_nn1_opp[0:59])\n",
      "neg_sd_nn2_opp = mean_s1_nn2_opp[0:59] - np.sqrt(var_s1_nn2_opp[0:59])\n",
      "pos_sd_nn2_opp = mean_s1_nn2_opp[0:59] + np.sqrt(var_s1_nn2_opp[0:59])\n",
      "neg_sd_nn3_opp = mean_s1_nn3_opp[0:59] - np.sqrt(var_s1_nn3_opp[0:59])\n",
      "pos_sd_nn3_opp = mean_s1_nn3_opp[0:59] + np.sqrt(var_s1_nn3_opp[0:59])\n",
      "neg_sd_opt_opp = mean_s1_opt_opp[0:59] - np.sqrt(var_s1_opt_opp[0:59])\n",
      "pos_sd_opt_opp = mean_s1_opt_opp[0:59] + np.sqrt(var_s1_opt_opp[0:59])\n",
      "plt.rc('text', usetex=True)\n",
      "plt.plot(s, mean_s1_nn1_opp[0:59], c='y', label='Mean')\n",
      "plt.plot(s, neg_sd_nn1_opp, c='y', alpha=.25, label='SD')\n",
      "plt.plot(s, pos_sd_nn1_opp, c='y', alpha=.25, label='SD')\n",
      "plt.plot(s, mean_s1_nn2_opp[0:59], c='b', label='Mean')\n",
      "plt.plot(s, neg_sd_nn2_opp, c='b', alpha=.25, label='SD')\n",
      "plt.plot(s, pos_sd_nn2_opp, c='b', alpha=.25, label='SD')\n",
      "plt.plot(s, mean_s1_nn3_opp[0:59], c='m', label='Mean')\n",
      "plt.plot(s, neg_sd_nn3_opp, c='m', alpha=.25, label='SD')\n",
      "plt.plot(s, pos_sd_nn3_opp, c='m', alpha=.25, label='SD')\n",
      "plt.plot(s, mean_s1_opt_opp[0:59], c='r', label='Mean')\n",
      "plt.plot(s, neg_sd_opt_opp, c='r', alpha=.25, label='SD')\n",
      "plt.plot(s, pos_sd_opt_opp, c='r', alpha=.25, label='SD')\n",
      "plt.xlabel(r'$s_1$',fontsize=16)\n",
      "plt.ylabel(r'\\hat{s_1}',fontsize=16)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = range(-30, 29)\n",
      "neg_sd_nn1_opp = mean_s2_nn1_opp[0:59] - np.sqrt(var_s2_nn1_opp[0:59])\n",
      "pos_sd_nn1_opp = mean_s2_nn1_opp[0:59] + np.sqrt(var_s2_nn1_opp[0:59])\n",
      "neg_sd_nn2_opp = mean_s2_nn2_opp[0:59] - np.sqrt(var_s2_nn2_opp[0:59])\n",
      "pos_sd_nn2_opp = mean_s2_nn2_opp[0:59] + np.sqrt(var_s2_nn2_opp[0:59])\n",
      "neg_sd_nn3_opp = mean_s2_nn3_opp[0:59] - np.sqrt(var_s2_nn3_opp[0:59])\n",
      "pos_sd_nn3_opp = mean_s2_nn3_opp[0:59] + np.sqrt(var_s2_nn3_opp[0:59])\n",
      "neg_sd_opt_opp = mean_s2_opt_opp[0:59] - np.sqrt(var_s2_opt_opp[0:59])\n",
      "pos_sd_opt_opp = mean_s2_opt_opp[0:59] + np.sqrt(var_s2_opt_opp[0:59])\n",
      "plt.rc('text', usetex=True)\n",
      "plt.plot(s, mean_s2_nn1_opp[0:59], c='y', label='Mean')\n",
      "plt.plot(s, neg_sd_nn1_opp, c='y', alpha=.5, label='SD')\n",
      "plt.plot(s, pos_sd_nn1_opp, c='y', alpha=.5, label='SD')\n",
      "plt.plot(s, mean_s2_nn2_opp[0:59], c='b', label='Mean')\n",
      "plt.plot(s, neg_sd_nn2_opp, c='b', alpha=.5, label='SD')\n",
      "plt.plot(s, pos_sd_nn2_opp, c='b', alpha=.5, label='SD')\n",
      "plt.plot(s, mean_s2_nn3_opp[0:59], c='m', label='Mean')\n",
      "plt.plot(s, neg_sd_nn3_opp, c='m', alpha=.5, label='SD')\n",
      "plt.plot(s, pos_sd_nn3_opp, c='m', alpha=.5, label='SD')\n",
      "plt.plot(s, mean_s2_opt_opp[0:59], c='r', label='Mean')\n",
      "plt.plot(s, neg_sd_opt_opp, c='r', alpha=.5, label='SD')\n",
      "plt.plot(s, pos_sd_opt_opp, c='r', alpha=.5, label='SD')\n",
      "plt.xlabel(r'$s_1$',fontsize=16)\n",
      "plt.ylabel(r'\\hat{s_2}',fontsize=16)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data_gain30_1 = generate_trainset(20000, r_max=30)\n",
      "train_data_gain30_2 = generate_trainset(60000, r_max=30)\n",
      "train_data_gain30_3 = generate_trainset(100000, r_max=30)\n",
      "nn_gains30_1, nn_gainsx30_1 = train_nn(train_data_gain30_1, n_hidden=20, learning_rate=.001, n_epochs=100, RMSProp=True)\n",
      "nn_gains30_2, nn_gainsx30_2 = train_nn(train_data_gain30_2, n_hidden=20, learning_rate=.001, n_epochs=100, RMSProp=True)\n",
      "nn_gains30_3, nn_gainsx30_3 = train_nn(train_data_gain30_3, n_hidden=20, learning_rate=.001, n_epochs=100, RMSProp=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s1 = -30\n",
      "num_deltas = 60\n",
      "\n",
      "mean_s1_nn_gain30_1 = [None] * num_deltas\n",
      "mean_s2_nn_gain30_1 = [None] * num_deltas\n",
      "bias_s1_nn_gain30_1 = [None] * num_deltas\n",
      "bias_s2_nn_gain30_1 = [None] * num_deltas\n",
      "var_s1_nn_gain30_1 = [None] * num_deltas\n",
      "var_s2_nn_gain30_1 = [None] * num_deltas\n",
      "corr_nn_gain30_1 = [None] * num_deltas\n",
      "cov_nn_gain30_1 = [None] * num_deltas\n",
      "\n",
      "mean_s1_nn_gain30_2 = [None] * num_deltas\n",
      "mean_s2_nn_gain30_2 = [None] * num_deltas\n",
      "bias_s1_nn_gain30_2 = [None] * num_deltas\n",
      "bias_s2_nn_gain30_2 = [None] * num_deltas\n",
      "var_s1_nn_gain30_2 = [None] * num_deltas\n",
      "var_s2_nn_gain30_2 = [None] * num_deltas\n",
      "corr_nn_gain30_2 = [None] * num_deltas\n",
      "cov_nn_gain30_2 = [None] * num_deltas\n",
      "\n",
      "mean_s1_nn_gain30_3 = [None] * num_deltas\n",
      "mean_s2_nn_gain30_3 = [None] * num_deltas\n",
      "bias_s1_nn_gain30_3 = [None] * num_deltas\n",
      "bias_s2_nn_gain30_3 = [None] * num_deltas\n",
      "var_s1_nn_gain30_3 = [None] * num_deltas\n",
      "var_s2_nn_gain30_3 = [None] * num_deltas\n",
      "corr_nn_gain30_3 = [None] * num_deltas\n",
      "cov_nn_gain30_3 = [None] * num_deltas\n",
      "\n",
      "var_s1_fisher = [None] * num_deltas\n",
      "var_s2_fisher = [None] * num_deltas\n",
      "corr_fisher = [None] * num_deltas\n",
      "\n",
      "for delta_s in range(num_deltas):\n",
      "    test_data = generate_s_data(s1, s1 + delta_s, 3000, r_max=30)\n",
      "    nn_preds1, _ = test_nn(nn_gains30_1, nn_gainsx30_1, test_data)\n",
      "    nn_preds2, _ = test_nn(nn_gains30_2, nn_gainsx30_2, test_data)\n",
      "    nn_preds3, _ = test_nn(nn_gains30_3, nn_gainsx30_3, test_data)\n",
      "    \n",
      "    nn_preds_gain30_1 = nn_preds1.T * 90\n",
      "    nn_preds_gain30_2 = nn_preds2.T * 90\n",
      "    nn_preds_gain30_3 = nn_preds3.T * 90\n",
      "\n",
      "    nn_stats_gain30_1 = get_statistics(s1, s1 + delta_s, nn_preds_gain30_1)\n",
      "    nn_stats_gain30_2 = get_statistics(s1, s1 + delta_s, nn_preds_gain30_2)\n",
      "    nn_stats_gain30_3 = get_statistics(s1, s1 + delta_s, nn_preds_gain30_3)\n",
      "    \n",
      "    if delta_s > 0:\n",
      "        FI = fisher_inf(s1, s1 + delta_s, .5, .5, r_max=30)\n",
      "        var_s1_fisher[delta_s] = FI[0, 0]\n",
      "        var_s2_fisher[delta_s] = FI[1, 1]\n",
      "        cov_fisher[delta_s] = FI[0, 1]\n",
      "    \n",
      "    mean_s1_nn_gain30_1[delta_s] = nn_stats_gain30_1['mean_s1']\n",
      "    mean_s2_nn_gain30_1[delta_s] = nn_stats_gain30_1['mean_s2']\n",
      "    bias_s1_nn_gain30_1[delta_s] = nn_stats_gain30_1['bias_s1']\n",
      "    bias_s2_nn_gain30_1[delta_s] = nn_stats_gain30_1['bias_s2']\n",
      "    var_s1_nn_gain30_1[delta_s] = nn_stats_gain30_1['var_s1']\n",
      "    var_s2_nn_gain30_1[delta_s] = nn_stats_gain30_1['var_s2']\n",
      "    corr_nn_gain30_1[delta_s] = nn_stats_gain30_1['corr']\n",
      "    cov_nn_gain30_1[delta_s] = nn_stats_gain30_1['cov']\n",
      "    \n",
      "    mean_s1_nn_gain30_2[delta_s] = nn_stats_gain30_2['mean_s1']\n",
      "    mean_s2_nn_gain30_2[delta_s] = nn_stats_gain30_2['mean_s2']\n",
      "    bias_s1_nn_gain30_2[delta_s] = nn_stats_gain30_2['bias_s1']\n",
      "    bias_s2_nn_gain30_2[delta_s] = nn_stats_gain30_2['bias_s2']\n",
      "    var_s1_nn_gain30_2[delta_s] = nn_stats_gain30_2['var_s1']\n",
      "    var_s2_nn_gain30_2[delta_s] = nn_stats_gain30_2['var_s2']\n",
      "    corr_nn_gain30_2[delta_s] = nn_stats_gain30_2['corr']\n",
      "    cov_nn_gain30_2[delta_s] = nn_stats_gain30_2['cov']\n",
      "    \n",
      "    mean_s1_nn_gain30_3[delta_s] = nn_stats_gain30_3['mean_s1']\n",
      "    mean_s2_nn_gain30_3[delta_s] = nn_stats_gain30_3['mean_s2']\n",
      "    bias_s1_nn_gain30_3[delta_s] = nn_stats_gain30_3['bias_s1']\n",
      "    bias_s2_nn_gain30_3[delta_s] = nn_stats_gain30_3['bias_s2']\n",
      "    var_s1_nn_gain30_3[delta_s] = nn_stats_gain30_3['var_s1']\n",
      "    var_s2_nn_gain30_3[delta_s] = nn_stats_gain30_3['var_s2']\n",
      "    corr_nn_gain30_3[delta_s] = nn_stats_gain30_3['corr']\n",
      "    cov_nn_gain30_3[delta_s] = nn_stats_gain30_3['cov']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5)\n",
      "#20000\n",
      "ax1.plot(range(1, 61), bias_s1_nn_gain30_1, c='y', label='NN')\n",
      "ax2.plot(range(1, 61), bias_s2_nn_gain30_1, c='y', label='NN')\n",
      "ax3.plot(range(1, 61), var_s1_nn_gain30_1, c='y', label='NN')\n",
      "ax4.plot(range(1, 61), var_s2_nn_gain30_1, c='y', label='NN')\n",
      "ax5.plot(range(1, 61), cov_nn_gain30_1, c='y', label='NN')\n",
      "#60000\n",
      "ax1.plot(range(1, 61), bias_s1_nn_gain30_2, c='b', label='NN')\n",
      "ax2.plot(range(1, 61), bias_s2_nn_gain30_2, c='b', label='NN')\n",
      "ax3.plot(range(1, 61), var_s1_nn_gain30_2, c='b', label='NN')\n",
      "ax4.plot(range(1, 61), var_s2_nn_gain30_2, c='b', label='NN')\n",
      "ax5.plot(range(1, 61), cov_nn_gain30_2, c='b', label='NN')\n",
      "#100000\n",
      "ax1.plot(range(1, 61), bias_s1_nn_gain30_3, c='m', label='NN')\n",
      "ax2.plot(range(1, 61), bias_s2_nn_gain30_3, c='m', label='NN')\n",
      "ax3.plot(range(1, 61), var_s1_nn_gain30_3, c='m', label='NN')\n",
      "ax4.plot(range(1, 61), var_s2_nn_gain30_3, c='m', label='NN')\n",
      "ax5.plot(range(1, 61), cov_nn_gain30_3, c='m', label='NN')\n",
      "#optimal in red\n",
      "ax1.plot(range(1, 61), bias_s1_opt, c='r', label='MLE')\n",
      "ax2.plot(range(1, 61), bias_s2_opt, c='r', label='MLE')\n",
      "ax3.plot(range(1, 61), var_s1_opt, c='r', label='MLE')\n",
      "ax4.plot(range(1, 61), var_s2_opt, c='r', label='MLE')\n",
      "ax5.plot(range(1, 61), cov_opt, c='r', label='MLE')\n",
      "#Fisher information in green\n",
      "ax3.plot(range(7, 61), var_s1_fisher[6:60], c='g', label='Fisher')\n",
      "ax4.plot(range(7, 61), var_s2_fisher[6:60], c='g', label='Fisher')\n",
      "ax5.plot(range(7, 61), cov_fisher[6:60], c='g', label='Fisher')\n",
      "ax1.locator_params(axis = 'y', nbins = 4)\n",
      "ax2.locator_params(axis = 'y', nbins = 4)\n",
      "ax3.locator_params(axis = 'y', nbins = 4)\n",
      "ax4.locator_params(axis = 'y', nbins = 4)\n",
      "ax5.locator_params(axis = 'y', nbins = 4)\n",
      "#ax1.legend()\n",
      "ax1.set_title(\"Bias $s_1$\")\n",
      "ax2.set_title(\"Bias $s_2$\")\n",
      "ax3.set_title(r'Variance $s_1$')\n",
      "ax4.set_title(r'Variance $s_2$')\n",
      "ax5.set_title('Covariance')\n",
      "ax5.set_xlabel(r'$\\Delta$ s')\n",
      "f.set_size_inches(10,10)\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data_1 = generate_trainset(20000)\n",
      "valid_data_1 = generate_trainset(20000)\n",
      "nn_rms, nnx_rms, valid_mse_rms = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100, RMSProp=True)\n",
      "\"\"\"\n",
      "nn_rms_m9, nnx_rms_m9, valid_mse_rms_m9 = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100, RMSProp=True, nesterov=True, momentum=0.9)\n",
      "nn_rms_m9f, nnx_rms_m9f, valid_mse_rms_m9f = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100, RMSProp=True, nesterov=False, momentum=0.9)\n",
      "nn_rms_m95, nnx_rms_m95, valid_mse_rms_m95 = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100, RMSProp=True, momentum=0.95)\n",
      "nn_rms_m99, nnx_rms_m99, valid_mse_rms_m99 = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100, RMSProp=True, momentum=0.99)\n",
      "\"\"\"\n",
      "nn_plain, nnx_plain, valid_mse_plain = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rv, s, _ = valid_data_1\n",
      "s=s.T*90\n",
      "opt_preds = fit_optimal(rv, sm, s)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mse_opt = np.mean((opt_preds[0] - s[0]) ** 2 + (opt_preds[1] - s[1]) ** 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "#plt.plot(range(20, 100), valid_mse_plain[20:100], c='b')\n",
      "#plt.plot(range(20, 100), valid_mse_rms[20:100], c='y')\n",
      "plt.plot(valid_mse_plain, c='b', label='Vanilla')\n",
      "plt.plot(valid_mse_rms, c='k', label='RMSProp')\n",
      "plt.plot(np.ones(100) * mse_opt, c='r', label='MAP')\n",
      "plt.legend()\n",
      "plt.xlabel('epoch')\n",
      "plt.ylabel('MSE')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s1 = -30\n",
      "num_deltas = 60\n",
      "#num_deltas = 5\n",
      "\n",
      "mean_s1_nn_rms = [None] * num_deltas\n",
      "mean_s2_nn_rms = [None] * num_deltas\n",
      "bias_s1_nn_rms = [None] * num_deltas\n",
      "bias_s2_nn_rms = [None] * num_deltas\n",
      "var_s1_nn_rms = [None] * num_deltas\n",
      "var_s2_nn_rms = [None] * num_deltas\n",
      "corr_nn_rms = [None] * num_deltas\n",
      "cov_nn_rms = [None] * num_deltas\n",
      "\n",
      "\"\"\"\n",
      "mean_s1_nn_rms_m9 = [None] * num_deltas\n",
      "mean_s2_nn_rms_m9 = [None] * num_deltas\n",
      "bias_s1_nn_rms_m9 = [None] * num_deltas\n",
      "bias_s2_nn_rms_m9 = [None] * num_deltas\n",
      "var_s1_nn_rms_m9 = [None] * num_deltas\n",
      "var_s2_nn_rms_m9 = [None] * num_deltas\n",
      "corr_nn_rms_m9 = [None] * num_deltas\n",
      "cov_nn_rms_m9 = [None] * num_deltas\n",
      "\n",
      "mean_s1_nn_rms_m9f = [None] * num_deltas\n",
      "mean_s2_nn_rms_m9f = [None] * num_deltas\n",
      "bias_s1_nn_rms_m9f = [None] * num_deltas\n",
      "bias_s2_nn_rms_m9f = [None] * num_deltas\n",
      "var_s1_nn_rms_m9f = [None] * num_deltas\n",
      "var_s2_nn_rms_m9f = [None] * num_deltas\n",
      "corr_nn_rms_m9f = [None] * num_deltas\n",
      "cov_nn_rms_m9f = [None] * num_deltas\n",
      "\n",
      "mean_s1_nn_rms_m95 = [None] * num_deltas\n",
      "mean_s2_nn_rms_m95 = [None] * num_deltas\n",
      "bias_s1_nn_rms_m95 = [None] * num_deltas\n",
      "bias_s2_nn_rms_m95 = [None] * num_deltas\n",
      "var_s1_nn_rms_m95 = [None] * num_deltas\n",
      "var_s2_nn_rms_m95 = [None] * num_deltas\n",
      "corr_nn_rms_m95 = [None] * num_deltas\n",
      "cov_nn_rms_m95 = [None] * num_deltas\n",
      "\n",
      "mean_s1_nn_rms_m99 = [None] * num_deltas\n",
      "mean_s2_nn_rms_m99 = [None] * num_deltas\n",
      "bias_s1_nn_rms_m99 = [None] * num_deltas\n",
      "bias_s2_nn_rms_m99 = [None] * num_deltas\n",
      "var_s1_nn_rms_m99 = [None] * num_deltas\n",
      "var_s2_nn_rms_m99 = [None] * num_deltas\n",
      "corr_nn_rms_m99 = [None] * num_deltas\n",
      "cov_nn_rms_m99 = [None] * num_deltas\n",
      "\n",
      "mean_s1_nn_plain = [None] * num_deltas\n",
      "mean_s2_nn_plain = [None] * num_deltas\n",
      "bias_s1_nn_plain = [None] * num_deltas\n",
      "bias_s2_nn_plain = [None] * num_deltas\n",
      "var_s1_nn_plain = [None] * num_deltas\n",
      "var_s2_nn_plain = [None] * num_deltas\n",
      "corr_nn_plain = [None] * num_deltas\n",
      "cov_nn_plain = [None] * num_deltas\n",
      "\"\"\"\n",
      "\n",
      "mean_s1_opt = [None] * num_deltas\n",
      "mean_s2_opt = [None] * num_deltas\n",
      "bias_s1_opt = [None] * num_deltas\n",
      "bias_s2_opt = [None] * num_deltas\n",
      "var_s1_opt = [None] * num_deltas\n",
      "var_s2_opt = [None] * num_deltas\n",
      "corr_opt = [None] * num_deltas\n",
      "cov_opt = [None] * num_deltas\n",
      "\n",
      "var_s1_fisher = [None] * num_deltas\n",
      "var_s2_fisher = [None] * num_deltas\n",
      "cov_fisher = [None] * num_deltas\n",
      "\n",
      "for delta_s in range(num_deltas):\n",
      "    #test_data = generate_s_data(s1, s1 + delta_s + 1, 3000)\n",
      "    test_data = generate_s_data(s1, s1 + delta_s, 300)\n",
      "    nn_preds_rms, _ = test_nn(nn_rms, nnx_rms, test_data)\n",
      "    \"\"\"\n",
      "    nn_preds_rms_m9, _ = test_nn(nn_rms_m9, nnx_rms_m9, test_data)\n",
      "    nn_preds_rms_m9f, _ = test_nn(nn_rms_m9f, nnx_rms_m9f, test_data)\n",
      "    nn_preds_rms_m95, _ = test_nn(nn_rms_m95, nnx_rms_m95, test_data)\n",
      "    nn_preds_rms_m99, _ = test_nn(nn_rms_m99, nnx_rms_m99, test_data)\n",
      "    nn_preds_plain, _ = test_nn(nn_plain, nnx_plain, test_data)\n",
      "    \"\"\"\n",
      "    r, _, _ = test_data\n",
      "    opt_preds = fit_optimal(r, sm, init={'s_1':s1, 's_2':s1 + delta_s + 1})\n",
      "    \n",
      "    nn_preds_rms = nn_preds_rms.T * 90\n",
      "    \"\"\"\n",
      "    nn_preds_rms_m9 = nn_preds_rms_m9.T * 90\n",
      "    nn_preds_rms_m9f = nn_preds_rms_m9f.T * 90\n",
      "    nn_preds_rms_m95 = nn_preds_rms_m95.T * 90\n",
      "    nn_preds_rms_m99 = nn_preds_rms_m99.T * 90\n",
      "    nn_preds_plain = nn_preds_plain.T * 90\n",
      "    \"\"\"\n",
      "\n",
      "    nn_stats_rms = get_statistics(s1, s1 + delta_s, nn_preds_rms)\n",
      "    \"\"\"\n",
      "    nn_stats_rms_m9 = get_statistics(s1, s1 + delta_s, nn_preds_rms_m9)\n",
      "    nn_stats_rms_m9f = get_statistics(s1, s1 + delta_s, nn_preds_rms_m9f)\n",
      "    nn_stats_rms_m95 = get_statistics(s1, s1 + delta_s, nn_preds_rms_m95)\n",
      "    nn_stats_rms_m99 = get_statistics(s1, s1 + delta_s, nn_preds_rms_m99)\n",
      "    nn_stats_plain = get_statistics(s1, s1 + delta_s, nn_preds_plain)\n",
      "    \"\"\"\n",
      "    opt_stats = get_statistics(s1, s1 + delta_s, opt_preds)\n",
      "    \n",
      "    if delta_s > 0:\n",
      "        FI = fisher_inf(s1, s1 + delta_s, .5, .5)\n",
      "        var_s1_fisher[delta_s] = FI[0, 0]\n",
      "        var_s2_fisher[delta_s] = FI[1, 1]\n",
      "        cov_fisher[delta_s] = FI[0, 1]\n",
      "    \n",
      "    mean_s1_nn_rms[delta_s] = nn_stats_rms['mean_s1']\n",
      "    mean_s2_nn_rms[delta_s] = nn_stats_rms['mean_s2']\n",
      "    bias_s1_nn_rms[delta_s] = nn_stats_rms['bias_s1']\n",
      "    bias_s2_nn_rms[delta_s] = nn_stats_rms['bias_s2']\n",
      "    var_s1_nn_rms[delta_s] = nn_stats_rms['var_s1']\n",
      "    var_s2_nn_rms[delta_s] = nn_stats_rms['var_s2']\n",
      "    corr_nn_rms[delta_s] = nn_stats_rms['corr']\n",
      "    cov_nn_rms[delta_s] = nn_stats_rms['cov']\n",
      "    \n",
      "    mean_s1_opt[delta_s] = opt_stats['mean_s1']\n",
      "    mean_s2_opt[delta_s] = opt_stats['mean_s2']\n",
      "    bias_s1_opt[delta_s] = opt_stats['bias_s1']\n",
      "    bias_s2_opt[delta_s] = opt_stats['bias_s2']\n",
      "    var_s1_opt[delta_s] = opt_stats['var_s1']\n",
      "    var_s2_opt[delta_s] = opt_stats['var_s2']\n",
      "    corr_opt[delta_s] = opt_stats['corr']\n",
      "    cov_opt[delta_s] = opt_stats['cov']\n",
      "    \n",
      "    \"\"\"\n",
      "    mean_s1_nn_rms_m9[delta_s] = nn_stats_rms_m9['mean_s1']\n",
      "    mean_s2_nn_rms_m9[delta_s] = nn_stats_rms_m9['mean_s2']\n",
      "    bias_s1_nn_rms_m9[delta_s] = nn_stats_rms_m9['bias_s1']\n",
      "    bias_s2_nn_rms_m9[delta_s] = nn_stats_rms_m9['bias_s2']\n",
      "    var_s1_nn_rms_m9[delta_s] = nn_stats_rms_m9['var_s1']\n",
      "    var_s2_nn_rms_m9[delta_s] = nn_stats_rms_m9['var_s2']\n",
      "    corr_nn_rms_m9[delta_s] = nn_stats_rms_m9['corr']\n",
      "    cov_nn_rms_m9[delta_s] = nn_stats_rms_m9['cov']\n",
      "    \n",
      "    mean_s1_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['mean_s1']\n",
      "    mean_s2_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['mean_s2']\n",
      "    bias_s1_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['bias_s1']\n",
      "    bias_s2_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['bias_s2']\n",
      "    var_s1_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['var_s1']\n",
      "    var_s2_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['var_s2']\n",
      "    corr_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['corr']\n",
      "    cov_nn_rms_m9f[delta_s] = nn_stats_rms_m9f['cov']\n",
      "    \n",
      "    mean_s1_nn_rms_m95[delta_s] = nn_stats_rms_m95['mean_s1']\n",
      "    mean_s2_nn_rms_m95[delta_s] = nn_stats_rms_m95['mean_s2']\n",
      "    bias_s1_nn_rms_m95[delta_s] = nn_stats_rms_m95['bias_s1']\n",
      "    bias_s2_nn_rms_m95[delta_s] = nn_stats_rms_m95['bias_s2']\n",
      "    var_s1_nn_rms_m95[delta_s] = nn_stats_rms_m95['var_s1']\n",
      "    var_s2_nn_rms_m95[delta_s] = nn_stats_rms_m95['var_s2']\n",
      "    corr_nn_rms_m95[delta_s] = nn_stats_rms_m95['corr']\n",
      "    cov_nn_rms_m95[delta_s] = nn_stats_rms_m95['cov']\n",
      "    \n",
      "    mean_s1_nn_rms_m99[delta_s] = nn_stats_rms_m99['mean_s1']\n",
      "    mean_s2_nn_rms_m99[delta_s] = nn_stats_rms_m99['mean_s2']\n",
      "    bias_s1_nn_rms_m99[delta_s] = nn_stats_rms_m99['bias_s1']\n",
      "    bias_s2_nn_rms_m99[delta_s] = nn_stats_rms_m99['bias_s2']\n",
      "    var_s1_nn_rms_m99[delta_s] = nn_stats_rms_m99['var_s1']\n",
      "    var_s2_nn_rms_m99[delta_s] = nn_stats_rms_m99['var_s2']\n",
      "    corr_nn_rms_m99[delta_s] = nn_stats_rms_m99['corr']\n",
      "    cov_nn_rms_m99[delta_s] = nn_stats_rms_m99['cov']\n",
      "    \n",
      "    mean_s1_nn_plain[delta_s] = nn_stats_plain['mean_s1']\n",
      "    mean_s2_nn_plain[delta_s] = nn_stats_plain['mean_s2']\n",
      "    bias_s1_nn_plain[delta_s] = nn_stats_plain['bias_s1']\n",
      "    bias_s2_nn_plain[delta_s] = nn_stats_plain['bias_s2']\n",
      "    var_s1_nn_plain[delta_s] = nn_stats_plain['var_s1']\n",
      "    var_s2_nn_plain[delta_s] = nn_stats_plain['var_s2']\n",
      "    corr_nn_plain[delta_s] = nn_stats_plain['corr']\n",
      "    cov_nn_plain[delta_s] = nn_stats_plain['cov']\n",
      "    \"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.patches as mpatches\n",
      "s = range(-30, 30)\n",
      "neg_sd_nn_rms = mean_s1_nn_rms - np.sqrt(var_s1_nn_rms)\n",
      "pos_sd_nn_rms = mean_s1_nn_rms + np.sqrt(var_s1_nn_rms)\n",
      "neg_sd_opt = mean_s1_opt - np.sqrt(var_s1_opt)\n",
      "pos_sd_opt = mean_s1_opt + np.sqrt(var_s1_opt)\n",
      "plt.rc('text', usetex=True)\n",
      "plt.figure(figsize=(10,10))\n",
      "plt.fill_between(s, pos_sd_nn_rms, neg_sd_nn_rms, facecolor='k', alpha=0.5, edgecolor=\"None\", label=\"MLE\")\n",
      "plt.fill_between(s, pos_sd_opt, neg_sd_opt, facecolor='m', alpha=0.5, edgecolor=\"None\", label=\"NN\")\n",
      "plt.ylim([-40,40])\n",
      "plt.xlabel(r'$s_2$',fontsize=30)\n",
      "plt.ylabel(r'\\hat{s_1}',fontsize=30)\n",
      "nn_patch = mpatches.Patch(color='k', label='NN')\n",
      "mle_patch = mpatches.Patch(color='m', label='MLE')\n",
      "plt.legend(handles=[nn_patch, mle_patch])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = range(-30, 30)\n",
      "neg_sd_nn_rms = mean_s2_nn_rms - np.sqrt(var_s2_nn_rms)\n",
      "pos_sd_nn_rms = mean_s2_nn_rms + np.sqrt(var_s2_nn_rms)\n",
      "neg_sd_opt = mean_s2_opt - np.sqrt(var_s2_opt)\n",
      "pos_sd_opt = mean_s2_opt + np.sqrt(var_s2_opt)\n",
      "plt.rc('text', usetex=True)\n",
      "plt.figure(figsize=(10,10))\n",
      "plt.fill_between(s, pos_sd_nn_rms, neg_sd_nn_rms, facecolor='k', alpha=0.5, edgecolor=\"None\")\n",
      "plt.fill_between(s, pos_sd_opt, neg_sd_opt, facecolor='m', alpha=0.5, edgecolor=\"None\")\n",
      "plt.xlabel(r'$s_2$',fontsize=30)\n",
      "plt.ylabel(r'\\hat{s_2}',fontsize=30)\n",
      "plt.ylim([-40,40])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for delta_s in range(num_deltas):    \n",
      "    if delta_s > 0:\n",
      "        FI = fisher_inf(s1, s1 + delta_s, .5, .5)\n",
      "        var_s1_fisher[delta_s] = FI[0, 0]\n",
      "        var_s2_fisher[delta_s] = FI[1, 1]\n",
      "        cov_fisher[delta_s] = FI[0, 1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5)\n",
      "#RMS\n",
      "ax1.plot(range(1, 61), bias_s1_nn_rms, c='k', label='NN')\n",
      "ax2.plot(range(1, 61), bias_s2_nn_rms, c='k', label='NN')\n",
      "ax3.plot(range(1, 61), var_s1_nn_rms, c='k', label='NN')\n",
      "ax4.plot(range(1, 61), var_s2_nn_rms, c='k', label='NN')\n",
      "ax5.plot(range(1, 61), cov_nn_rms, c='k', label='NN')\n",
      "#optimal in red\n",
      "ax1.plot(range(1, 61), bias_s1_opt, c='m', label='MLE')\n",
      "ax2.plot(range(1, 61), bias_s2_opt, c='m', label='MLE')\n",
      "ax3.plot(range(1, 61), var_s1_opt, c='m', label='MLE')\n",
      "ax4.plot(range(1, 61), var_s2_opt, c='m', label='MLE')\n",
      "ax5.plot(range(1, 61), cov_opt, c='m', label='MLE')\n",
      "\"\"\"\n",
      "#RMS with momentum\n",
      "ax1.plot(range(1, 61), bias_s1_nn_rms_m9, c='m', label='NN')\n",
      "ax2.plot(range(1, 61), bias_s2_nn_rms_m9, c='m', label='NN')\n",
      "ax3.plot(range(1, 61), var_s1_nn_rms_m9, c='m', label='NN')\n",
      "ax4.plot(range(1, 61), var_s2_nn_rms_m9, c='m', label='NN')\n",
      "ax5.plot(range(1, 61), cov_nn_rms_m9, c='m', label='NN')\n",
      "#RMS with momentum\n",
      "ax1.plot(range(1, 61), bias_s1_nn_rms_m95, c='c', label='NN')\n",
      "ax2.plot(range(1, 61), bias_s2_nn_rms_m95, c='c', label='NN')\n",
      "ax3.plot(range(1, 61), var_s1_nn_rms_m95, c='c', label='NN')\n",
      "ax4.plot(range(1, 61), var_s2_nn_rms_m95, c='c', label='NN')\n",
      "ax5.plot(range(1, 61), cov_nn_rms_m95, c='c', label='NN')\n",
      "#RMS with momentum\n",
      "ax1.plot(range(1, 61), bias_s1_nn_rms_m99, c='k', label='NN')\n",
      "ax2.plot(range(1, 61), bias_s2_nn_rms_m99, c='k', label='NN')\n",
      "ax3.plot(range(1, 61), var_s1_nn_rms_m99, c='k', label='NN')\n",
      "ax4.plot(range(1, 61), var_s2_nn_rms_m99, c='k', label='NN')\n",
      "ax5.plot(range(1, 61), cov_nn_rms_m99, c='k', label='NN')\n",
      "#RMS\n",
      "ax1.plot(range(1, 61), bias_s1_nn_rms_m9f, c='r', label='NN')\n",
      "ax2.plot(range(1, 61), bias_s2_nn_rms_m9f, c='r', label='NN')\n",
      "ax3.plot(range(1, 61), var_s1_nn_rms_m9f, c='r', label='NN')\n",
      "ax4.plot(range(1, 61), var_s2_nn_rms_m9f, c='r', label='NN')\n",
      "ax5.plot(range(1, 61), cov_nn_rms_m9f, c='r', label='NN')\n",
      "#Plain\n",
      "ax1.plot(range(1, 61), bias_s1_nn_plain, c='b', label='NN')\n",
      "ax2.plot(range(1, 61), bias_s2_nn_plain, c='b', label='NN')\n",
      "ax3.plot(range(1, 61), var_s1_nn_plain, c='b', label='NN')\n",
      "ax4.plot(range(1, 61), var_s2_nn_plain, c='b', label='NN')\n",
      "ax5.plot(range(1, 61), cov_nn_plain, c='b', label='NN')\n",
      "\"\"\"\n",
      "#Fisher information in green\n",
      "ax3.plot(range(7, 61), var_s1_fisher[6:60], c='orange', label='Fisher')\n",
      "ax4.plot(range(7, 61), var_s2_fisher[6:60], c='orange', label='Fisher')\n",
      "ax5.plot(range(7, 61), cov_fisher[6:60], c='orange', label='Fisher')\n",
      "ax1.locator_params(axis = 'y', nbins = 4)\n",
      "ax2.locator_params(axis = 'y', nbins = 4)\n",
      "ax3.locator_params(axis = 'y', nbins = 4)\n",
      "ax4.locator_params(axis = 'y', nbins = 4)\n",
      "ax5.locator_params(axis = 'y', nbins = 4)\n",
      "#ax1.legend()\n",
      "ax1.set_title(\"Bias $s_1$\")\n",
      "ax2.set_title(\"Bias $s_2$\")\n",
      "ax3.set_title(r'Variance $s_1$')\n",
      "ax4.set_title(r'Variance $s_2$')\n",
      "ax5.set_title('Covariance')\n",
      "ax5.set_xlabel(r'$\\Delta$ s')\n",
      "f.set_size_inches(10,10)\n",
      "ax3.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Separated Training:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def random_s_separated(ndata):\n",
      "    s_0 = np.random.rand(ndata) * 30 - 60\n",
      "    s_1 = np.random.rand(ndata) * 30 + 30\n",
      "    return s_0, s_1\n",
      "\n",
      "def generate_trainset_separated(ndata):\n",
      "    s_0, s_1 = random_s_separated(ndata)\n",
      "    c_0, c_1 = np.ones((2, ndata)) * .5\n",
      "    r, s, c = generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True, s_0, s_1, c_0, c_1)\n",
      "    return r, s, c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data1 = generate_trainset_separated(20000)\n",
      "train_data2 = generate_trainset_separated(60000)\n",
      "train_data3 = generate_trainset_separated(100000)\n",
      "nn1, nnx1 = train_nn(train_data1, n_hidden=20, learning_rate=.001, n_epochs=100)\n",
      "nn2, nnx2 = train_nn(train_data2, n_hidden=20, learning_rate=.001, n_epochs=100)\n",
      "nn3, nnx3 = train_nn(train_data3, n_hidden=20, learning_rate=.001, n_epochs=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s1 = -30\n",
      "num_deltas = 60\n",
      "mean_s1_nn1 = [None] * num_deltas\n",
      "mean_s2_nn1 = [None] * num_deltas\n",
      "bias_s1_nn1 = [None] * num_deltas\n",
      "bias_s2_nn1 = [None] * num_deltas\n",
      "var_s1_nn1 = [None] * num_deltas\n",
      "var_s2_nn1 = [None] * num_deltas\n",
      "corr_nn1 = [None] * num_deltas\n",
      "cov_nn1 = [None] * num_deltas\n",
      "\n",
      "mean_s1_nn2 = [None] * num_deltas\n",
      "mean_s2_nn2 = [None] * num_deltas\n",
      "bias_s1_nn2 = [None] * num_deltas\n",
      "bias_s2_nn2 = [None] * num_deltas\n",
      "var_s1_nn2 = [None] * num_deltas\n",
      "var_s2_nn2 = [None] * num_deltas\n",
      "corr_nn2 = [None] * num_deltas\n",
      "cov_nn2 = [None] * num_deltas\n",
      "\n",
      "mean_s1_nn3 = [None] * num_deltas\n",
      "mean_s2_nn3 = [None] * num_deltas\n",
      "bias_s1_nn3 = [None] * num_deltas\n",
      "bias_s2_nn3 = [None] * num_deltas\n",
      "var_s1_nn3 = [None] * num_deltas\n",
      "var_s2_nn3 = [None] * num_deltas\n",
      "corr_nn3 = [None] * num_deltas\n",
      "cov_nn3 = [None] * num_deltas\n",
      "\n",
      "mean_s1_opt = [None] * num_deltas\n",
      "mean_s2_opt = [None] * num_deltas\n",
      "bias_s1_opt = [None] * num_deltas\n",
      "bias_s2_opt = [None] * num_deltas\n",
      "var_s1_opt = [None] * num_deltas\n",
      "var_s2_opt = [None] * num_deltas\n",
      "corr_opt = [None] * num_deltas\n",
      "cov_opt = [None] * num_deltas\n",
      "\n",
      "var_s1_fisher = [None] * num_deltas\n",
      "var_s2_fisher = [None] * num_deltas\n",
      "corr_fisher = [None] * num_deltas\n",
      "\n",
      "for delta_s in range(num_deltas):\n",
      "    test_data = generate_s_data(s1, s1 + delta_s, 3000)\n",
      "    nn_preds1, _ = test_nn(nn1, nnx1, test_data)\n",
      "    nn_preds2, _ = test_nn(nn2, nnx2, test_data)\n",
      "    nn_preds3, _ = test_nn(nn3, nnx3, test_data)\n",
      "    \n",
      "    nn_preds1 = nn_preds1.T * 90\n",
      "    nn_preds2 = nn_preds2.T * 90\n",
      "    nn_preds3 = nn_preds3.T * 90\n",
      "    r, _, _ = test_data\n",
      "    opt_preds = fit_optimal(r, sm)\n",
      "    \n",
      "    nn_stats1 = get_statistics(s1, s1 + delta_s, nn_preds1)\n",
      "    nn_stats2 = get_statistics(s1, s1 + delta_s, nn_preds2)\n",
      "    nn_stats3 = get_statistics(s1, s1 + delta_s, nn_preds3)\n",
      "    opt_stats = get_statistics(s1, s1 + delta_s, opt_preds)\n",
      "    \n",
      "    if delta_s > 0:\n",
      "        FI = fisher_inf(s1, s1 + delta_s, .5, .5)\n",
      "        var_s1_fisher[delta_s] = FI[0, 0]\n",
      "        var_s2_fisher[delta_s] = FI[1, 1]\n",
      "        cov_fisher[delta_s] = FI[0, 1]\n",
      "        \n",
      "    mean_s1_nn1[delta_s] = nn_stats1['mean_s1']\n",
      "    mean_s2_nn1[delta_s] = nn_stats1['mean_s2']\n",
      "    bias_s1_nn1[delta_s] = nn_stats1['bias_s1']\n",
      "    bias_s2_nn1[delta_s] = nn_stats1['bias_s2']\n",
      "    var_s1_nn1[delta_s] = nn_stats1['var_s1']\n",
      "    var_s2_nn1[delta_s] = nn_stats1['var_s2']\n",
      "    corr_nn1[delta_s] = nn_stats1['corr']\n",
      "    cov_nn1[delta_s] = nn_stats1['cov']\n",
      "    \n",
      "    mean_s1_nn2[delta_s] = nn_stats2['mean_s1']\n",
      "    mean_s2_nn2[delta_s] = nn_stats2['mean_s2']\n",
      "    bias_s1_nn2[delta_s] = nn_stats2['bias_s1']\n",
      "    bias_s2_nn2[delta_s] = nn_stats2['bias_s2']\n",
      "    var_s1_nn2[delta_s] = nn_stats2['var_s1']\n",
      "    var_s2_nn2[delta_s] = nn_stats2['var_s2']\n",
      "    corr_nn2[delta_s] = nn_stats2['corr']\n",
      "    cov_nn2[delta_s] = nn_stats2['cov']\n",
      "    \n",
      "    mean_s1_nn3[delta_s] = nn_stats3['mean_s1']\n",
      "    mean_s2_nn3[delta_s] = nn_stats3['mean_s2']\n",
      "    bias_s1_nn3[delta_s] = nn_stats3['bias_s1']\n",
      "    bias_s2_nn3[delta_s] = nn_stats3['bias_s2']\n",
      "    var_s1_nn3[delta_s] = nn_stats3['var_s1']\n",
      "    var_s2_nn3[delta_s] = nn_stats3['var_s2']\n",
      "    corr_nn3[delta_s] = nn_stats3['corr']\n",
      "    cov_nn3[delta_s] = nn_stats3['cov']\n",
      "    \n",
      "    mean_s1_opt[delta_s] = opt_stats['mean_s1']\n",
      "    mean_s2_opt[delta_s] = opt_stats['mean_s2']\n",
      "    bias_s1_opt[delta_s] = opt_stats['bias_s1']\n",
      "    bias_s2_opt[delta_s] = opt_stats['bias_s2']\n",
      "    var_s1_opt[delta_s] = opt_stats['var_s1']\n",
      "    var_s2_opt[delta_s] = opt_stats['var_s2']\n",
      "    corr_opt[delta_s] = opt_stats['corr']\n",
      "    cov_opt[delta_s] = opt_stats['cov']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5)\n",
      "#20,000 training/50 units in blue\n",
      "ax1.plot(range(1, 61), bias_s1_nn1, c='b', label='Neural Net 1')\n",
      "ax2.plot(range(1, 61), bias_s2_nn1, c='b', label='Neural Net 1')\n",
      "ax3.plot(range(1, 61), var_s1_nn1, c='b', label='Neural Net 1')\n",
      "ax4.plot(range(1, 61), var_s2_nn1, c='b', label='Neural Net 1')\n",
      "ax5.plot(range(1, 61), cov_nn1, c='b', label='Neural Net 1')\n",
      "#60,000 training/50 units in magenta\n",
      "ax1.plot(range(1, 61), bias_s1_nn2, c='m', label='Neural Net 2')\n",
      "ax2.plot(range(1, 61), bias_s2_nn2, c='m', label='Neural Net 2')\n",
      "ax3.plot(range(1, 61), var_s1_nn2, c='m', label='Neural Net 2')\n",
      "ax4.plot(range(1, 61), var_s2_nn2, c='m', label='Neural Net 2')\n",
      "ax5.plot(range(1, 61), cov_nn2, c='m', label='Neural Net 2')\n",
      "#100,000 training/50 units in black\n",
      "ax1.plot(range(1, 61), bias_s1_nn3, c='k', label='Neural Net 3')\n",
      "ax2.plot(range(1, 61), bias_s2_nn3, c='k', label='Neural Net 3')\n",
      "ax3.plot(range(1, 61), var_s1_nn3, c='k', label='Neural Net 3')\n",
      "ax4.plot(range(1, 61), var_s2_nn3, c='k', label='Neural Net 3')\n",
      "ax5.plot(range(1, 61), cov_nn3, c='k', label='Neural Net 3') \n",
      "#optimal in red\n",
      "ax1.plot(range(1, 61), bias_s1_opt, c='r', label='MLE')\n",
      "ax2.plot(range(1, 61), bias_s2_opt, c='r', label='MLE')\n",
      "ax3.plot(range(1, 61), var_s1_opt, c='r', label='MLE')\n",
      "ax4.plot(range(1, 61), var_s2_opt, c='r', label='MLE')\n",
      "ax5.plot(range(1, 61), cov_opt, c='r', label='MLE')\n",
      "#Fisher information in green\n",
      "ax3.plot(range(9, 61), var_s1_fisher[8:60], c='g', label='Fisher')\n",
      "ax4.plot(range(9, 61), var_s2_fisher[8:60], c='g', label='Fisher')\n",
      "ax5.plot(range(9, 61), cov_fisher[8:60], c='g', label='Fisher')\n",
      "ax1.locator_params(axis = 'y', nbins = 4)\n",
      "ax2.locator_params(axis = 'y', nbins = 4)\n",
      "ax3.locator_params(axis = 'y', nbins = 4)\n",
      "ax3.set_ylim([0,30])\n",
      "ax4.locator_params(axis = 'y', nbins = 4)\n",
      "ax4.set_ylim([0,30])\n",
      "ax5.locator_params(axis = 'y', nbins = 4)\n",
      "ax5.set_ylim([-30,30])\n",
      "#ax1.legend()\n",
      "ax1.set_title(\"Bias $s_1$\")\n",
      "ax2.set_title(\"Bias $s_2$\")\n",
      "ax3.set_title(r'Variance $s_1$')\n",
      "ax4.set_title(r'Variance $s_2$')\n",
      "ax5.set_title('Covariance')\n",
      "ax5.set_xlabel(r'$\\Delta$ s')\n",
      "f.set_size_inches(10,10)\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = range(-30, 30)\n",
      "neg_sd_nn1s = mean_s1_nn1 - np.sqrt(var_s1_nn1)\n",
      "pos_sd_nn1s = mean_s1_nn1 + np.sqrt(var_s1_nn1)\n",
      "neg_sd_nn2s = mean_s1_nn2 - np.sqrt(var_s1_nn2)\n",
      "pos_sd_nn2s = mean_s1_nn2 + np.sqrt(var_s1_nn2)\n",
      "neg_sd_nn3s = mean_s1_nn3 - np.sqrt(var_s1_nn3)\n",
      "pos_sd_nn3s = mean_s1_nn3 + np.sqrt(var_s1_nn3)\n",
      "neg_sd_opt = mean_s1_opt - np.sqrt(var_s1_opt)\n",
      "pos_sd_opt = mean_s1_opt + np.sqrt(var_s1_opt)\n",
      "plt.rc('text', usetex=True)\n",
      "plt.plot(s, mean_s1_nn1, c='b', label='Mean')\n",
      "plt.plot(s, neg_sd_nn1s, c='b', alpha=.5, label='SD')\n",
      "plt.plot(s, pos_sd_nn1s, c='b', alpha=.5, label='SD')\n",
      "plt.plot(s, mean_s1_nn2, c='m', label='Mean')\n",
      "plt.plot(s, neg_sd_nn2s, c='m', alpha=.5, label='SD')\n",
      "plt.plot(s, pos_sd_nn2s, c='m', alpha=.5, label='SD')\n",
      "plt.plot(s, mean_s1_nn3, c='k', label='Mean')\n",
      "plt.plot(s, neg_sd_nn3s, c='k', alpha=.5, label='SD')\n",
      "plt.plot(s, pos_sd_nn3s, c='k', alpha=.5, label='SD')\n",
      "plt.plot(s, mean_s1_opt, c='r', label='Mean')\n",
      "plt.plot(s, neg_sd_opt, c='r', alpha=.5, label='SD')\n",
      "plt.plot(s, pos_sd_opt, c='r', alpha=.5, label='SD')\n",
      "plt.xlabel(r'$s_2$',fontsize=16)\n",
      "plt.ylabel(r'\\hat{s_1}',fontsize=16)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = range(-30, 30)\n",
      "neg_sd_nn1s = mean_s2_nn1 - np.sqrt(var_s2_nn1)\n",
      "pos_sd_nn1s = mean_s2_nn1 + np.sqrt(var_s2_nn1)\n",
      "neg_sd_nn2s = mean_s2_nn2 - np.sqrt(var_s2_nn2)\n",
      "pos_sd_nn2s = mean_s2_nn2 + np.sqrt(var_s2_nn2)\n",
      "neg_sd_nn3s = mean_s2_nn3 - np.sqrt(var_s2_nn3)\n",
      "pos_sd_nn3s = mean_s2_nn3 + np.sqrt(var_s2_nn3)\n",
      "neg_sd_opt = mean_s2_opt - np.sqrt(var_s2_opt)\n",
      "pos_sd_opt = mean_s2_opt + np.sqrt(var_s2_opt)\n",
      "plt.rc('text', usetex=True)\n",
      "plt.plot(s, mean_s2_nn1, c='b', label='Mean')\n",
      "plt.plot(s, neg_sd_nn1s, c='b', alpha=.5, label='SD')\n",
      "plt.plot(s, pos_sd_nn1s, c='b', alpha=.5, label='SD')\n",
      "plt.plot(s, mean_s2_nn2, c='m', label='Mean')\n",
      "plt.plot(s, neg_sd_nn2s, c='m', alpha=.5, label='SD')\n",
      "plt.plot(s, pos_sd_nn2s, c='m', alpha=.5, label='SD')\n",
      "plt.plot(s, mean_s2_nn3, c='k', label='Mean')\n",
      "plt.plot(s, neg_sd_nn3s, c='k', alpha=.5, label='SD')\n",
      "plt.plot(s, pos_sd_nn3s, c='k', alpha=.5, label='SD')\n",
      "plt.plot(s, mean_s2_opt, c='r', label='Mean')\n",
      "plt.plot(s, neg_sd_opt, c='r', alpha=.5, label='SD')\n",
      "plt.plot(s, pos_sd_opt, c='r', alpha=.5, label='SD')\n",
      "plt.xlabel(r'$s_2$',fontsize=16)\n",
      "plt.ylabel(r'\\hat{s_2}',fontsize=16)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Figure out what's wrong with numpy\n",
      "\n",
      "td20 = generate_trainset(2000000)\n",
      "td30 = generate_trainset(3000000)\n",
      "td40 = generate_trainset(4000000)\n",
      "td50 = generate_trainset(5000000)\n",
      "td60 = generate_trainset(6000000)\n",
      "td70 = generate_trainset(7000000)\n",
      "td80 = generate_trainset(8000000)\n",
      "td90 = generate_trainset(9000000)\n",
      "td100 = generate_trainset(10000000)\n",
      "nn20, nnx20 = train_nn(td20, n_hidden=20, learning_rate=.001, n_epochs=1)\n",
      "nn30, nnx30 = train_nn(td30, n_hidden=20, learning_rate=.001, n_epochs=1)\n",
      "nn40, nnx40 = train_nn(td40, n_hidden=20, learning_rate=.001, n_epochs=1)\n",
      "nn50, nnx50 = train_nn(td50, n_hidden=20, learning_rate=.001, n_epochs=1)\n",
      "nn60, nnx60 = train_nn(td60, n_hidden=20, learning_rate=.001, n_epochs=1)\n",
      "nn70, nnx70 = train_nn(td70, n_hidden=20, learning_rate=.001, n_epochs=1)\n",
      "nn80, nnx80 = train_nn(td80, n_hidden=20, learning_rate=.001, n_epochs=1)\n",
      "nn90, nnx90 = train_nn(td90, n_hidden=20, learning_rate=.001, n_epochs=1)\n",
      "nn100, nnx100 = train_nn(td100, n_hidden=20, learning_rate=.001, n_epochs=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r20, s, c = generate_s_data(-20, 20, 1, con_0=.5, con_1=0)\n",
      "rm20, s, c = generate_s_data(-20, 20, 1, con_0=0, con_1=.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(np.linspace(min_angle, max_angle, nneuron), r20[0], c='r')\n",
      "plt.ylabel(\"spikes\", fontsize=30)\n",
      "plt.xlabel(\"$s_{pref}$\", fontsize=30)\n",
      "plt.ylim([-.3, 11])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(np.linspace(min_angle, max_angle, nneuron), rm20[0])\n",
      "plt.ylabel(\"spikes\", fontsize=30)\n",
      "plt.xlabel(\"$s_{pref}$\", fontsize=30)\n",
      "plt.ylim([-.3, 11])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(10,10))\n",
      "plt.scatter(np.linspace(min_angle, max_angle, nneuron), rm20[0] + r20[0], c='m')\n",
      "plt.ylabel(\"spikes\", fontsize=30)\n",
      "plt.xlabel(\"$s_{pref}$\", fontsize=30)\n",
      "plt.ylim([-.3, 11])\n",
      "plt.arrow(-20, 5, 0, -4.85, head_width=5, head_length=.5, fc='orange', ec='orange')\n",
      "plt.arrow(20, 5, 0, -4.85, head_width=5, head_length=.5, fc='orange', ec='orange')\n",
      "plt.arrow(-19.7, 5, 0, -4.85, head_width=5, head_length=.5, fc='m', ec='m')\n",
      "plt.arrow(22.7, 5, 0, -4.85, head_width=5, head_length=.5, fc='m', ec='m')\n",
      "plt.arrow(-21, 5, 0, -4.85, head_width=5, head_length=.5, fc='k', ec='k')\n",
      "plt.arrow(23.5, 5, 0, -4.85, head_width=5, head_length=.5, fc='k', ec='k')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r, _, _ = valid_data_1\n",
      "fit_optimal(r[0:300], sm)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fit_optimal([rm20[0] + r20[0]], sm)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn_preds_rms, _ = test_nn(nn_rms, nnx_rms, ([rm20[0] + r20[0]], s, c))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn_preds_rms * 90 "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rv, _, _ = valid_data_1\n",
      "fit_optimal(rv, sm)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}