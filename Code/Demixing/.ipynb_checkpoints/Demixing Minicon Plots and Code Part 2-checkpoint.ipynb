{
 "metadata": {
  "name": "",
  "signature": "sha256:28458e3c9b6b091fd66c8b333b5c48c8fba9211fc1f798f4fabb2ea35324915c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Part 2: Predicting precision: straight sum, linearly trained, specifically trained.\n",
      "\n",
      "Setup:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.stats import poisson\n",
      "import matplotlib.patches as mpatches\n",
      "from functools import partial\n",
      "\n",
      "nneuron = 61\n",
      "min_angle = -90\n",
      "max_angle = 90\n",
      "sprefs = np.linspace(min_angle, max_angle, nneuron)\n",
      "eps = np.finfo(np.float64).eps\n",
      "sigtc_sq = float(10**2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cartesian(arrays, out=None):\n",
      "    \"\"\"Generate a cartesian product of input arrays.\n",
      "    Parameters\n",
      "    ----------\n",
      "    arrays : list of array-like\n",
      "        1-D arrays to form the cartesian product of.\n",
      "    out : ndarray\n",
      "        Array to place the cartesian product in.\n",
      "    Returns\n",
      "    -------\n",
      "    out : ndarray\n",
      "        2-D array of shape (M, len(arrays)) containing cartesian products\n",
      "        formed of input arrays.\n",
      "    Examples\n",
      "    --------\n",
      "    >>> cartesian(([1, 2, 3], [4, 5], [6, 7]))\n",
      "    array([[1, 4, 6],\n",
      "           [1, 4, 7],\n",
      "           [1, 5, 6],\n",
      "           [1, 5, 7],\n",
      "           [2, 4, 6],\n",
      "           [2, 4, 7],\n",
      "           [2, 5, 6],\n",
      "           [2, 5, 7],\n",
      "           [3, 4, 6],\n",
      "           [3, 4, 7],\n",
      "           [3, 5, 6],\n",
      "           [3, 5, 7]])\n",
      "    \"\"\"\n",
      "    arrays = [np.asarray(x) for x in arrays]\n",
      "    shape = (len(x) for x in arrays)\n",
      "    dtype = arrays[0].dtype\n",
      "\n",
      "    ix = np.indices(shape)\n",
      "    ix = ix.reshape(len(arrays), -1).T\n",
      "\n",
      "    if out is None:\n",
      "        out = np.empty_like(ix, dtype=dtype)\n",
      "\n",
      "    for n, arr in enumerate(arrays):\n",
      "        out[:, n] = arrays[n][ix[:, n]]\n",
      "\n",
      "    return out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generate data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def random_s(ndata, sort):\n",
      "    s = np.random.rand(2, ndata) * 120 - 60\n",
      "    if sort:\n",
      "        s = np.sort(s, axis=0)\n",
      "    return s[0], s[1]\n",
      "\n",
      "def random_c(ndata, ndims, low, high, sort):\n",
      "    c_range = high - low\n",
      "    if ndims == 1:\n",
      "        c = np.random.rand(ndims, ndata)[0] * c_range + low\n",
      "    else:\n",
      "        c = np.random.rand(ndims, ndata) * c_range + low\n",
      "    if sort:\n",
      "        c = np.sort(c, axis=0)\n",
      "    return c\n",
      "    \n",
      "def generate_popcode_data(ndata, nneuron, sigtc_sq, r_max, noise, sort, s_0, s_1, c_0, c_1, c_50=13.1):\n",
      "    c_rms = np.sqrt(np.square(c_0) + np.square(c_1))\n",
      "    sprefs_data = np.tile(sprefs, (ndata, 1))\n",
      "    s_0t = np.exp(-np.square((np.transpose(np.tile(s_0, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_0 = c_0 * s_0t.T\n",
      "    s_1t = np.exp(-np.square((np.transpose(np.tile(s_1, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_1 = c_1 * s_1t.T\n",
      "    #r = r_max * (stim_0 + stim_1)/(c_50 + c_rms)\n",
      "    r = r_max * (stim_0 + stim_1)\n",
      "    r = r.T\n",
      "    s = np.array((s_0, s_1)).T\n",
      "    s = s/90\n",
      "    c = np.array((c_0, c_1)).T\n",
      "    if noise == \"poisson\":\n",
      "        r = np.random.poisson(r) + 0.0\n",
      "    return r, s, c\n",
      "\n",
      "def generate_trainset(ndata, highlow=False, discrete_c=None, low=.3, high=.7, r_max=10):\n",
      "    s_0, s_1 = random_s(ndata, True)\n",
      "    if highlow:\n",
      "        c_0, c_1 = np.concatenate((np.ones((2, ndata/2)) * low, np.ones((2, ndata/2)) * high), axis=1)\n",
      "    elif discrete_c:\n",
      "        cs = np.linspace(low, high, discrete_c)\n",
      "        perm_cs = cartesian((cs, cs)).T\n",
      "        c_0, c_1 = np.repeat(perm_cs, ndata/(discrete_c**2), axis=1)\n",
      "        print ndata/(discrete_c**2), \"trials per contrast level\"\n",
      "        if ndata%(discrete_c**2) != 0:\n",
      "            print \"Not divisible, only generated\", ndata / (discrete_c**2) * (discrete_c**2), \"trials\"\n",
      "        ndata = ndata / (discrete_c**2) * (discrete_c**2)\n",
      "    else:\n",
      "        c_0, c_1 = np.ones((2, ndata)) * .5\n",
      "    r, s, c = generate_popcode_data(ndata, nneuron, sigtc_sq, r_max, \"poisson\", True, s_0, s_1, c_0, c_1)\n",
      "    return r, s, c\n",
      "\n",
      "def generate_testset(ndata, stim_0=None, stim_1=None, con_0=None, con_1=None, discrete_c=None, low=.5, high=.5, r_max=10):\n",
      "    if con_0:\n",
      "        c_0 = np.ones(ndata) * con_0\n",
      "        c_1 = np.ones(ndata) * con_1\n",
      "    else:\n",
      "        c_range = high - low\n",
      "        if discrete_c:\n",
      "            cs = np.linspace(low, high, discrete_c)\n",
      "            perm_cs = cartesian((cs, cs)).T\n",
      "            c_0, c_1 = np.repeat(perm_cs, ndata/(discrete_c**2), axis=1)\n",
      "            print ndata/(discrete_c**2), \"trials per contrast level\"\n",
      "            if ndata%(discrete_c**2) != 0:\n",
      "                print \"Not divisible, only generated\", ndata / (discrete_c**2) * (discrete_c**2), \"trials\"\n",
      "            ndata = ndata / (discrete_c**2) * (discrete_c**2)\n",
      "        else:\n",
      "            c_0, c_1 = np.random.rand(2, ndata) * c_range + low\n",
      "    if not stim_0:\n",
      "        s_0, s_1 = random_s(ndata, True)\n",
      "    else:\n",
      "        s_0, s_1 = np.ones((2, ndata))\n",
      "        s_0 = s_0 * stim_0\n",
      "        s_1 = s_1 * stim_1\n",
      "    r, s, c = generate_popcode_data(ndata, nneuron, sigtc_sq, r_max, \"poisson\", True, s_0, s_1, c_0, c_1)\n",
      "    return r, s, c\n",
      "\n",
      "def generate_trainset_cat(ndata, low=.3, high=1.3, crange=.5, r_max=10):\n",
      "    numvec = np.random.binomial(1, .5, size=ndata).astype(int)\n",
      "    c_0 = random_c(ndata, 1, high, high+crange, True)\n",
      "    c_1 = random_c(ndata, 1, low, low+crange, True)\n",
      "    s_0, s_1 = np.random.rand(2, ndata) * 120 - 60\n",
      "    r, numvec, s, c  = generate_popcode_data_cat(ndata, numvec, nneuron, sigtc_sq, c_50, r_max, \"poisson\", s_0, s_1, c_0, c_1)\n",
      "    y = s[range(ndata), numvec]\n",
      "    return r, y, s, c, numvec \n",
      "    \n",
      "def generate_popcode_data_cat(ndata, numvec, nneuron, sigtc_sq, c_50, r_max, noise, s_0, s_1, c_0, c_1):\n",
      "    c0vec = c_0 * np.ones(ndata)\n",
      "    c1vec = c_1 * numvec\n",
      "    c_rms = np.sqrt(np.square(c0vec) + np.square(c1vec))\n",
      "    sprefs_data = np.tile(sprefs, (ndata, 1))\n",
      "    s_0t = np.exp(-np.square((np.transpose(np.tile(s_0, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_0 = c0vec * s_0t.T\n",
      "    s_1t = np.exp(-np.square((np.transpose(np.tile(s_1, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_1 = c1vec * s_1t.T\n",
      "    #r = r_max * (stim_0 + stim_1)/(c_50 + c_rms)\n",
      "    r = r_max * (stim_0 + stim_1)/(c_rms)\n",
      "    #r = r_max * (stim_0 + stim_1)\n",
      "    r = r.T\n",
      "    s = np.array((s_0, s_1)).T\n",
      "    s = s/90\n",
      "    c = np.array((c_0, c_1)).T\n",
      "    if noise == \"poisson\":\n",
      "        r = np.random.poisson(r) + 0.0\n",
      "    return r, numvec, s, c "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Posterior computation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lik_means(s_1, s_2, c_0=.5, c_1=.5, sprefs=sprefs, sigtc_sq=sigtc_sq, r_max=10):\n",
      "    sprefs_data = np.tile(sprefs, (len(s_1), 1))\n",
      "    s_0t = np.exp(-np.square((np.transpose(np.tile(s_1, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_0 = c_0 * s_0t.T\n",
      "    s_1t = np.exp(-np.square((np.transpose(np.tile(s_2, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_1 = c_1 * s_1t.T\n",
      "    r = r_max * (stim_0 + stim_1)\n",
      "    return r.T\n",
      "def posterior(r, means, s1_grid, s2_grid):\n",
      "    ns_liks = poisson.pmf(r, mu=means)\n",
      "    stim_liks = np.prod(ns_liks, axis=1)\n",
      "    #p_s = 2/14400\n",
      "    #logp_s = np.log(p_s)\n",
      "    logp_s = -3.8573325\n",
      "    loglik = np.sum(np.log(ns_liks), axis=1)\n",
      "    mean1 = np.sum(s1_grid * np.exp(loglik + logp_s)/np.sum(np.exp(loglik + logp_s)))\n",
      "    mean2 = np.sum(s2_grid * np.exp(loglik + logp_s)/np.sum(np.exp(loglik + logp_s)))\n",
      "    expsquare1 = np.sum(np.square(s1_grid) * np.exp(loglik + logp_s)/np.sum(np.exp(loglik + logp_s)))\n",
      "    expsquare2 = np.sum(np.square(s2_grid) * np.exp(loglik + logp_s)/np.sum(np.exp(loglik + logp_s)))\n",
      "    var1 = expsquare1 - np.square(mean1)\n",
      "    var2 = expsquare2 - np.square(mean2)\n",
      "    return mean1, mean2, var1, var2\n",
      "def posterior_setup(low=.3, high=.7, discrete_c = 3, num_s=100, r_max=10):\n",
      "    grid = np.linspace(-60, 60, num_s)\n",
      "    s1s = np.concatenate([[grid[i]]*(num_s-i) for i in range(num_s)])\n",
      "    cs = np.linspace(low, high, discrete_c)\n",
      "    s1_grid, c1_grid, c2_grid = cartesian((s1s, cs, cs)).T\n",
      "    s2s = np.concatenate([grid[i:num_s+1] for i in range(num_s)])\n",
      "    s2_grid = np.repeat(s2s, (discrete_c**2), axis=0)\n",
      "    means = lik_means(s1_grid, s2_grid, c_0=c1_grid, c_1=c2_grid, r_max=r_max)\n",
      "    partial_post = partial(posterior, means=means, s1_grid=s1_grid, s2_grid=s2_grid)\n",
      "    return partial_post\n",
      "def get_posteriors(r, post_func):\n",
      "    posteriors = {'mean_s1': None, 'mean_s2': None, 'var_s1': None, 'var_s2': None}\n",
      "    p = np.array([post_func(r[i]) for i in range(len(r))]).T\n",
      "    posteriors['mean_s1'], posteriors['mean_s2'], posteriors['var_s1'], posteriors['var_s2'] = p\n",
      "    return posteriors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Network computation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Multilayer ReLU net\n",
      "\"\"\"\n",
      "\n",
      "def relu(x):\n",
      "    return theano.tensor.switch(x<0, 0, x)\n",
      "\n",
      "class HiddenLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
      "                 activation=T.nnet.sigmoid):\n",
      "        \"\"\"\n",
      "        Typical hidden layer of a MLP: units are fully-connected and have\n",
      "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
      "        and the bias vector b is of shape (n_out,).\n",
      "\n",
      "        :type rng: np.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.dmatrix\n",
      "        :param input: a symbolic tensor of shape (n_examples, n_in)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: dimensionality of input\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of hidden units\n",
      "\n",
      "        :type activation: theano.Op or function\n",
      "        :param activation: Non linearity to be applied in the hidden\n",
      "                           layer\n",
      "        \"\"\"\n",
      "        self.input = input\n",
      "        if W is None:\n",
      "            W_values = (1/np.sqrt(n_in)) * np.random.randn(n_in, n_out)\n",
      "            \n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        if b is None:\n",
      "            b_values = np.zeros((n_out,), dtype=theano.config.floatX)\n",
      "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "\n",
      "        lin_output = T.dot(input, self.W) + self.b\n",
      "        self.output = (\n",
      "            lin_output if activation is None\n",
      "            else activation(lin_output)\n",
      "        )\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "class MLP(object):\n",
      "\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
      "\n",
      "        :type rng: np.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: symbolic variable that describes the input of the\n",
      "        architecture (one minibatch)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: number of input units, the dimension of the space in\n",
      "        which the datapoints lie\n",
      "\n",
      "        :type n_hidden: int\n",
      "        :param n_hidden: number of hidden units\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "        which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        self.hiddenLayer1 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            #activation=T.nnet.sigmoid\n",
      "            activation=relu\n",
      "        )\n",
      "        \n",
      "        self.hiddenLayer2 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=self.hiddenLayer1.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out,\n",
      "            #activation=relu\n",
      "            activation=None\n",
      "        )\n",
      "        \n",
      "        self.y_pred = self.hiddenLayer2.output\n",
      "        \n",
      "        # the parameters of the model are the parameters of the two layers it is made out of\n",
      "        self.params = self.hiddenLayer1.params + self.hiddenLayer2.params\n",
      "    \n",
      "    def get_params(self):\n",
      "\n",
      "        params = {}\n",
      "        for param in self.params:\n",
      "            name = param.name\n",
      "            if name in params:\n",
      "                name = name, 2\n",
      "            params[name] = param.get_value()\n",
      "        return params\n",
      "    \n",
      "    def mse(self, y):\n",
      "        # error between output and target\n",
      "        if y.ndim == 1:\n",
      "            se = (self.y_pred.T - y)**2\n",
      "        else:\n",
      "            se = T.sum((self.y_pred - y)**2, axis=1)\n",
      "        return T.mean(se)\n",
      "        \n",
      "    \n",
      "    def valid_mse(self, y):\n",
      "        if y.ndim == 1:\n",
      "            se = (self.y_pred.T * 90 - y * 90)**2\n",
      "        else:\n",
      "            se = T.sum((self.y_pred * 90 - y * 90)**2, axis=1)\n",
      "        return T.mean(se)\n",
      "\n",
      "    \n",
      "class Perceptron(object):\n",
      "\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_out):\n",
      "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
      "\n",
      "        :type rng: np.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: symbolic variable that describes the input of the\n",
      "        architecture (one minibatch)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: number of input units, the dimension of the space in\n",
      "        which the datapoints lie\n",
      "\n",
      "        :type n_hidden: int\n",
      "        :param n_hidden: number of hidden units\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "        which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        self.layer = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_out,\n",
      "            #activation=T.nnet.sigmoid\n",
      "            activation=relu\n",
      "        )\n",
      "        \n",
      "        self.y_pred = self.layer.output\n",
      "        \n",
      "        # the parameters of the model are the parameters of the two layers it is made out of\n",
      "        self.params = self.layer.params\n",
      "        \n",
      "    def get_params(self):\n",
      "\n",
      "        params = {}\n",
      "        for param in self.params:\n",
      "            name = param.name\n",
      "            if name in params:\n",
      "                name = name, 2\n",
      "            params[name] = param.get_value()\n",
      "        return params\n",
      "    \n",
      "    def mse(self, y):\n",
      "        # error between output and target\n",
      "        if y.ndim == 1:\n",
      "            se = (self.y_pred.T - y)**2\n",
      "        else:\n",
      "            se = T.sum((self.y_pred - y)**2, axis=1)\n",
      "        return T.mean(se)\n",
      "    \n",
      "    def valid_mse(self, y):\n",
      "        return mse(self, y)\n",
      "        \n",
      "\n",
      "def shared_dataset(data_xy, borrow=True, no_c=False):\n",
      "        \"\"\" Function that loads the dataset into shared variables\n",
      "        \"\"\"\n",
      "        data_x, data_y = data_xy[:2]\n",
      "        shared_x = theano.shared(np.asarray(data_x,\n",
      "                                               dtype='float32'),\n",
      "                                 borrow=borrow)\n",
      "        shared_y = theano.shared(np.asarray(data_y,\n",
      "                                               dtype='float32'),\n",
      "                                 borrow=borrow)\n",
      "        return shared_x, shared_y\n",
      "\n",
      "def train_nn(train_dataset, valid_dataset=None, n_hidden=20, learning_rate=0.01, n_epochs=10, batch_size=20, linear=False, mult_ys=True, rho=0, nesterov=True, mu=0, n_in=61, n_out=2):\n",
      "    \"\"\"\n",
      "    Demonstrate stochastic gradient descent optimization for a multilayer\n",
      "    perceptron\n",
      "\n",
      "    :type learning_rate: float\n",
      "    :param learning_rate: learning rate used (factor for the stochastic\n",
      "    gradient\n",
      "\n",
      "    :type n_epochs: int\n",
      "    :param n_epochs: maximal number of epochs to run the optimizer\n",
      "\n",
      "   \"\"\"\n",
      "    train_set_x, train_set_y = shared_dataset(train_dataset)\n",
      "    if valid_dataset:\n",
      "        valid_set_x, valid_set_y = shared_dataset(valid_dataset)\n",
      "    \n",
      "    # compute number of minibatches for training, validation and testing\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    \n",
      "    \n",
      "    ######################\n",
      "    # BUILD ACTUAL MODEL #\n",
      "    ######################\n",
      "    print '... building the model'\n",
      "\n",
      "    # allocate symbolic variables for the data\n",
      "    index = T.lscalar()  # index to a [mini]batch\n",
      "    x = T.fmatrix('x')   # input data from visual neurons\n",
      "    if n_out == 1:\n",
      "        y = T.fvector('y') # ground truth\n",
      "    else:\n",
      "        y = T.fmatrix('y')  # ground truth\n",
      "\n",
      "    rng = np.random.RandomState(1234)\n",
      "\n",
      "    # construct the MLP class\n",
      "    if linear:\n",
      "        nn = Perceptron(rng=rng, input=x, n_in=n_in, n_out=n_out)\n",
      "    else:\n",
      "        nn = MLP(rng=rng, input=x, n_in=n_in, n_hidden=n_hidden, n_out=n_out)\n",
      "    \n",
      "    cost = nn.mse(y)\n",
      "    \n",
      "    def RMSprop(cost, params, learning_rate=0.001, rho=0.9, epsilon=1e-6, mu=0, nesterov=False):\n",
      "        gparams = T.grad(cost, params)\n",
      "        updates = []\n",
      "        for p, g in zip(params, gparams):\n",
      "            v = theano.shared(p.get_value() * 0.)\n",
      "            ms = theano.shared(p.get_value() * 0.)\n",
      "            ms_new = rho * ms + (1 - rho) * g ** 2\n",
      "            gradient_scaling = T.sqrt(ms_new + epsilon)\n",
      "            g = g / gradient_scaling\n",
      "            \"\"\"\n",
      "            (1) v_t = mu * v_t-1 - lr * gradient_f(params_t)\n",
      "            or\n",
      "            classic\n",
      "            (2) params_t = params_t-1 + v_t\n",
      "            nesterov\n",
      "            (7) params_t = params_t-1 + mu * v_t - lr * gradient_f(params_t-1)\n",
      "            (8) params_t = params_t-1 + mu**2 * v_t-1 - (1+mu) * lr * gradient_f(params_t-1)\n",
      "            \"\"\"\n",
      "            v_new = mu * v - (1 - mu) * learning_rate * g\n",
      "            if nesterov:\n",
      "                p_new = p + mu * v_new - (1 - mu) * learning_rate * g\n",
      "            else:\n",
      "                p_new = p + v_new\n",
      "            updates.append((ms, ms_new))\n",
      "            updates.append((v, v_new))\n",
      "            updates.append((p, p_new))\n",
      "                \n",
      "        return updates\n",
      "    \n",
      "    if rho:\n",
      "        updates = RMSprop(cost, nn.params, learning_rate=learning_rate, rho=rho, mu=mu, nesterov=nesterov)\n",
      "    else:\n",
      "        # compute the gradient of cost with respect to theta (sotred in params)\n",
      "        # the resulting gradients will be stored in a list gparams\n",
      "        gparams = [T.grad(cost, param) for param in nn.params]\n",
      "\n",
      "        # specify how to update the parameters of the model as a list of\n",
      "        # (variable, update expression) pairs\n",
      "\n",
      "        updates = [\n",
      "            (param, param - learning_rate * gparam)\n",
      "            for param, gparam in zip(nn.params, gparams)\n",
      "        ]\n",
      "    \n",
      "    def inspect_inputs(i, node, fn):\n",
      "        print i, node, \"input(s) value(s):\", [input[0] for input in fn.inputs]\n",
      "\n",
      "    def inspect_outputs(i, node, fn):\n",
      "        print \"output(s) value(s):\", [output[0] for output in fn.outputs]\n",
      "\n",
      "    # compiling a Theano function `train_model` that returns the cost, but\n",
      "    # in the same time updates the parameter of the model based on the rules\n",
      "    # defined in `updates`\n",
      "    train_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=cost,\n",
      "        updates=updates,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "    \n",
      "    if mult_ys:\n",
      "        valid_mse = nn.valid_mse(y)\n",
      "    else:\n",
      "        valid_mse = cost\n",
      "    \n",
      "    validate_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=valid_mse,\n",
      "        givens={\n",
      "            x: valid_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "            y: valid_set_y[index * batch_size:(index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "\n",
      "    ###############\n",
      "    # TRAIN MODEL #\n",
      "    ###############\n",
      "    print '... training'\n",
      "\n",
      "    epoch = 0 \n",
      "\n",
      "    if valid_dataset:\n",
      "        valid_mse = np.zeros(n_epochs)\n",
      "\n",
      "    while (epoch < n_epochs):\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "            \n",
      "            minibatch_avg_cost = train_model(minibatch_index)\n",
      "            \n",
      "        if valid_dataset:\n",
      "            validation_losses = [validate_model(i) for i in xrange(n_valid_batches)]\n",
      "            this_validation_loss = np.mean(validation_losses)\n",
      "            valid_mse[epoch] = this_validation_loss\n",
      "            print(\n",
      "                'epoch %i, validation error %f' %\n",
      "                (\n",
      "                    epoch,\n",
      "                    this_validation_loss,\n",
      "                )\n",
      "            )\n",
      "            \n",
      "        epoch = epoch + 1\n",
      "\n",
      "    if valid_dataset:\n",
      "        return nn, x, valid_mse\n",
      "    return nn, x\n",
      "\n",
      "def test_nn(nn, nnx, test_data):\n",
      "    print 'testing'\n",
      "    test_batch_size = 1\n",
      "    test_set_x, test_set_y = shared_dataset(test_data)\n",
      "    index = T.lscalar()  # index to a [mini]batch\n",
      "    x = nnx   # input data from visual neurons\n",
      "    test_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=nn.y_pred,\n",
      "        givens={\n",
      "            x: test_set_x[index * test_batch_size: (index + 1) * test_batch_size]\n",
      "        },\n",
      "    )\n",
      "    \n",
      "    true_ys = test_set_y.get_value()\n",
      "    pred_ys = np.zeros((len(true_ys), 2))\n",
      "    for i in range(len(true_ys)):\n",
      "        pred_ys[i] = test_model(i)\n",
      "        #print test_model(i)[0], true_ys[i]\n",
      "        #print test_model(i)[0] * 90, true_ys[i]\n",
      "    \n",
      "    #print nn.get_params()\n",
      "    return pred_ys, true_ys"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Train network:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r1, s1, c1 = generate_testset(20000, discrete_c=3, low=.3, high=.7)\n",
      "vr1, vs1, vc1 = generate_testset(300, discrete_c=3, low=.3, high=.7)\n",
      "nn, nnx, valid_nn = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100, rho=.9)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Readout of posterior:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def python_relu(x):\n",
      "    return x * (x > 0)\n",
      "\n",
      "def get_hu_responses(r, nn):\n",
      "    nn_params = nn.get_params()\n",
      "    b = nn_params['b']\n",
      "    W = nn_params['W']\n",
      "    trials = python_relu(np.dot(r, W) + b)\n",
      "    return trials\n",
      "\n",
      "def post_trainset(r, nn, post_func):\n",
      "    x = get_hu_responses(r, nn)\n",
      "    posts = get_posteriors(r, post_func)\n",
      "    y = np.array((1/posts['var_s1'], 1/posts['var_s2']))\n",
      "    return x, y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "post_func = posterior_setup(low=.3, high=.7, discrete_c = 3, num_s=120)\n",
      "r2, s2, c2 = generate_testset(20000, discrete_c=3, low=.3, high=.7)\n",
      "vr2, vs2, vc2 = generate_testset(300, discrete_c=3, low=.3, high=.7)\n",
      "hlr, post = post_trainset(r, nn_rms, post_func)\n",
      "print \"valid\"\n",
      "vhlr, vpost = post_trainset(vr, nn_rms, post_func)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.3  0.3  0.3  0.5  0.5  0.5  0.7  0.7  0.7]\n",
        " [ 0.3  0.5  0.7  0.3  0.5  0.7  0.3  0.5  0.7]]\n",
        "2222 trials per contrast level\n",
        "Not divisible, only generated 19998 trials\n"
       ]
      },
      {
       "ename": "NameError",
       "evalue": "name 'nn_rms' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-37-ca45ca9ae017>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_testset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscrete_c\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_testset_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mhlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost_trainset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_rms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mvhlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvpost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost_trainset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_rms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'nn_rms' is not defined"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}