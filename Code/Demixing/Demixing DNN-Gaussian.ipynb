{
 "metadata": {
  "name": "",
  "signature": "sha256:58bf4adf962a426c801795745ddf8c94463b35f6c07115d1139757efe0a78ff1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import math\n",
      "\n",
      "nneuron = 61\n",
      "min_angle = 0\n",
      "max_angle = 180\n",
      "sprefs = np.linspace(min_angle, max_angle, nneuron)\n",
      "ndata = 3000\n",
      "\n",
      "r_max = 30\n",
      "sigtc_sq = float(10**2)\n",
      "sigtc = 10\n",
      "c_50 = 13.1\n",
      "\n",
      "def generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, noise, sort):\n",
      "    s = np.random.rand(2, ndata) * 120 + 30\n",
      "    if sort:\n",
      "        s = np.sort(s, axis=0)\n",
      "    c = np.ones((2, ndata)) * .5\n",
      "    c_rms = np.sqrt(np.square(c[0]) + np.square(c[1]))\n",
      "    sprefs_data = np.tile(sprefs, (ndata, 1))\n",
      "    s_0 = np.exp(-np.square((np.transpose(np.tile(s[0], (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_0 = c[0] * s_0.T\n",
      "    s_1 = np.exp(-np.square((np.transpose(np.tile(s[1], (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_1 = c[1] * s_1.T\n",
      "    r = r_max * (stim_0 + stim_1)/(c_50 + c_rms)\n",
      "    r = r.T\n",
      "    s = s.T\n",
      "    #s = s/90\n",
      "    c = c.T\n",
      "    if noise == \"poisson\":\n",
      "        r = np.random.poisson(r) + 0.0\n",
      "    return r, s, c\n",
      "r, s, c = generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True)\n",
      "print sprefs, r[0], s[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[   0.    3.    6.    9.   12.   15.   18.   21.   24.   27.   30.   33.\n",
        "   36.   39.   42.   45.   48.   51.   54.   57.   60.   63.   66.   69.\n",
        "   72.   75.   78.   81.   84.   87.   90.   93.   96.   99.  102.  105.\n",
        "  108.  111.  114.  117.  120.  123.  126.  129.  132.  135.  138.  141.\n",
        "  144.  147.  150.  153.  156.  159.  162.  165.  168.  171.  174.  177.\n",
        "  180.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  2.\n",
        "  1.  1.  2.  2.  1.  3.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0.  0.  0.  1.  2.  1.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0.  0.  0.  0.  0.] [  61.63361167  127.37810312]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pystan\n",
      "neurons_code = \"\"\"\n",
      "data {\n",
      "    int<lower=0> N; // number of neurons\n",
      "    int r[N]; // neural response\n",
      "    real sprefs[N]; // preferred stimuli\n",
      "    real<lower=0> c_1;\n",
      "    real<lower=0> c_2;\n",
      "    real c_50;\n",
      "    int r_max;\n",
      "    real c_rms;\n",
      "    real<lower=0> sig_tc;\n",
      "}\n",
      "parameters {\n",
      "    real s_1;\n",
      "    real s_2;\n",
      "}\n",
      "transformed parameters {\n",
      "    real lambda[N];\n",
      "    for (n in 1:N)\n",
      "        lambda[n] <- r_max * ((c_1 * exp(normal_log(s_1, sprefs[n], sig_tc)) + c_2 * exp(normal_log(s_2, sprefs[n], sig_tc)))/(c_rms + c_50));\n",
      "}\n",
      "model {\n",
      "    s_1 ~ uniform(-60, 60);\n",
      "    s_2 ~ uniform(-60, 60);\n",
      "    r ~ poisson(lambda);\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "ndata = 1\n",
      "r, s, _ = generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True)\n",
      "\n",
      "neurons_dat = {'N': 61,\n",
      "               'r': r[0].astype(int),\n",
      "               'sprefs': sprefs,\n",
      "               'c_1': .5,\n",
      "               'c_2': .5,\n",
      "               'c_50': 13.1,\n",
      "               'r_max': r_max,\n",
      "               'c_rms': 0.707106781,\n",
      "               'sig_tc': 10}\n",
      "\n",
      "fit = pystan.stan(model_code=neurons_code, data=neurons_dat,\n",
      "                  iter=1000, chains=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "//anaconda/lib/python2.7/multiprocessing/queues.py:390: UserWarning: Pickling fit objects is an experimental feature!\n",
        "The relevant StanModel instance must be pickled along with this fit object.\n",
        "When unpickling the StanModel must be unpickled first.\n",
        "  return send(obj)\n",
        "//anaconda/lib/python2.7/multiprocessing/queues.py:390: UserWarning: Pickling fit objects is an experimental feature!\n",
        "The relevant StanModel instance must be pickled along with this fit object.\n",
        "When unpickling the StanModel must be unpickled first.\n",
        "  return send(obj)\n",
        "//anaconda/lib/python2.7/multiprocessing/queues.py:390: UserWarning: Pickling fit objects is an experimental feature!\n",
        "The relevant StanModel instance must be pickled along with this fit object.\n",
        "When unpickling the StanModel must be unpickled first.\n",
        "  return send(obj)\n",
        "//anaconda/lib/python2.7/multiprocessing/queues.py:390: UserWarning: Pickling fit objects is an experimental feature!\n",
        "The relevant StanModel instance must be pickled along with this fit object.\n",
        "When unpickling the StanModel must be unpickled first.\n",
        "  return send(obj)\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "print r[0]\n",
      "print s[0]\n",
      "print fit\n",
      "samps = fit.extract(['s_1', 's_2'])\n",
      "print len(samps['s_1'])\n",
      "print np.mean(samps['s_1'])\n",
      "print np.mean(samps['s_2'])\n",
      "plt.scatter(samps['s_1'], samps['s_2'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0.  2.\n",
        "  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  1.  1.\n",
        "  0.  0.  2.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0.  0.  0.  0.  0.]\n",
        "[-41.38038125  13.49284968]\n",
        "Inference for Stan model: anon_model_6e983d68febafdd60f07695d738ff861.\n",
        "4 chains, each with iter=1000; warmup=500; thin=1; \n",
        "post-warmup draws per chain=500, total post-warmup draws=2000.\n",
        "\n",
        "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
        "s_1        -25.96   18.38  25.99 -48.39 -42.55 -39.24  -7.73  23.42    2.0   7.02\n",
        "s_2          3.51   18.29  25.87 -45.72 -13.71  16.67  19.64  24.85    2.0   7.66\n",
        "lambda[0]  1.1e-6  1.2e-7 2.6e-6 3.2e-9 6.4e-8 2.5e-7 8.6e-7 8.3e-6  469.0    nan\n",
        "lambda[1]  3.7e-6  3.8e-7 8.3e-6 1.7e-8 2.9e-7 1.0e-6 3.3e-6 2.8e-5  470.0    nan\n",
        "lambda[2]  1.2e-5  1.1e-6 2.4e-5 8.4e-8 1.2e-6 4.0e-6 1.2e-5 8.3e-5  472.0    nan\n",
        "lambda[3]  3.6e-5  3.0e-6 6.5e-5 3.8e-7 4.5e-6 1.4e-5 3.8e-5 2.3e-4  475.0    nan\n",
        "lambda[4]  9.9e-5  7.2e-6 1.6e-4 1.5e-6 1.6e-5 4.4e-5 1.1e-4 5.8e-4  480.0    nan\n",
        "lambda[5]  2.5e-4  1.6e-5 3.6e-4 5.7e-6 4.9e-5 1.3e-4 3.0e-4 1.3e-3  485.0    nan\n",
        "lambda[6]  5.9e-4  3.3e-5 7.4e-4 1.9e-5 1.4e-4 3.4e-4 7.4e-4 2.8e-3  492.0    nan\n",
        "lambda[7]  1.3e-3  6.2e-5 1.4e-3 6.0e-5 3.7e-4 8.3e-4 1.7e-3 5.4e-3  501.0    1.0\n",
        "lambda[8]  2.6e-3  1.1e-4 2.4e-3 1.7e-4 9.0e-4 1.9e-3 3.4e-3 9.6e-3  510.0    1.0\n",
        "lambda[9]  4.8e-3  1.7e-4 3.9e-3 4.4e-4 2.0e-3 3.8e-3 6.4e-3   0.02  520.0    1.0\n",
        "lambda[10] 8.2e-3  2.4e-4 5.6e-3 1.0e-3 4.0e-3 7.0e-3   0.01   0.02  531.0    1.0\n",
        "lambda[11]   0.01  3.1e-4 7.3e-3 2.3e-3 7.4e-3   0.01   0.02   0.03  541.0    1.0\n",
        "lambda[12]   0.02  3.7e-4 8.6e-3 4.5e-3   0.01   0.02   0.02   0.04  550.0    1.0\n",
        "lambda[13]   0.03  3.9e-4 9.1e-3 8.1e-3   0.02   0.03   0.03   0.04  555.0    1.0\n",
        "lambda[14]   0.03  3.5e-4 8.3e-3   0.01   0.03   0.03   0.04   0.04  551.0    1.0\n",
        "lambda[15]   0.04  2.8e-4 6.3e-3   0.02   0.03   0.04   0.04   0.04  516.0    1.0\n",
        "lambda[16]   0.04  2.0e-4 4.2e-3   0.03   0.04   0.04   0.04   0.04  435.0   1.01\n",
        "lambda[17]   0.04  2.1e-4 4.4e-3   0.03   0.04   0.04   0.04   0.04  443.0    1.0\n",
        "lambda[18]   0.04  2.9e-4 6.6e-3   0.02   0.03   0.04   0.04   0.04  519.0    1.0\n",
        "lambda[19]   0.03  3.6e-4 8.4e-3   0.01   0.03   0.03   0.04   0.04  548.0    1.0\n",
        "lambda[20]   0.02  3.8e-4 9.1e-3 7.6e-3   0.02   0.02   0.03   0.04  555.0    1.0\n",
        "lambda[21]   0.02  3.6e-4 8.5e-3 4.2e-3   0.01   0.02   0.02   0.04  553.0    1.0\n",
        "lambda[22]   0.01  3.1e-4 7.1e-3 2.1e-3 6.5e-3   0.01   0.02   0.03  546.0    1.0\n",
        "lambda[23] 7.4e-3  2.3e-4 5.4e-3 9.8e-4 3.5e-3 6.0e-3 9.9e-3   0.02  539.0    1.0\n",
        "lambda[24] 4.4e-3  1.6e-4 3.7e-3 4.8e-4 1.8e-3 3.3e-3 5.8e-3   0.01  532.0    1.0\n",
        "lambda[25] 2.6e-3  1.0e-4 2.4e-3 3.0e-4 1.0e-3 1.8e-3 3.3e-3 9.2e-3  524.0   1.01\n",
        "lambda[26] 1.8e-3  6.7e-5 1.5e-3 2.7e-4 8.1e-4 1.4e-3 2.3e-3 6.0e-3  510.0   1.01\n",
        "lambda[27] 2.0e-3  6.9e-5 1.6e-3 3.3e-4 9.3e-4 1.6e-3 2.6e-3 6.2e-3  506.0   1.01\n",
        "lambda[28] 3.1e-3  1.1e-4 2.4e-3 4.7e-4 1.4e-3 2.5e-3 4.1e-3   0.01  518.0   1.01\n",
        "lambda[29] 5.5e-3  1.6e-4 3.8e-3 8.6e-4 2.7e-3 4.6e-3 7.3e-3   0.02  528.0    1.0\n",
        "lambda[30] 9.2e-3  2.3e-4 5.4e-3 1.7e-3 5.1e-3 8.2e-3   0.01   0.02  537.0    1.0\n",
        "lambda[31]   0.01  3.0e-4 7.0e-3 3.3e-3 9.0e-3   0.01   0.02   0.03  545.0    1.0\n",
        "lambda[32]   0.02  3.5e-4 8.1e-3 6.2e-3   0.01   0.02   0.03   0.04  549.0    1.0\n",
        "lambda[33]   0.03  3.5e-4 8.3e-3   0.01   0.02   0.03   0.03   0.04  547.0    1.0\n",
        "lambda[34]   0.03  3.2e-4 7.3e-3   0.02   0.03   0.04   0.04   0.04  531.0    1.0\n",
        "lambda[35]   0.04  2.4e-4 5.3e-3   0.02   0.04   0.04   0.04   0.04  466.0    1.0\n",
        "lambda[36]   0.04  1.8e-4 3.5e-3   0.03   0.04   0.04   0.04   0.04  382.0    1.0\n",
        "lambda[37]   0.04  2.0e-4 4.4e-3   0.03   0.04   0.04   0.04   0.04  480.0    1.0\n",
        "lambda[38]   0.04  2.8e-4 6.5e-3   0.02   0.03   0.04   0.04   0.04  544.0    1.0\n",
        "lambda[39]   0.03  3.4e-4 8.0e-3   0.01   0.02   0.03   0.04   0.04  556.0    1.0\n",
        "lambda[40]   0.02  3.5e-4 8.3e-3 7.5e-3   0.02   0.02   0.03   0.04  550.0    1.0\n",
        "lambda[41]   0.02  3.2e-4 7.5e-3 4.1e-3   0.01   0.01   0.02   0.03  536.0    1.0\n",
        "lambda[42]   0.01  2.7e-4 6.1e-3 2.0e-3 5.8e-3 9.0e-3   0.01   0.03  517.0    1.0\n",
        "lambda[43] 6.2e-3  2.0e-4 4.5e-3 9.2e-4 3.0e-3 5.1e-3 8.2e-3   0.02  483.0    1.0\n",
        "lambda[44] 3.4e-3  1.4e-4 3.0e-3 3.8e-4 1.5e-3 2.6e-3 4.5e-3   0.01  453.0    1.0\n",
        "lambda[45] 1.8e-3  9.1e-5 1.9e-3 1.5e-4 6.4e-4 1.2e-3 2.3e-3 6.7e-3  425.0    1.0\n",
        "lambda[46] 8.5e-4  5.4e-5 1.1e-3 5.0e-5 2.5e-4 5.2e-4 1.1e-3 3.6e-3  399.0    1.0\n",
        "lambda[47] 3.8e-4  3.0e-5 5.8e-4 1.6e-5 9.3e-5 2.0e-4 4.4e-4 1.8e-3  377.0    nan\n",
        "lambda[48] 1.5e-4  1.5e-5 2.9e-4 4.6e-6 3.1e-5 7.3e-5 1.7e-4 7.9e-4  358.0    nan\n",
        "lambda[49] 5.9e-5  7.1e-6 1.3e-4 1.2e-6 9.5e-6 2.4e-5 6.0e-5 3.2e-4  344.0    nan\n",
        "lambda[50] 2.1e-5  3.1e-6 5.6e-5 3.0e-7 2.6e-6 7.2e-6 1.9e-5 1.2e-4  334.0    nan\n",
        "lambda[51] 6.8e-6  1.2e-6 2.2e-5 6.6e-8 6.7e-7 2.0e-6 5.7e-6 4.1e-5  327.0    nan\n",
        "lambda[52] 2.1e-6  4.5e-7 8.1e-6 1.3e-8 1.6e-7 4.9e-7 1.5e-6 1.3e-5  323.0    nan\n",
        "lambda[53] 5.8e-7  1.5e-7 2.7e-6 2.5e-9 3.3e-8 1.1e-7 3.8e-7 3.7e-6  322.0    nan\n",
        "lambda[54] 1.5e-7  4.7e-8 8.5e-74.2e-10 6.5e-9 2.3e-8 8.5e-8 9.6e-7  323.0    nan\n",
        "lambda[55] 3.8e-8  1.3e-8 2.4e-76.4e-11 1.2e-9 4.5e-9 1.7e-8 2.3e-7  325.0    nan\n",
        "lambda[56] 8.5e-9  3.5e-9 6.4e-89.1e-121.9e-107.8e-10 3.3e-9 5.0e-8  329.0    nan\n",
        "lambda[57] 1.8e-9 8.4e-10 1.5e-81.2e-122.8e-111.2e-105.6e-10 9.9e-9  333.0    nan\n",
        "lambda[58]3.5e-10 1.9e-10 3.4e-91.4e-133.8e-121.8e-118.8e-11 1.8e-9  339.0    nan\n",
        "lambda[59]6.4e-11 3.7e-116.9e-101.5e-144.7e-132.4e-121.3e-113.0e-10  344.0    nan\n",
        "lambda[60]1.1e-11 6.9e-121.3e-101.5e-155.3e-142.9e-131.6e-124.6e-11  350.0    nan\n",
        "lp__       -56.82    0.05   0.97 -59.36 -57.19 -56.53 -56.11 -55.86  323.0   1.01\n",
        "\n",
        "Samples were drawn using NUTS(diag_e) at Sun Aug  9 15:33:52 2015.\n",
        "For each parameter, n_eff is a crude measure of effective sample size,\n",
        "and Rhat is the potential scale reduction factor on split chains (at \n",
        "convergence, Rhat=1)."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2000\n",
        "-25.9627004321\n",
        "3.51463176263\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "<matplotlib.collections.PathCollection at 0x10bbe8250>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10dca3310>"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Multilayer ReLU net\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import numpy\n",
      "\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "def relu(x):\n",
      "    return theano.tensor.switch(x<0, 0, x)\n",
      "\n",
      "class HiddenLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
      "                 activation=T.nnet.sigmoid):\n",
      "        \"\"\"\n",
      "        Typical hidden layer of a MLP: units are fully-connected and have\n",
      "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
      "        and the bias vector b is of shape (n_out,).\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.dmatrix\n",
      "        :param input: a symbolic tensor of shape (n_examples, n_in)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: dimensionality of input\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of hidden units\n",
      "\n",
      "        :type activation: theano.Op or function\n",
      "        :param activation: Non linearity to be applied in the hidden\n",
      "                           layer\n",
      "        \"\"\"\n",
      "        self.input = input\n",
      "        if W is None:\n",
      "            W_values = numpy.random.randn(n_in, n_out)\n",
      "            #W_values = .1 * numpy.random.randn(n_in, n_out)\n",
      "\n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        if b is None:\n",
      "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
      "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "\n",
      "        lin_output = T.dot(input, self.W) + self.b\n",
      "        self.output = (\n",
      "            lin_output if activation is None\n",
      "            else activation(lin_output)\n",
      "        )\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "class MLP(object):\n",
      "\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: symbolic variable that describes the input of the\n",
      "        architecture (one minibatch)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: number of input units, the dimension of the space in\n",
      "        which the datapoints lie\n",
      "\n",
      "        :type n_hidden: int\n",
      "        :param n_hidden: number of hidden units\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "        which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        # Since we are dealing with a one hidden layer MLP, this will translate\n",
      "        # into a HiddenLayer with a tanh activation function connected to the\n",
      "        # LogisticRegression layer; the activation function can be replaced by\n",
      "        # sigmoid or any other nonlinear function\n",
      "        self.hiddenLayer1 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            activation=T.nnet.sigmoid\n",
      "        )\n",
      "        \n",
      "        self.hiddenLayer2 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=self.hiddenLayer1.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out,\n",
      "            activation=relu\n",
      "        )\n",
      "        \n",
      "        self.y_pred = self.hiddenLayer2.output\n",
      "        \n",
      "        # the parameters of the model are the parameters of the two layers it is made out of\n",
      "        self.params = self.hiddenLayer1.params + self.hiddenLayer2.params\n",
      "    \n",
      "    def get_params(self):\n",
      "\n",
      "        params = {}\n",
      "        for param in self.params:\n",
      "            name = param.name\n",
      "            if name in params:\n",
      "                name = name, 2\n",
      "            params[name] = param.get_value()\n",
      "        return params\n",
      "    \n",
      "    def mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "    \n",
      "    def sym_mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean(((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "                      * ((self.y_pred[1] - y[0]) ** 2 + (self.y_pred[0] - y[1]) ** 2))\n",
      "        \n",
      "\n",
      "def shared_dataset(data_xy, borrow=True):\n",
      "        \"\"\" Function that loads the dataset into shared variables\n",
      "        \"\"\"\n",
      "        data_x, data_y, _ = data_xy\n",
      "        shared_x = theano.shared(numpy.asarray(data_x,\n",
      "                                               dtype='float32'),\n",
      "                                 borrow=borrow)\n",
      "        shared_y = theano.shared(numpy.asarray(data_y,\n",
      "                                               dtype='float32'),\n",
      "                                 borrow=borrow)\n",
      "        return shared_x, shared_y\n",
      "\n",
      "def train_nn(dataset, n_hidden=20, learning_rate=0.01, n_epochs=10, batch_size=20, test_data=None):\n",
      "    \"\"\"\n",
      "    Demonstrate stochastic gradient descent optimization for a multilayer\n",
      "    perceptron\n",
      "\n",
      "    :type learning_rate: float\n",
      "    :param learning_rate: learning rate used (factor for the stochastic\n",
      "    gradient\n",
      "\n",
      "    :type n_epochs: int\n",
      "    :param n_epochs: maximal number of epochs to run the optimizer\n",
      "\n",
      "    :type dataset: string\n",
      "    :param dataset: the path of the MNIST dataset file from\n",
      "                 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
      "\n",
      "\n",
      "   \"\"\"\n",
      "    train_set_x, train_set_y = shared_dataset(dataset)\n",
      "\n",
      "    # compute number of minibatches for training, validation and testing\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    \n",
      "    test_batch_size = 1\n",
      "    \n",
      "    ######################\n",
      "    # BUILD ACTUAL MODEL #\n",
      "    ######################\n",
      "    print '... building the model'\n",
      "\n",
      "    # allocate symbolic variables for the data\n",
      "    index = T.lscalar()  # index to a [mini]batch\n",
      "    x = T.fmatrix('x')   # input data from visual neurons\n",
      "    y = T.fmatrix('y')  # posterior\n",
      "\n",
      "    rng = numpy.random.RandomState(1234)\n",
      "\n",
      "    # construct the MLP class\n",
      "    nn = MLP(\n",
      "        rng=rng,\n",
      "        input=x,\n",
      "        n_in=61,\n",
      "        n_hidden=n_hidden,\n",
      "        n_out=2\n",
      "    )\n",
      "\n",
      "    # the cost we minimize during training is the mean squared error; cost is expressed\n",
      "    # here symbolically\n",
      "    cost = nn.mse(y)\n",
      "\n",
      "    # compute the gradient of cost with respect to theta (sotred in params)\n",
      "    # the resulting gradients will be stored in a list gparams\n",
      "    gparams = [T.grad(cost, param) for param in nn.params]\n",
      "\n",
      "    # specify how to update the parameters of the model as a list of\n",
      "    # (variable, update expression) pairs\n",
      "\n",
      "    # given two list the zip A = [a1, a2, a3, a4] and B = [b1, b2, b3, b4] of\n",
      "    # same length, zip generates a list C of same size, where each element\n",
      "    # is a pair formed from the two lists :\n",
      "    #    C = [(a1, b1), (a2, b2), (a3, b3), (a4, b4)]\n",
      "    updates = [\n",
      "        (param, param - learning_rate * gparam)\n",
      "        for param, gparam in zip(nn.params, gparams)\n",
      "    ]\n",
      "\n",
      "    # compiling a Theano function `train_model` that returns the cost, but\n",
      "    # in the same time updates the parameter of the model based on the rules\n",
      "    # defined in `updates`\n",
      "    train_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=cost,\n",
      "        updates=updates,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "\n",
      "    ###############\n",
      "    # TRAIN MODEL #\n",
      "    ###############\n",
      "    print '... training'\n",
      "\n",
      "    start_time = time.clock()\n",
      "\n",
      "    epoch = 0\n",
      "    done_looping = False\n",
      "\n",
      "    while (epoch < n_epochs) and (not done_looping):\n",
      "        epoch = epoch + 1\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "            \n",
      "            minibatch_avg_cost = train_model(minibatch_index)\n",
      "            \n",
      "            #print \"epoch \" + repr(epoch)\n",
      "            #print \"minibatch \" + repr(minibatch_index + 1) + \"/\" + repr(n_train_batches)\n",
      "            #print \"average cost \" + repr(minibatch_avg_cost)\n",
      "\n",
      "    end_time = time.clock()\n",
      "\n",
      "    def inspect_inputs(i, node, fn):\n",
      "        print i, node, \"input(s) value(s):\", [input[0] for input in fn.inputs]\n",
      "\n",
      "    def inspect_outputs(i, node, fn):\n",
      "        print \"output(s) value(s):\", [output[0] for output in fn.outputs]\n",
      "    \n",
      "    print 'testing'\n",
      "    test_batch_size = 1\n",
      "    test_set_x, test_set_y = shared_dataset(test_data)\n",
      "    test_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=nn.y_pred,\n",
      "        #mode=theano.compile.MonitorMode(pre_func=inspect_inputs, post_func=inspect_outputs),\n",
      "        givens={\n",
      "            x: test_set_x[index * test_batch_size: (index + 1) * test_batch_size]\n",
      "        },\n",
      "    )\n",
      "    \n",
      "    true_ys = test_set_y.get_value()\n",
      "    pred_ys = numpy.zeros(len(true_ys))\n",
      "    for i in range(len(true_ys)):\n",
      "        #pred_ys[i] = test_model(i)\n",
      "        print test_model(i), true_ys[i]\n",
      "    \n",
      "    print nn.get_params()\n",
      "    return pred_ys, true_ys\n",
      "\n",
      "ndata = 3000\n",
      "train_data = generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True)\n",
      "test_data = generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True)\n",
      "nn, _ = train_nn(train_data, n_hidden=10, learning_rate=.01, n_epochs=100, test_data=test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "... building the model\n",
        "... training"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "testing"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " InplaceDimShuffle{x,0}(b) input(s) value(s): [array([ -6.70330512, -36.3014527 ])]\n",
        "output(s) value(s): [array([[ -6.70330512, -36.3014527 ]])]\n",
        "1 InplaceDimShuffle{x,0}(b) input(s) value(s): [array([ -3.84938703e+02,  -1.18383764e+01,  -1.43649759e+01,\n",
        "        -3.33467960e-01,  -2.41810424e+01,  -1.70988290e+02,\n",
        "        -3.86985792e+01,  -4.74918754e+00,  -4.28672627e+02,\n",
        "        -2.42400780e-02])]\n",
        "output(s) value(s): [array([[ -3.84938703e+02,  -1.18383764e+01,  -1.43649759e+01,\n",
        "         -3.33467960e-01,  -2.41810424e+01,  -1.70988290e+02,\n",
        "         -3.86985792e+01,  -4.74918754e+00,  -4.28672627e+02,\n",
        "         -2.42400780e-02]])]\n",
        "2 ScalarFromTensor(<TensorType(int64, scalar)>) input(s) value(s): [array(0)]\n",
        "output(s) value(s): [0]\n",
        "3 Elemwise{add,no_inplace}(TensorConstant{1}, <TensorType(int64, scalar)>) input(s) value(s): [array(1), array(0)]\n",
        "output(s) value(s): [array(1)]\n",
        "4 ScalarFromTensor(Elemwise{add,no_inplace}.0) input(s) value(s): [array(1)]\n",
        "output(s) value(s): [1]\n",
        "5 Subtensor{int64:int64:}(<TensorType(float32, matrix)>, ScalarFromTensor.0, ScalarFromTensor.0) input(s) value(s): [array([[ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  3.,  0.,  0.,  1.,  1.,\n",
        "         1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
        "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
        "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  2.,  2.,  1.,  1.,  0.,  2.,\n",
        "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32), 0, 1]\n",
        "output(s) value(s): [array([[ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  3.,  0.,  0.,  1.,  1.,\n",
        "         1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
        "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
        "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  2.,  2.,  1.,  1.,  0.,  2.,\n",
        "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)]\n",
        "6 dot(Subtensor{int64:int64:}.0, W) input(s) value(s): [array([[ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  3.,  0.,  0.,  1.,  1.,\n",
        "         1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
        "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
        "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  2.,  2.,  1.,  1.,  0.,  2.,\n",
        "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32), array([[  7.98453304e-02,  -9.79311468e-01,  -8.65057946e-01,\n",
        "         -2.66771457e+00,   9.08660808e-02,  -2.78496093e-01,\n",
        "          1.81571115e+00,   9.88889062e-01,   1.15826800e+00,\n",
        "          8.45964604e-02],\n",
        "       [  4.79891368e-01,  -1.82884872e-01,  -8.59825997e-01,\n",
        "         -2.16983996e-02,  -1.16027116e+00,   8.09231538e-01,\n",
        "         -7.98060091e-01,  -2.18950239e-01,  -4.74165489e-01,\n",
        "          2.62740166e-01],\n",
        "       [ -9.86127765e-01,  -9.20675277e-01,   5.95425069e-01,\n",
        "          1.76329806e+00,  -5.16446724e-02,  -4.73301574e-01,\n",
        "         -1.66838668e-01,   2.64818366e-01,  -6.71224716e-02,\n",
        "         -4.47747142e-01],\n",
        "       [  3.45825295e-01,  -1.19117741e-01,  -5.76091482e-01,\n",
        "         -4.98359954e-01,   7.01095344e-01,   2.20809152e-01,\n",
        "         -1.16334580e+00,   3.96534696e-01,  -5.61087455e-01,\n",
        "          2.46915530e-01],\n",
        "       [  1.72021450e+00,   8.18601701e-01,  -4.48881239e-01,\n",
        "          4.49418176e-01,   8.57094955e-01,   4.26417680e-01,\n",
        "          4.87585821e-01,   2.00844975e+00,   4.02962679e-01,\n",
        "          1.43658920e+00],\n",
        "       [  9.53395870e-01,   1.54335284e+00,   3.49993430e-01,\n",
        "         -1.33337487e+00,  -1.28678316e+00,  -1.60336093e-01,\n",
        "         -1.38345002e+00,   1.59396802e+00,  -1.13727179e+00,\n",
        "         -4.50465742e-01],\n",
        "       [ -2.61462486e-01,   2.06632761e-01,   1.55235810e-01,\n",
        "         -2.26121635e+00,   1.54907837e-01,   3.41526385e+00,\n",
        "         -1.59833148e+00,   7.49908355e-01,   6.79269128e-01,\n",
        "         -7.53659661e-01],\n",
        "       [  3.64983568e-01,   2.90973200e-01,   1.71569207e+00,\n",
        "         -2.14131560e+00,   2.14161050e-01,   1.38803314e+00,\n",
        "          8.87262316e-01,  -1.04905762e+00,   7.28536651e-01,\n",
        "          4.71018901e-01],\n",
        "       [ -2.56566014e-01,  -4.08336747e-01,   1.20441898e+00,\n",
        "         -1.80493748e-01,  -1.28118572e-01,  -6.28342589e+01,\n",
        "         -1.37923312e+01,  -2.42122513e-01,  -1.57797505e+02,\n",
        "          1.91053062e-01],\n",
        "       [ -4.32354305e+00,   1.27914363e-02,   1.69900670e+00,\n",
        "          4.16175877e-01,  -1.57198230e+00,  -4.19147496e-01,\n",
        "          3.72202289e-01,   5.47051136e-01,   3.28379407e-01,\n",
        "         -3.09863647e-01],\n",
        "       [ -1.20960681e+00,   5.63928733e-01,   7.90964441e-01,\n",
        "          3.28052989e-01,   7.60041240e-01,   3.71830995e+00,\n",
        "         -2.07119131e+00,  -8.75841851e-01,   1.09106534e+00,\n",
        "          1.07943112e+00],\n",
        "       [ -2.65273805e-01,  -7.31031871e-01,  -1.11367092e+00,\n",
        "          1.12436740e+00,  -3.81266196e-01,  -6.41163691e+01,\n",
        "         -1.34091382e+01,  -2.10369543e-01,  -1.57161462e+02,\n",
        "          3.92037551e-01],\n",
        "       [ -1.18201210e+01,   3.67840734e+00,  -5.17175472e-01,\n",
        "         -3.65464435e-02,   5.88013862e+00,   1.72197531e+00,\n",
        "         -1.22270134e-02,   8.60118974e-01,  -2.25524929e+00,\n",
        "         -9.04677293e-01],\n",
        "       [  6.30830597e-01,  -1.06756604e+00,  -2.26056862e-01,\n",
        "         -3.70918930e-01,   1.14079450e+00,  -1.33563711e+02,\n",
        "         -2.82976312e+01,  -3.52553033e-01,  -3.12935904e+02,\n",
        "          1.10468849e-01],\n",
        "       [ -1.71347706e+00,  -2.74206690e+00,  -9.80985108e-01,\n",
        "          2.32535932e+00,   1.70093374e+00,  -1.34548650e+00,\n",
        "          9.10006658e-01,   1.46008176e+00,   1.74815545e+00,\n",
        "          2.10484012e+00],\n",
        "       [ -6.39906066e+00,   7.46296055e-01,  -8.69699701e-01,\n",
        "          8.80103741e-01,  -1.20551734e+00,  -6.53030360e+01,\n",
        "         -1.49626086e+01,  -3.71182473e-01,  -1.56090937e+02,\n",
        "          3.87808128e-01],\n",
        "       [ -1.68888559e+00,  -2.64632435e+00,  -4.39148744e-01,\n",
        "          6.00703111e-01,  -2.79078776e-01,  -2.68504174e+02,\n",
        "         -5.83591137e+01,  -4.86564420e-01,  -6.28368055e+02,\n",
        "         -8.61119070e-01],\n",
        "       [ -1.34448365e+01,   3.19663521e+00,   4.26072032e-01,\n",
        "         -5.60535506e-01,   3.22762722e+00,  -6.74125846e+01,\n",
        "         -1.51459857e+01,   9.39457898e-02,  -1.56604425e+02,\n",
        "          6.17430707e-01],\n",
        "       [ -2.57891742e+00,   1.98873837e-01,  -1.62750992e+00,\n",
        "          1.31502536e+00,   1.87833512e+00,  -2.64733423e+00,\n",
        "          9.73688439e-01,  -5.42456289e-01,  -2.83932087e-01,\n",
        "          1.27445833e+00],\n",
        "       [ -1.43810057e-01,   1.99874639e+00,  -1.83088572e+00,\n",
        "          6.89587721e-01,   2.79050719e+00,  -6.81294976e+01,\n",
        "         -1.44149341e+01,   1.22843788e-01,  -1.57051583e+02,\n",
        "         -1.65180574e+00],\n",
        "       [ -3.95129989e+02,   2.28713695e+00,  -1.43259482e+01,\n",
        "          2.57387992e-01,   7.43178690e-01,  -6.68421666e+01,\n",
        "         -1.50268193e+01,   7.17330776e-01,  -1.55252174e+02,\n",
        "         -4.19660955e-01],\n",
        "       [ -4.13040996e+02,   3.84116902e+00,  -1.39118099e+01,\n",
        "         -9.31457427e-01,   3.60974142e+00,   7.72891644e-01,\n",
        "         -2.33403327e+00,  -9.67959899e-02,   1.74246463e-01,\n",
        "         -1.49471270e+00],\n",
        "       [ -6.68265320e+00,   1.74953780e+00,  -6.10429159e-01,\n",
        "         -8.67774627e-01,   3.02728121e+00,   5.29989097e-01,\n",
        "          3.27516046e-01,   1.93988964e-01,  -1.98654262e+00,\n",
        "         -1.16012884e+00],\n",
        "       [ -3.90470949e+02,  -3.99377028e-01,  -1.42629289e+01,\n",
        "         -4.75499560e-01,   2.65352356e+00,  -7.34800488e-01,\n",
        "          7.46831750e-01,  -7.32451519e-01,  -1.43337517e+00,\n",
        "          1.65730294e+00],\n",
        "       [ -3.92701725e+02,  -6.91824875e-01,  -1.51563444e+01,\n",
        "          9.84556656e-01,  -4.54373815e-01,  -1.23841793e+00,\n",
        "          2.52941850e-02,  -1.86684420e+00,  -1.39836154e+00,\n",
        "         -3.46255752e-01],\n",
        "       [ -3.95802473e+02,   1.61781502e+00,  -1.32534541e+01,\n",
        "         -1.44774773e+00,  -7.03696194e-01,  -5.75737287e-01,\n",
        "         -3.29326737e-01,   2.07402052e-01,  -1.93891695e-01,\n",
        "          7.34233477e-02],\n",
        "       [ -1.16739124e+03,   1.02743232e+00,  -4.17740504e+01,\n",
        "          1.61116669e-01,   1.64402324e+00,  -8.44169292e-02,\n",
        "          1.31908994e-01,  -3.05892429e-01,   3.20184211e-02,\n",
        "         -4.91889999e-01],\n",
        "       [ -7.79287746e+02,   4.16231276e+00,  -2.73867957e+01,\n",
        "         -5.47470365e-01,   3.45984813e+00,   7.58390470e-01,\n",
        "         -1.90040989e+00,  -1.56042862e+00,  -6.18015308e-01,\n",
        "          3.11167440e-01],\n",
        "       [ -6.29118523e+00,   1.59165072e+00,   9.44179106e-03,\n",
        "         -1.49784016e-01,   6.93661959e-01,   9.37881300e-01,\n",
        "         -1.92629908e+00,   1.17022572e+00,  -1.40134347e-01,\n",
        "          7.39939072e-01],\n",
        "       [  1.27974090e+01,   3.16088041e+00,  -8.69989040e-01,\n",
        "         -1.15945242e+00,   1.30805493e+00,  -2.68992606e+02,\n",
        "         -5.93546817e+01,  -8.14585054e-01,  -6.27556913e+02,\n",
        "          1.18781911e+00],\n",
        "       [ -7.82482118e+02,  -1.64198397e+01,  -2.99109837e+01,\n",
        "          8.87886782e-01,  -2.65166331e+01,   2.21491118e+01,\n",
        "          1.45039600e+00,  -6.73632883e+00,   3.69981082e+00,\n",
        "         -1.14462767e+00],\n",
        "       [  2.36903265e+01,  -2.82351607e+00,  -1.86991016e+00,\n",
        "         -1.30566133e+00,   7.20416294e-01,  -6.65918387e+01,\n",
        "         -1.43929103e+01,  -1.08181723e+00,  -1.56019585e+02,\n",
        "         -2.64925690e-02],\n",
        "       [  2.95604799e+01,  -1.64868511e+01,  -2.15532831e+00,\n",
        "          6.33988017e-01,  -2.75435249e+01,  -5.45323093e+01,\n",
        "         -1.31625318e+01,  -3.48713461e+00,  -1.55637747e+02,\n",
        "         -7.63586285e-01],\n",
        "       [  3.43021259e+01,  -4.83768323e+01,  -3.52897212e-01,\n",
        "         -1.83321859e+00,  -8.02205461e+01,  -4.47778665e+01,\n",
        "         -1.50543349e+01,  -6.31829698e+00,  -1.54889376e+02,\n",
        "         -1.91722415e+00],\n",
        "       [  2.37363991e+01,  -1.15952197e+01,  -6.01416662e-01,\n",
        "         -1.06205099e+00,  -2.33328984e+01,  -6.60064604e+01,\n",
        "         -1.26168127e+01,  -3.72159789e+00,  -1.54849654e+02,\n",
        "         -4.75844953e-01],\n",
        "       [ -3.47928909e+02,  -1.65501886e+01,  -1.42783726e+01,\n",
        "         -1.27721831e+00,  -2.73801125e+01,  -4.31655290e+01,\n",
        "         -1.45258624e+01,  -6.65302041e+00,  -1.51829542e+02,\n",
        "          6.37006837e-01],\n",
        "       [ -3.92907697e+02,  -3.19838160e+01,  -1.53600925e+01,\n",
        "         -8.69040227e-01,  -5.22020751e+01,   3.47631001e+00,\n",
        "          8.21633731e-01,  -1.08471159e+00,  -3.59710770e-01,\n",
        "         -1.48882570e-01],\n",
        "       [ -7.56284024e+02,  -1.89136894e+01,  -2.81022691e+01,\n",
        "         -9.60036945e-01,  -2.78829635e+01,   6.95406230e-01,\n",
        "          5.54403480e-01,  -1.56342613e+00,   1.71830310e+00,\n",
        "          6.99628702e-02],\n",
        "       [  2.17481159e+01,  -1.59105053e+01,  -1.23836684e+00,\n",
        "         -9.07320691e-01,  -2.50224910e+01,   1.32461204e+01,\n",
        "         -4.63005909e-01,  -5.34795390e+00,   4.38492361e-01,\n",
        "         -1.14516560e+00],\n",
        "       [ -7.79257356e+02,  -3.35988527e+01,  -2.72567951e+01,\n",
        "         -1.39993541e+00,  -5.57879281e+01,   1.24177206e+01,\n",
        "          9.87386257e-01,  -4.32362552e+00,   3.52532865e+00,\n",
        "          1.52215460e-01],\n",
        "       [ -7.42152610e+02,  -1.19609516e+01,  -2.83107047e+01,\n",
        "          1.69148949e-02,  -2.49981552e+01,   1.11725657e+01,\n",
        "          1.46439178e+00,  -5.01689083e+00,   3.39936041e+00,\n",
        "         -9.32873576e-01],\n",
        "       [  6.86998764e-01,  -1.26991104e+01,  -1.46522582e-02,\n",
        "         -1.31367491e-01,  -2.45517100e+01,  -9.41489075e+01,\n",
        "         -2.32217576e+01,  -8.73359013e+00,  -2.66272527e+02,\n",
        "         -7.68849014e-01],\n",
        "       [ -3.83475349e+02,   1.78150033e+00,  -1.31543480e+01,\n",
        "          1.31121962e+00,   7.41326358e-01,   1.02112063e+01,\n",
        "          7.92218267e-01,  -7.55522142e+00,   4.39641136e+00,\n",
        "          3.89725785e-01],\n",
        "       [ -3.88487305e+02,  -1.51867383e+01,  -1.44034502e+01,\n",
        "         -8.10903474e-01,  -2.85702362e+01,  -2.11771969e+02,\n",
        "         -5.03037284e+01,  -7.97574372e+00,  -5.44731034e+02,\n",
        "         -7.10923671e-01],\n",
        "       [ -3.94050602e+02,   2.80573969e+00,  -1.36265291e+01,\n",
        "         -1.92946764e+00,   1.18308536e+00,   3.65541987e+01,\n",
        "         -1.08947016e-01,  -1.08936372e+01,   4.46282459e+00,\n",
        "          5.47779155e-01],\n",
        "       [  1.62433605e+01,   2.99307803e+00,   7.60856576e-02,\n",
        "          1.45115206e-02,  -3.59353139e-01,   1.01114610e+01,\n",
        "          5.77811488e+00,  -6.40692863e+00,   6.99302445e+00,\n",
        "          5.47519114e-01],\n",
        "       [  1.44404573e+01,   2.00481161e-01,  -1.97408579e+00,\n",
        "          1.22473060e-01,  -3.57397643e-01,   2.28264565e+01,\n",
        "         -6.75566576e-01,  -5.58984710e+00,   3.74763208e+00,\n",
        "          1.29319846e+00],\n",
        "       [ -1.09877978e+01,   2.46353986e+00,   4.20972926e-01,\n",
        "          3.96502698e-01,  -1.26669426e+00,  -1.05826563e+02,\n",
        "         -2.41603701e+01,  -5.98384316e+00,  -2.70628689e+02,\n",
        "         -1.72975603e+00],\n",
        "       [ -1.81536815e+01,   4.99707737e+00,  -8.19069309e-01,\n",
        "         -8.52822245e-01,  -9.37522865e-01,  -4.66976647e+02,\n",
        "         -9.76240572e+01,  -1.43579969e+00,  -1.09516438e+03,\n",
        "         -9.08448926e-01],\n",
        "       [ -1.15978881e+01,   5.87154129e+00,  -5.89543721e-01,\n",
        "          7.39336046e-01,  -1.12802005e+00,  -2.34363056e+02,\n",
        "         -5.09926318e+01,   5.33066478e-01,  -5.48447805e+02,\n",
        "          1.00612648e+00],\n",
        "       [ -2.95437203e-01,   5.03496652e+00,  -7.08965350e-01,\n",
        "         -1.33921174e+00,  -2.31252827e+00,  -1.17263625e+02,\n",
        "         -2.40409274e+01,  -2.38097927e+00,  -2.72933285e+02,\n",
        "          8.74292412e-01],\n",
        "       [  2.52842062e+00,   3.31036257e-02,   7.36552354e-01,\n",
        "         -1.39262174e-01,   1.56868576e-01,  -1.18068017e+02,\n",
        "         -2.30869426e+01,  -1.36158634e+00,  -2.73930753e+02,\n",
        "         -2.62702916e+00],\n",
        "       [ -6.14154695e+00,   3.60341837e-01,  -1.85198724e-01,\n",
        "          5.12133437e-01,  -6.68937507e-01,  -1.15405239e+02,\n",
        "         -2.71589332e+01,  -8.79501290e-01,  -2.76477813e+02,\n",
        "         -4.93301483e-01],\n",
        "       [  1.63137055e+00,  -1.32811913e+00,  -1.87837242e+00,\n",
        "         -3.82996756e-01,   1.43840625e+00,  -6.07729118e-01,\n",
        "          3.99330335e-01,  -1.58098557e+00,  -1.36702463e+00,\n",
        "         -4.88311944e-01],\n",
        "       [ -2.32218679e-01,   1.16796557e+00,  -2.93449148e-01,\n",
        "         -9.90525087e-02,  -1.49570540e+00,  -4.50746108e-01,\n",
        "         -1.15267031e+00,   5.03162994e-02,   8.67550109e-01,\n",
        "         -5.22962953e-01],\n",
        "       [ -2.73640951e-01,   2.69922892e-01,  -1.33219545e+00,\n",
        "          3.69664072e-01,  -3.39731825e-03,  -1.18619830e-01,\n",
        "         -1.03409123e+00,  -1.26833631e+00,   1.77306189e+00,\n",
        "         -1.05492574e+00],\n",
        "       [ -5.55703945e-01,  -5.53051886e-01,   1.68461252e+00,\n",
        "          4.77130730e-01,   1.03288347e+00,  -8.92926936e-02,\n",
        "          2.44535600e-01,   1.34900614e+00,  -1.15059678e+00,\n",
        "         -1.56478914e+00],\n",
        "       [  6.81526195e-01,  -1.30285107e+00,  -1.72817755e+00,\n",
        "          9.73513776e-01,   9.37038069e-01,   1.07741885e+00,\n",
        "          2.87371134e+00,  -1.42893321e+00,  -1.67807578e-01,\n",
        "         -6.16759299e-01],\n",
        "       [  4.22055055e-01,   1.17731927e-01,  -7.14907209e-01,\n",
        "          7.36574640e-01,  -7.82660167e-01,   3.16644054e-01,\n",
        "          1.92218424e+00,   1.43981848e+00,   1.06851908e+00,\n",
        "          1.41286302e+00],\n",
        "       [ -2.53121567e-01,   3.53296023e-01,  -1.12851749e+00,\n",
        "          4.82239222e-01,  -3.75051663e-01,  -4.38352910e-01,\n",
        "         -2.71605034e+00,  -1.93427400e+00,  -6.26904349e-01,\n",
        "          3.57789172e-01],\n",
        "       [  1.50671721e+00,   2.88349217e-01,  -1.42013450e+00,\n",
        "          8.81588598e-01,  -8.34984842e-02,  -3.63907951e-01,\n",
        "         -1.62946191e-01,   7.55716163e-01,  -2.90604354e-01,\n",
        "          7.90290897e-02]])]\n",
        "output(s) value(s): [array([[ -4.18298961e+02,   3.61055751e+00,  -1.80958077e+01,\n",
        "          2.01686045e-01,  -2.38233120e+01,  -1.76761749e+03,\n",
        "         -3.93440446e+02,  -3.46588228e+01,  -4.37332464e+03,\n",
        "         -7.97432149e+00]])]\n",
        "7 Elemwise{Composite{Switch(LT((i0 + i1), i2), i2, (i0 + i1))}}[(0, 0)](dot.0, InplaceDimShuffle{x,0}.0, TensorConstant{(1, 1) of 0}) input(s) value(s): [array([[ -4.18298961e+02,   3.61055751e+00,  -1.80958077e+01,\n",
        "          2.01686045e-01,  -2.38233120e+01,  -1.76761749e+03,\n",
        "         -3.93440446e+02,  -3.46588228e+01,  -4.37332464e+03,\n",
        "         -7.97432149e+00]]), array([[ -3.84938703e+02,  -1.18383764e+01,  -1.43649759e+01,\n",
        "         -3.33467960e-01,  -2.41810424e+01,  -1.70988290e+02,\n",
        "         -3.86985792e+01,  -4.74918754e+00,  -4.28672627e+02,\n",
        "         -2.42400780e-02]]), array([[0]], dtype=int8)]\n",
        "output(s) value(s): [array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])]\n",
        "8 Dot22(Elemwise{Composite{Switch(LT((i0 + i1), i2), i2, (i0 + i1))}}[(0, 0)].0, W) input(s) value(s): [array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]), array([[ -3.10538852e+01,  -2.22260577e+03],\n",
        "       [ -1.27284482e+01,  -4.44811547e+01],\n",
        "       [  1.32711214e+00,  -9.53083517e+01],\n",
        "       [ -7.61529072e-01,   1.44652266e-01],\n",
        "       [ -2.43219463e-01,  -7.28465392e+01],\n",
        "       [ -6.06908665e+02,  -1.71961932e+03],\n",
        "       [ -1.20214129e+02,  -3.35485209e+02],\n",
        "       [ -1.28179906e+00,   1.49161537e+00],\n",
        "       [ -1.85695106e+02,  -5.38101401e+02],\n",
        "       [  3.35468604e-01,   2.28222127e-01]])]\n",
        "output(s) value(s): [array([[ 0.,  0.]])]\n",
        "9 Elemwise{Composite{Switch(LT((i0 + i1), i2), i2, (i0 + i1))}}[(0, 0)](Dot22.0, InplaceDimShuffle{x,0}.0, TensorConstant{(1, 1) of 0}) input(s) value(s): [array([[ 0.,  0.]]), array([[ -6.70330512, -36.3014527 ]]), array([[0]], dtype=int8)]\n",
        "output(s) value(s): [array([[ 0.,  0.]])]\n",
        "[[ 0.  0.]] [  31.67548943  139.022995  ]\n",
        "{('W', 2): array([[ -3.10538852e+01,  -2.22260577e+03],\n",
        "       [ -1.27284482e+01,  -4.44811547e+01],\n",
        "       [  1.32711214e+00,  -9.53083517e+01],\n",
        "       [ -7.61529072e-01,   1.44652266e-01],\n",
        "       [ -2.43219463e-01,  -7.28465392e+01],\n",
        "       [ -6.06908665e+02,  -1.71961932e+03],\n",
        "       [ -1.20214129e+02,  -3.35485209e+02],\n",
        "       [ -1.28179906e+00,   1.49161537e+00],\n",
        "       [ -1.85695106e+02,  -5.38101401e+02],\n",
        "       [  3.35468604e-01,   2.28222127e-01]]), ('b', 2): array([ -6.70330512, -36.3014527 ]), 'b': array([ -3.84938703e+02,  -1.18383764e+01,  -1.43649759e+01,\n",
        "        -3.33467960e-01,  -2.41810424e+01,  -1.70988290e+02,\n",
        "        -3.86985792e+01,  -4.74918754e+00,  -4.28672627e+02,\n",
        "        -2.42400780e-02]), 'W': array([[  7.98453304e-02,  -9.79311468e-01,  -8.65057946e-01,\n",
        "         -2.66771457e+00,   9.08660808e-02,  -2.78496093e-01,\n",
        "          1.81571115e+00,   9.88889062e-01,   1.15826800e+00,\n",
        "          8.45964604e-02],\n",
        "       [  4.79891368e-01,  -1.82884872e-01,  -8.59825997e-01,\n",
        "         -2.16983996e-02,  -1.16027116e+00,   8.09231538e-01,\n",
        "         -7.98060091e-01,  -2.18950239e-01,  -4.74165489e-01,\n",
        "          2.62740166e-01],\n",
        "       [ -9.86127765e-01,  -9.20675277e-01,   5.95425069e-01,\n",
        "          1.76329806e+00,  -5.16446724e-02,  -4.73301574e-01,\n",
        "         -1.66838668e-01,   2.64818366e-01,  -6.71224716e-02,\n",
        "         -4.47747142e-01],\n",
        "       [  3.45825295e-01,  -1.19117741e-01,  -5.76091482e-01,\n",
        "         -4.98359954e-01,   7.01095344e-01,   2.20809152e-01,\n",
        "         -1.16334580e+00,   3.96534696e-01,  -5.61087455e-01,\n",
        "          2.46915530e-01],\n",
        "       [  1.72021450e+00,   8.18601701e-01,  -4.48881239e-01,\n",
        "          4.49418176e-01,   8.57094955e-01,   4.26417680e-01,\n",
        "          4.87585821e-01,   2.00844975e+00,   4.02962679e-01,\n",
        "          1.43658920e+00],\n",
        "       [  9.53395870e-01,   1.54335284e+00,   3.49993430e-01,\n",
        "         -1.33337487e+00,  -1.28678316e+00,  -1.60336093e-01,\n",
        "         -1.38345002e+00,   1.59396802e+00,  -1.13727179e+00,\n",
        "         -4.50465742e-01],\n",
        "       [ -2.61462486e-01,   2.06632761e-01,   1.55235810e-01,\n",
        "         -2.26121635e+00,   1.54907837e-01,   3.41526385e+00,\n",
        "         -1.59833148e+00,   7.49908355e-01,   6.79269128e-01,\n",
        "         -7.53659661e-01],\n",
        "       [  3.64983568e-01,   2.90973200e-01,   1.71569207e+00,\n",
        "         -2.14131560e+00,   2.14161050e-01,   1.38803314e+00,\n",
        "          8.87262316e-01,  -1.04905762e+00,   7.28536651e-01,\n",
        "          4.71018901e-01],\n",
        "       [ -2.56566014e-01,  -4.08336747e-01,   1.20441898e+00,\n",
        "         -1.80493748e-01,  -1.28118572e-01,  -6.28342589e+01,\n",
        "         -1.37923312e+01,  -2.42122513e-01,  -1.57797505e+02,\n",
        "          1.91053062e-01],\n",
        "       [ -4.32354305e+00,   1.27914363e-02,   1.69900670e+00,\n",
        "          4.16175877e-01,  -1.57198230e+00,  -4.19147496e-01,\n",
        "          3.72202289e-01,   5.47051136e-01,   3.28379407e-01,\n",
        "         -3.09863647e-01],\n",
        "       [ -1.20960681e+00,   5.63928733e-01,   7.90964441e-01,\n",
        "          3.28052989e-01,   7.60041240e-01,   3.71830995e+00,\n",
        "         -2.07119131e+00,  -8.75841851e-01,   1.09106534e+00,\n",
        "          1.07943112e+00],\n",
        "       [ -2.65273805e-01,  -7.31031871e-01,  -1.11367092e+00,\n",
        "          1.12436740e+00,  -3.81266196e-01,  -6.41163691e+01,\n",
        "         -1.34091382e+01,  -2.10369543e-01,  -1.57161462e+02,\n",
        "          3.92037551e-01],\n",
        "       [ -1.18201210e+01,   3.67840734e+00,  -5.17175472e-01,\n",
        "         -3.65464435e-02,   5.88013862e+00,   1.72197531e+00,\n",
        "         -1.22270134e-02,   8.60118974e-01,  -2.25524929e+00,\n",
        "         -9.04677293e-01],\n",
        "       [  6.30830597e-01,  -1.06756604e+00,  -2.26056862e-01,\n",
        "         -3.70918930e-01,   1.14079450e+00,  -1.33563711e+02,\n",
        "         -2.82976312e+01,  -3.52553033e-01,  -3.12935904e+02,\n",
        "          1.10468849e-01],\n",
        "       [ -1.71347706e+00,  -2.74206690e+00,  -9.80985108e-01,\n",
        "          2.32535932e+00,   1.70093374e+00,  -1.34548650e+00,\n",
        "          9.10006658e-01,   1.46008176e+00,   1.74815545e+00,\n",
        "          2.10484012e+00],\n",
        "       [ -6.39906066e+00,   7.46296055e-01,  -8.69699701e-01,\n",
        "          8.80103741e-01,  -1.20551734e+00,  -6.53030360e+01,\n",
        "         -1.49626086e+01,  -3.71182473e-01,  -1.56090937e+02,\n",
        "          3.87808128e-01],\n",
        "       [ -1.68888559e+00,  -2.64632435e+00,  -4.39148744e-01,\n",
        "          6.00703111e-01,  -2.79078776e-01,  -2.68504174e+02,\n",
        "         -5.83591137e+01,  -4.86564420e-01,  -6.28368055e+02,\n",
        "         -8.61119070e-01],\n",
        "       [ -1.34448365e+01,   3.19663521e+00,   4.26072032e-01,\n",
        "         -5.60535506e-01,   3.22762722e+00,  -6.74125846e+01,\n",
        "         -1.51459857e+01,   9.39457898e-02,  -1.56604425e+02,\n",
        "          6.17430707e-01],\n",
        "       [ -2.57891742e+00,   1.98873837e-01,  -1.62750992e+00,\n",
        "          1.31502536e+00,   1.87833512e+00,  -2.64733423e+00,\n",
        "          9.73688439e-01,  -5.42456289e-01,  -2.83932087e-01,\n",
        "          1.27445833e+00],\n",
        "       [ -1.43810057e-01,   1.99874639e+00,  -1.83088572e+00,\n",
        "          6.89587721e-01,   2.79050719e+00,  -6.81294976e+01,\n",
        "         -1.44149341e+01,   1.22843788e-01,  -1.57051583e+02,\n",
        "         -1.65180574e+00],\n",
        "       [ -3.95129989e+02,   2.28713695e+00,  -1.43259482e+01,\n",
        "          2.57387992e-01,   7.43178690e-01,  -6.68421666e+01,\n",
        "         -1.50268193e+01,   7.17330776e-01,  -1.55252174e+02,\n",
        "         -4.19660955e-01],\n",
        "       [ -4.13040996e+02,   3.84116902e+00,  -1.39118099e+01,\n",
        "         -9.31457427e-01,   3.60974142e+00,   7.72891644e-01,\n",
        "         -2.33403327e+00,  -9.67959899e-02,   1.74246463e-01,\n",
        "         -1.49471270e+00],\n",
        "       [ -6.68265320e+00,   1.74953780e+00,  -6.10429159e-01,\n",
        "         -8.67774627e-01,   3.02728121e+00,   5.29989097e-01,\n",
        "          3.27516046e-01,   1.93988964e-01,  -1.98654262e+00,\n",
        "         -1.16012884e+00],\n",
        "       [ -3.90470949e+02,  -3.99377028e-01,  -1.42629289e+01,\n",
        "         -4.75499560e-01,   2.65352356e+00,  -7.34800488e-01,\n",
        "          7.46831750e-01,  -7.32451519e-01,  -1.43337517e+00,\n",
        "          1.65730294e+00],\n",
        "       [ -3.92701725e+02,  -6.91824875e-01,  -1.51563444e+01,\n",
        "          9.84556656e-01,  -4.54373815e-01,  -1.23841793e+00,\n",
        "          2.52941850e-02,  -1.86684420e+00,  -1.39836154e+00,\n",
        "         -3.46255752e-01],\n",
        "       [ -3.95802473e+02,   1.61781502e+00,  -1.32534541e+01,\n",
        "         -1.44774773e+00,  -7.03696194e-01,  -5.75737287e-01,\n",
        "         -3.29326737e-01,   2.07402052e-01,  -1.93891695e-01,\n",
        "          7.34233477e-02],\n",
        "       [ -1.16739124e+03,   1.02743232e+00,  -4.17740504e+01,\n",
        "          1.61116669e-01,   1.64402324e+00,  -8.44169292e-02,\n",
        "          1.31908994e-01,  -3.05892429e-01,   3.20184211e-02,\n",
        "         -4.91889999e-01],\n",
        "       [ -7.79287746e+02,   4.16231276e+00,  -2.73867957e+01,\n",
        "         -5.47470365e-01,   3.45984813e+00,   7.58390470e-01,\n",
        "         -1.90040989e+00,  -1.56042862e+00,  -6.18015308e-01,\n",
        "          3.11167440e-01],\n",
        "       [ -6.29118523e+00,   1.59165072e+00,   9.44179106e-03,\n",
        "         -1.49784016e-01,   6.93661959e-01,   9.37881300e-01,\n",
        "         -1.92629908e+00,   1.17022572e+00,  -1.40134347e-01,\n",
        "          7.39939072e-01],\n",
        "       [  1.27974090e+01,   3.16088041e+00,  -8.69989040e-01,\n",
        "         -1.15945242e+00,   1.30805493e+00,  -2.68992606e+02,\n",
        "         -5.93546817e+01,  -8.14585054e-01,  -6.27556913e+02,\n",
        "          1.18781911e+00],\n",
        "       [ -7.82482118e+02,  -1.64198397e+01,  -2.99109837e+01,\n",
        "          8.87886782e-01,  -2.65166331e+01,   2.21491118e+01,\n",
        "          1.45039600e+00,  -6.73632883e+00,   3.69981082e+00,\n",
        "         -1.14462767e+00],\n",
        "       [  2.36903265e+01,  -2.82351607e+00,  -1.86991016e+00,\n",
        "         -1.30566133e+00,   7.20416294e-01,  -6.65918387e+01,\n",
        "         -1.43929103e+01,  -1.08181723e+00,  -1.56019585e+02,\n",
        "         -2.64925690e-02],\n",
        "       [  2.95604799e+01,  -1.64868511e+01,  -2.15532831e+00,\n",
        "          6.33988017e-01,  -2.75435249e+01,  -5.45323093e+01,\n",
        "         -1.31625318e+01,  -3.48713461e+00,  -1.55637747e+02,\n",
        "         -7.63586285e-01],\n",
        "       [  3.43021259e+01,  -4.83768323e+01,  -3.52897212e-01,\n",
        "         -1.83321859e+00,  -8.02205461e+01,  -4.47778665e+01,\n",
        "         -1.50543349e+01,  -6.31829698e+00,  -1.54889376e+02,\n",
        "         -1.91722415e+00],\n",
        "       [  2.37363991e+01,  -1.15952197e+01,  -6.01416662e-01,\n",
        "         -1.06205099e+00,  -2.33328984e+01,  -6.60064604e+01,\n",
        "         -1.26168127e+01,  -3.72159789e+00,  -1.54849654e+02,\n",
        "         -4.75844953e-01],\n",
        "       [ -3.47928909e+02,  -1.65501886e+01,  -1.42783726e+01,\n",
        "         -1.27721831e+00,  -2.73801125e+01,  -4.31655290e+01,\n",
        "         -1.45258624e+01,  -6.65302041e+00,  -1.51829542e+02,\n",
        "          6.37006837e-01],\n",
        "       [ -3.92907697e+02,  -3.19838160e+01,  -1.53600925e+01,\n",
        "         -8.69040227e-01,  -5.22020751e+01,   3.47631001e+00,\n",
        "          8.21633731e-01,  -1.08471159e+00,  -3.59710770e-01,\n",
        "         -1.48882570e-01],\n",
        "       [ -7.56284024e+02,  -1.89136894e+01,  -2.81022691e+01,\n",
        "         -9.60036945e-01,  -2.78829635e+01,   6.95406230e-01,\n",
        "          5.54403480e-01,  -1.56342613e+00,   1.71830310e+00,\n",
        "          6.99628702e-02],\n",
        "       [  2.17481159e+01,  -1.59105053e+01,  -1.23836684e+00,\n",
        "         -9.07320691e-01,  -2.50224910e+01,   1.32461204e+01,\n",
        "         -4.63005909e-01,  -5.34795390e+00,   4.38492361e-01,\n",
        "         -1.14516560e+00],\n",
        "       [ -7.79257356e+02,  -3.35988527e+01,  -2.72567951e+01,\n",
        "         -1.39993541e+00,  -5.57879281e+01,   1.24177206e+01,\n",
        "          9.87386257e-01,  -4.32362552e+00,   3.52532865e+00,\n",
        "          1.52215460e-01],\n",
        "       [ -7.42152610e+02,  -1.19609516e+01,  -2.83107047e+01,\n",
        "          1.69148949e-02,  -2.49981552e+01,   1.11725657e+01,\n",
        "          1.46439178e+00,  -5.01689083e+00,   3.39936041e+00,\n",
        "         -9.32873576e-01],\n",
        "       [  6.86998764e-01,  -1.26991104e+01,  -1.46522582e-02,\n",
        "         -1.31367491e-01,  -2.45517100e+01,  -9.41489075e+01,\n",
        "         -2.32217576e+01,  -8.73359013e+00,  -2.66272527e+02,\n",
        "         -7.68849014e-01],\n",
        "       [ -3.83475349e+02,   1.78150033e+00,  -1.31543480e+01,\n",
        "          1.31121962e+00,   7.41326358e-01,   1.02112063e+01,\n",
        "          7.92218267e-01,  -7.55522142e+00,   4.39641136e+00,\n",
        "          3.89725785e-01],\n",
        "       [ -3.88487305e+02,  -1.51867383e+01,  -1.44034502e+01,\n",
        "         -8.10903474e-01,  -2.85702362e+01,  -2.11771969e+02,\n",
        "         -5.03037284e+01,  -7.97574372e+00,  -5.44731034e+02,\n",
        "         -7.10923671e-01],\n",
        "       [ -3.94050602e+02,   2.80573969e+00,  -1.36265291e+01,\n",
        "         -1.92946764e+00,   1.18308536e+00,   3.65541987e+01,\n",
        "         -1.08947016e-01,  -1.08936372e+01,   4.46282459e+00,\n",
        "          5.47779155e-01],\n",
        "       [  1.62433605e+01,   2.99307803e+00,   7.60856576e-02,\n",
        "          1.45115206e-02,  -3.59353139e-01,   1.01114610e+01,\n",
        "          5.77811488e+00,  -6.40692863e+00,   6.99302445e+00,\n",
        "          5.47519114e-01],\n",
        "       [  1.44404573e+01,   2.00481161e-01,  -1.97408579e+00,\n",
        "          1.22473060e-01,  -3.57397643e-01,   2.28264565e+01,\n",
        "         -6.75566576e-01,  -5.58984710e+00,   3.74763208e+00,\n",
        "          1.29319846e+00],\n",
        "       [ -1.09877978e+01,   2.46353986e+00,   4.20972926e-01,\n",
        "          3.96502698e-01,  -1.26669426e+00,  -1.05826563e+02,\n",
        "         -2.41603701e+01,  -5.98384316e+00,  -2.70628689e+02,\n",
        "         -1.72975603e+00],\n",
        "       [ -1.81536815e+01,   4.99707737e+00,  -8.19069309e-01,\n",
        "         -8.52822245e-01,  -9.37522865e-01,  -4.66976647e+02,\n",
        "         -9.76240572e+01,  -1.43579969e+00,  -1.09516438e+03,\n",
        "         -9.08448926e-01],\n",
        "       [ -1.15978881e+01,   5.87154129e+00,  -5.89543721e-01,\n",
        "          7.39336046e-01,  -1.12802005e+00,  -2.34363056e+02,\n",
        "         -5.09926318e+01,   5.33066478e-01,  -5.48447805e+02,\n",
        "          1.00612648e+00],\n",
        "       [ -2.95437203e-01,   5.03496652e+00,  -7.08965350e-01,\n",
        "         -1.33921174e+00,  -2.31252827e+00,  -1.17263625e+02,\n",
        "         -2.40409274e+01,  -2.38097927e+00,  -2.72933285e+02,\n",
        "          8.74292412e-01],\n",
        "       [  2.52842062e+00,   3.31036257e-02,   7.36552354e-01,\n",
        "         -1.39262174e-01,   1.56868576e-01,  -1.18068017e+02,\n",
        "         -2.30869426e+01,  -1.36158634e+00,  -2.73930753e+02,\n",
        "         -2.62702916e+00],\n",
        "       [ -6.14154695e+00,   3.60341837e-01,  -1.85198724e-01,\n",
        "          5.12133437e-01,  -6.68937507e-01,  -1.15405239e+02,\n",
        "         -2.71589332e+01,  -8.79501290e-01,  -2.76477813e+02,\n",
        "         -4.93301483e-01],\n",
        "       [  1.63137055e+00,  -1.32811913e+00,  -1.87837242e+00,\n",
        "         -3.82996756e-01,   1.43840625e+00,  -6.07729118e-01,\n",
        "          3.99330335e-01,  -1.58098557e+00,  -1.36702463e+00,\n",
        "         -4.88311944e-01],\n",
        "       [ -2.32218679e-01,   1.16796557e+00,  -2.93449148e-01,\n",
        "         -9.90525087e-02,  -1.49570540e+00,  -4.50746108e-01,\n",
        "         -1.15267031e+00,   5.03162994e-02,   8.67550109e-01,\n",
        "         -5.22962953e-01],\n",
        "       [ -2.73640951e-01,   2.69922892e-01,  -1.33219545e+00,\n",
        "          3.69664072e-01,  -3.39731825e-03,  -1.18619830e-01,\n",
        "         -1.03409123e+00,  -1.26833631e+00,   1.77306189e+00,\n",
        "         -1.05492574e+00],\n",
        "       [ -5.55703945e-01,  -5.53051886e-01,   1.68461252e+00,\n",
        "          4.77130730e-01,   1.03288347e+00,  -8.92926936e-02,\n",
        "          2.44535600e-01,   1.34900614e+00,  -1.15059678e+00,\n",
        "         -1.56478914e+00],\n",
        "       [  6.81526195e-01,  -1.30285107e+00,  -1.72817755e+00,\n",
        "          9.73513776e-01,   9.37038069e-01,   1.07741885e+00,\n",
        "          2.87371134e+00,  -1.42893321e+00,  -1.67807578e-01,\n",
        "         -6.16759299e-01],\n",
        "       [  4.22055055e-01,   1.17731927e-01,  -7.14907209e-01,\n",
        "          7.36574640e-01,  -7.82660167e-01,   3.16644054e-01,\n",
        "          1.92218424e+00,   1.43981848e+00,   1.06851908e+00,\n",
        "          1.41286302e+00],\n",
        "       [ -2.53121567e-01,   3.53296023e-01,  -1.12851749e+00,\n",
        "          4.82239222e-01,  -3.75051663e-01,  -4.38352910e-01,\n",
        "         -2.71605034e+00,  -1.93427400e+00,  -6.26904349e-01,\n",
        "          3.57789172e-01],\n",
        "       [  1.50671721e+00,   2.88349217e-01,  -1.42013450e+00,\n",
        "          8.81588598e-01,  -8.34984842e-02,  -3.63907951e-01,\n",
        "         -1.62946191e-01,   7.55716163e-01,  -2.90604354e-01,\n",
        "          7.90290897e-02]])}\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}