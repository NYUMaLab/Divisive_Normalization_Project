{
 "metadata": {
  "name": "",
  "signature": "sha256:6c87f5cee38ae4152c035a5c16269d8681b0f1d6a75adfb7daad1b663c57b732"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import math\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "import pystan\n",
      "import matplotlib.pyplot as plt\n",
      "import argparse\n",
      "from itertools import product\n",
      "from scipy.stats import poisson\n",
      "import matplotlib.patches as mpatches\n",
      "\n",
      "nneuron = 61\n",
      "min_angle = -90\n",
      "max_angle = 90\n",
      "sprefs = np.linspace(min_angle, max_angle, nneuron)\n",
      "eps = np.finfo(np.float64).eps\n",
      "\n",
      "r_max = 10\n",
      "sigtc_sq = float(10**2)\n",
      "sigtc = 10\n",
      "c_50 = 13.1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def random_c2d(ndata, low, high, sort):\n",
      "    c_range = high - low\n",
      "    c = np.random.rand(2, ndata) * c_range + low\n",
      "    if sort:\n",
      "        c = np.sort(c, axis=0)\n",
      "    return c[0], c[1]\n",
      "\n",
      "def random_c(ndata, ndims, low, high, sort):\n",
      "    c_range = high - low\n",
      "    if ndims == 1:\n",
      "        c = np.random.rand(ndims, ndata)[0] * c_range + low\n",
      "    c = np.random.rand(ndims, ndata) * c_range + low\n",
      "    if sort:\n",
      "        c = np.sort(c, axis=0)\n",
      "    return c\n",
      "\n",
      "def generate_trainset(ndata, low=.3, high=1.3, crange=.5, r_max=10):\n",
      "    s_0, s_1 = np.random.rand(2, ndata) * 120 - 60\n",
      "    c_0 = random_c(ndata, 1, low, low+crange, True)\n",
      "    c_1 = random_c(ndata, 1, high, high+crange, True)\n",
      "    r, s, c = generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True, s_0, s_1, c_0, c_1)\n",
      "    return r, s, c\n",
      "\n",
      "def generate_testset(ndata, low=.3, high=1.3, r_max=10):\n",
      "    c_0, c_1 = random_c2d(ndata, low, high, True)\n",
      "    s_0, s_1 = np.random.rand(2, ndata) * 120 - 60\n",
      "    r, s, c = generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, \"poisson\", True, s_0, s_1, c_0, c_1)\n",
      "    return r, s, c\n",
      "    \n",
      "def generate_popcode_data(ndata, nneuron, sigtc_sq, c_50, r_max, noise, sort, s_0, s_1, c_0, c_1):\n",
      "    c_rms = np.sqrt(np.square(c_0) + np.square(c_1))\n",
      "    sprefs_data = np.tile(sprefs, (ndata, 1))\n",
      "    s_0t = np.exp(-np.square((np.transpose(np.tile(s_0, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_0 = c_0 * s_0t.T\n",
      "    s_1t = np.exp(-np.square((np.transpose(np.tile(s_1, (nneuron, 1))) - sprefs_data))/(2 * sigtc_sq))\n",
      "    stim_1 = c_1 * s_1t.T\n",
      "    #r = r_max * (stim_0 + stim_1)/(c_50 + c_rms)\n",
      "    r = r_max * (stim_0 + stim_1)\n",
      "    r = r.T\n",
      "    s = np.array((s_0, s_1)).T\n",
      "    s = s/90\n",
      "    c = np.array((c_0, c_1)).T\n",
      "    if noise == \"poisson\":\n",
      "        r = np.random.poisson(r) + 0.0\n",
      "    return r, s, c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "neurons_code = \"\"\"\n",
      "    data {\n",
      "        int<lower=0> N; // number of neurons\n",
      "        int r[N]; // neural response\n",
      "        real sprefs[N]; // preferred stimuli\n",
      "        int r_max;\n",
      "        //real c_rms;\n",
      "        //real c_50;\n",
      "        //real<lower=0> sig_tc;\n",
      "        real<lower=0> sigtc_sq;\n",
      "    }\n",
      "    parameters {\n",
      "        real s_1;\n",
      "        real s_2;\n",
      "        real c_1;\n",
      "        real<lower=c_1> c_2;\n",
      "    }\n",
      "    transformed parameters {\n",
      "        real lambda[N];\n",
      "        for (n in 1:N)\n",
      "            // lambda[n] <- r_max * ((c_1 * exp(normal_log(s_1, sprefs[n], sig_tc)) + c_2 * exp(normal_log(s_2, sprefs[n], sig_tc)))/(c_rms + c_50));\n",
      "            // lambda[n] <- r_max * (c_1 * exp(normal_log(s_1, sprefs[n], sig_tc)) + c_2 * exp(normal_log(s_2, sprefs[n], sig_tc)));\n",
      "            lambda[n] <- r_max * (c_1 * exp(- square(s_1 - sprefs[n])/(2 * sigtc_sq)) + c_2 * exp(- square(s_2 - sprefs[n])/(2 * sigtc_sq)));\n",
      "    }\n",
      "    model {\n",
      "        s_1 ~ uniform(-60, 60);\n",
      "        s_2 ~ uniform(-60, 60);\n",
      "        c_1 ~ uniform(0, 1);\n",
      "        c_2 ~ uniform(c_1, 1);\n",
      "        r ~ poisson(lambda);\n",
      "    }\n",
      "    \"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fisher_inf(s_0, s_1, c_0, c_1, r_max=10):\n",
      "    fs_0 = np.exp(-np.square((np.transpose(np.tile(s_0, (nneuron, 1))) - sprefs))/(2 * sigtc_sq))[0]\n",
      "    qs_0 = r_max * c_0 * fs_0\n",
      "    df_s0 = ((-s_0 + sprefs)/sigtc_sq) * qs_0\n",
      "    fs_1 = np.exp(-np.square((np.transpose(np.tile(s_1, (nneuron, 1))) - sprefs))/(2 * sigtc_sq))[0]\n",
      "    qs_1 = r_max * c_1 * fs_1\n",
      "    df_s1 = ((-s_1 + sprefs)/sigtc_sq) * qs_1\n",
      "    Q = qs_0 + qs_1\n",
      "    Q_inv = 1/Q\n",
      "    J_11 = np.sum(np.square(df_s0) * Q_inv)\n",
      "    J_22 = np.sum(np.square(df_s1) * Q_inv)\n",
      "    J_12 = J_21 = np.sum(df_s0 * df_s1 * Q_inv)\n",
      "    fisher = np.linalg.inv([[J_11, J_12], [J_21, J_22]])\n",
      "    return fisher\n",
      "\n",
      "def fit_optimal(r, sm, s=None, N=61, init=None, sprefs=sprefs, sampling=False, sort=False, c_1=[.5], c_2=[.5], c_50=13.1, r_max=10, c_rms=0.707106781, sig_tc=10, sigtc_sq=10**2):\n",
      "    ndata = len(r)\n",
      "    neurons_dat = {'N': 61,\n",
      "                   'r': r[0].astype(int),\n",
      "                   'sprefs': sprefs,\n",
      "                   'c_50': 13.1,\n",
      "                   'r_max': r_max,\n",
      "                   'c_rms': 0.707106781,\n",
      "                   'sig_tc': 10,\n",
      "                   'sigtc_sq': sigtc_sq}\n",
      "    optimal = np.zeros((2, ndata))\n",
      "    print init\n",
      "    for i in range(len(r)):\n",
      "        neurons_dat['r'] = r[i].astype(int)\n",
      "        if sampling:\n",
      "            op = sm.sampling(data=neurons_dat)\n",
      "            optimal[0][i], optimal[1][i] = np.mean(op['s_1']), np.mean(op['s_2'])\n",
      "        else:\n",
      "            if s:\n",
      "                if s.any():\n",
      "                    init = {'s_1':max(s[0][i], -60 + eps), 's_2':min(s[1][i] + eps, 60 - eps)}\n",
      "                    print init\n",
      "                    op = sm.optimizing(data=neurons_dat, init=init)\n",
      "            elif not init:\n",
      "                op = sm.optimizing(data=neurons_dat)\n",
      "            else:     \n",
      "                op = sm.optimizing(data=neurons_dat, init=init)\n",
      "            optimal[0][i], optimal[1][i] = op['s_1'], op['s_2']\n",
      "        if sort:\n",
      "            optimal = np.sort(optimal, axis=0)\n",
      "    return optimal"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Multilayer ReLU net\n",
      "\"\"\"\n",
      "\n",
      "def relu(x):\n",
      "    return theano.tensor.switch(x<0, 0, x)\n",
      "\n",
      "class HiddenLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
      "                 activation=T.nnet.sigmoid):\n",
      "        \"\"\"\n",
      "        Typical hidden layer of a MLP: units are fully-connected and have\n",
      "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
      "        and the bias vector b is of shape (n_out,).\n",
      "\n",
      "        :type rng: np.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.dmatrix\n",
      "        :param input: a symbolic tensor of shape (n_examples, n_in)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: dimensionality of input\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of hidden units\n",
      "\n",
      "        :type activation: theano.Op or function\n",
      "        :param activation: Non linearity to be applied in the hidden\n",
      "                           layer\n",
      "        \"\"\"\n",
      "        self.input = input\n",
      "        if W is None:\n",
      "            W_values = (1/np.sqrt(n_in)) * np.random.randn(n_in, n_out)\n",
      "            \n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        if b is None:\n",
      "            b_values = np.zeros((n_out,), dtype=theano.config.floatX)\n",
      "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "\n",
      "        lin_output = T.dot(input, self.W) + self.b\n",
      "        self.output = (\n",
      "            lin_output if activation is None\n",
      "            else activation(lin_output)\n",
      "        )\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "class COMLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None):\n",
      "        \"\"\"\n",
      "        Layer with Center of Mass decoder\n",
      "        Params same as above\n",
      "        \"\"\"\n",
      "        self.input = input\n",
      "        if W is None:\n",
      "            W_values = (1/np.sqrt(n_in)) * np.random.randn(n_in, n_out)\n",
      "\n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        \n",
      "        self.ones = np.ones((n_in, n_out))\n",
      "        \n",
      "        self.output = T.dot(input, self.W)/T.dot(input, self.ones)\n",
      "        \n",
      "        # parameters of the model\n",
      "        self.params = [self.W]\n",
      "\n",
      "class MLP(object):\n",
      "\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
      "\n",
      "        :type rng: np.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: symbolic variable that describes the input of the\n",
      "        architecture (one minibatch)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: number of input units, the dimension of the space in\n",
      "        which the datapoints lie\n",
      "\n",
      "        :type n_hidden: int\n",
      "        :param n_hidden: number of hidden units\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "        which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        self.hiddenLayer1 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            #activation=T.nnet.sigmoid\n",
      "            activation=relu\n",
      "        )\n",
      "        \n",
      "        self.hiddenLayer2 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=self.hiddenLayer1.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out,\n",
      "            #activation=relu\n",
      "            activation=None\n",
      "        )\n",
      "        \n",
      "        self.y_pred = self.hiddenLayer2.output\n",
      "        \n",
      "        # the parameters of the model are the parameters of the two layers it is made out of\n",
      "        self.params = self.hiddenLayer1.params + self.hiddenLayer2.params\n",
      "    \n",
      "    def get_params(self):\n",
      "\n",
      "        params = {}\n",
      "        for param in self.params:\n",
      "            name = param.name\n",
      "            if name in params:\n",
      "                name = name, 2\n",
      "            params[name] = param.get_value()\n",
      "        return params\n",
      "    \n",
      "    def mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "    \n",
      "    def mse_s1(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean((self.y_pred[0] - y[0]) ** 2)\n",
      "    \n",
      "    def valid_mse(self, y):\n",
      "        return T.mean(((self.y_pred[0] * 90) - (y[0] * 90)) ** 2 + ((self.y_pred[1] * 90) - (y[1] * 90)) ** 2)\n",
      "    \n",
      "    def sym_mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean(((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "                      * ((self.y_pred[1] - y[0]) ** 2 + (self.y_pred[0] - y[1]) ** 2))\n",
      "        \n",
      "class COMMLP(object):\n",
      "\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"\n",
      "        Params same as above\n",
      "        \"\"\"\n",
      "\n",
      "        self.hiddenLayer1 = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            activation=T.nnet.sigmoid\n",
      "        )\n",
      "        \n",
      "        self.hiddenLayer2 = COMLayer(\n",
      "            rng=rng,\n",
      "            input=self.hiddenLayer1.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out,\n",
      "        )\n",
      "        \n",
      "        self.y_pred = self.hiddenLayer2.output\n",
      "        \n",
      "        # the parameters of the model are the parameters of the two layers it is made out of\n",
      "        self.params = self.hiddenLayer1.params + self.hiddenLayer2.params\n",
      "    \n",
      "    def get_params(self):\n",
      "        params = {}\n",
      "        for param in self.params:\n",
      "            name = param.name\n",
      "            if name in params:\n",
      "                name = name, 2\n",
      "            params[name] = param.get_value()\n",
      "        return params\n",
      "    \n",
      "    def mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "    \n",
      "    def sym_mse(self, y):\n",
      "        # error between output and target\n",
      "        return T.mean(((self.y_pred[0] - y[0]) ** 2 + (self.y_pred[1] - y[1]) ** 2)\n",
      "                      * ((self.y_pred[1] - y[0]) ** 2 + (self.y_pred[0] - y[1]) ** 2))\n",
      "        \n",
      "\n",
      "def shared_dataset(data_xy, borrow=True):\n",
      "        \"\"\" Function that loads the dataset into shared variables\n",
      "        \"\"\"\n",
      "        data_x, data_y, _ = data_xy\n",
      "        shared_x = theano.shared(np.asarray(data_x,\n",
      "                                               dtype='float32'),\n",
      "                                 borrow=borrow)\n",
      "        shared_y = theano.shared(np.asarray(data_y,\n",
      "                                               dtype='float32'),\n",
      "                                 borrow=borrow)\n",
      "        return shared_x, shared_y\n",
      "\n",
      "def train_nn(train_dataset, valid_dataset=None, n_hidden=20, learning_rate=0.01, n_epochs=10, batch_size=20, test_data=None, COM=False, rho=False, nesterov=True, momentum=0, n_in=61, n_out=2):\n",
      "    \"\"\"\n",
      "    Demonstrate stochastic gradient descent optimization for a multilayer\n",
      "    perceptron\n",
      "\n",
      "    :type learning_rate: float\n",
      "    :param learning_rate: learning rate used (factor for the stochastic\n",
      "    gradient\n",
      "\n",
      "    :type n_epochs: int\n",
      "    :param n_epochs: maximal number of epochs to run the optimizer\n",
      "\n",
      "   \"\"\"\n",
      "    train_set_x, train_set_y = shared_dataset(train_dataset)\n",
      "    if valid_dataset:\n",
      "        valid_set_x, valid_set_y = shared_dataset(valid_dataset)\n",
      "\n",
      "    # compute number of minibatches for training, validation and testing\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    \n",
      "    \n",
      "    ######################\n",
      "    # BUILD ACTUAL MODEL #\n",
      "    ######################\n",
      "    print '... building the model'\n",
      "\n",
      "    # allocate symbolic variables for the data\n",
      "    index = T.lscalar()  # index to a [mini]batch\n",
      "    x = T.fmatrix('x')   # input data from visual neurons\n",
      "    y = T.fmatrix('y')  # ground truth\n",
      "\n",
      "    rng = np.random.RandomState(1234)\n",
      "\n",
      "    # construct the MLP class\n",
      "    nn = MLP(rng=rng, input=x, n_in=n_in, n_hidden=n_hidden, n_out=n_out)\n",
      "    \n",
      "    if COM:\n",
      "        nn = COMMLP(rng=rng, input=x, n_in=n_in, n_hidden=n_hidden, n_out=n_out)\n",
      "    else:\n",
      "        nn = MLP(rng=rng, input=x, n_in=n_in, n_hidden=n_hidden, n_out=n_out)\n",
      "\n",
      "    cost = nn.mse(y)\n",
      "    \n",
      "    def RMSprop(cost, params, learning_rate=0.001, rho=0.9, epsilon=1e-6, mu=0, nesterov=False):\n",
      "        gparams = T.grad(cost, params)\n",
      "        updates = []\n",
      "        for p, g in zip(params, gparams):\n",
      "            v = theano.shared(p.get_value() * 0.)\n",
      "            ms = theano.shared(p.get_value() * 0.)\n",
      "            ms_new = rho * ms + (1 - rho) * g ** 2\n",
      "            gradient_scaling = T.sqrt(ms_new + epsilon)\n",
      "            g = g / gradient_scaling\n",
      "            \"\"\"\n",
      "            (1) v_t = mu * v_t-1 - lr * gradient_f(params_t)\n",
      "            or\n",
      "            classic\n",
      "            (2) params_t = params_t-1 + v_t\n",
      "            nesterov\n",
      "            (7) params_t = params_t-1 + mu * v_t - lr * gradient_f(params_t-1)\n",
      "            (8) params_t = params_t-1 + mu**2 * v_t-1 - (1+mu) * lr * gradient_f(params_t-1)\n",
      "            \"\"\"\n",
      "            v_new = mu * v - (1 - mu) * learning_rate * g\n",
      "            if nesterov:\n",
      "                p_new = p + mu * v_new - (1 - mu) * learning_rate * g\n",
      "            else:\n",
      "                p_new = p + v_new\n",
      "            updates.append((ms, ms_new))\n",
      "            updates.append((v, v_new))\n",
      "            updates.append((p, p_new))\n",
      "                \n",
      "        return updates\n",
      "    \n",
      "    if rho:\n",
      "        updates = RMSprop(cost, nn.params, rho=rho, learning_rate=learning_rate, mu=momentum, nesterov=nesterov)\n",
      "    else:\n",
      "        # compute the gradient of cost with respect to theta (sotred in params)\n",
      "        # the resulting gradients will be stored in a list gparams\n",
      "        gparams = [T.grad(cost, param) for param in nn.params]\n",
      "\n",
      "        # specify how to update the parameters of the model as a list of\n",
      "        # (variable, update expression) pairs\n",
      "\n",
      "        updates = [\n",
      "            (param, param - learning_rate * gparam)\n",
      "            for param, gparam in zip(nn.params, gparams)\n",
      "        ]\n",
      "    \n",
      "    def inspect_inputs(i, node, fn):\n",
      "        print i, node, \"input(s) value(s):\", [input[0] for input in fn.inputs]\n",
      "\n",
      "    def inspect_outputs(i, node, fn):\n",
      "        print \"output(s) value(s):\", [output[0] for output in fn.outputs]\n",
      "\n",
      "    # compiling a Theano function `train_model` that returns the cost, but\n",
      "    # in the same time updates the parameter of the model based on the rules\n",
      "    # defined in `updates`\n",
      "    train_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=cost,\n",
      "        updates=updates,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "    \n",
      "    validate_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=nn.valid_mse(y),\n",
      "        givens={\n",
      "            x: valid_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "            y: valid_set_y[index * batch_size:(index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "\n",
      "    ###############\n",
      "    # TRAIN MODEL #\n",
      "    ###############\n",
      "    print '... training'\n",
      "\n",
      "    start_time = time.clock()\n",
      "\n",
      "    epoch = 0 \n",
      "    done_looping = False\n",
      "    \n",
      "    if valid_dataset:\n",
      "        valid_mse = np.zeros(n_epochs)\n",
      "\n",
      "    while (epoch < n_epochs) and (not done_looping):\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "            \n",
      "            minibatch_avg_cost = train_model(minibatch_index)\n",
      "            \n",
      "        if valid_dataset:\n",
      "            validation_losses = [validate_model(i) for i in xrange(n_valid_batches)]\n",
      "            this_validation_loss = np.mean(validation_losses)\n",
      "            valid_mse[epoch] = this_validation_loss \n",
      "\n",
      "            print(\n",
      "                'epoch %i, minibatch %i/%i, validation error %f' %\n",
      "                (\n",
      "                    epoch,\n",
      "                    minibatch_index + 1,\n",
      "                    n_train_batches,\n",
      "                    this_validation_loss\n",
      "                )\n",
      "            )\n",
      "            \n",
      "        epoch = epoch + 1\n",
      "\n",
      "    end_time = time.clock()\n",
      "    \n",
      "    if valid_dataset:\n",
      "        return nn, x, valid_mse\n",
      "    return nn, x\n",
      "\n",
      "def test_nn(nn, nnx, test_data):\n",
      "    print 'testing'\n",
      "    test_batch_size = 1\n",
      "    test_set_x, test_set_y = shared_dataset(test_data)\n",
      "    index = T.lscalar()  # index to a [mini]batch\n",
      "    x = nnx   # input data from visual neurons\n",
      "    test_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=nn.y_pred,\n",
      "        givens={\n",
      "            x: test_set_x[index * test_batch_size: (index + 1) * test_batch_size]\n",
      "        },\n",
      "    )\n",
      "    \n",
      "    true_ys = test_set_y.get_value()\n",
      "    pred_ys = np.zeros((len(true_ys), 2))\n",
      "    for i in range(len(true_ys)):\n",
      "        pred_ys[i] = test_model(i)\n",
      "        #print test_model(i)[0], true_ys[i]\n",
      "        #print test_model(i)[0] * 90, true_ys[i]\n",
      "    \n",
      "    #print nn.get_params()\n",
      "    return pred_ys, true_ys"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Setting up models\n",
      "sm = pystan.StanModel(model_code=neurons_code)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_statistics(s1, s2, preds):\n",
      "    mean_s1 = np.mean(preds[0])\n",
      "    mean_s2 = np.mean(preds[1])\n",
      "    bias_s1 = mean_s1 - s1\n",
      "    bias_s2 = mean_s2 - s2\n",
      "    covmat = np.cov(preds)\n",
      "    var_s1 = covmat[0, 0]\n",
      "    var_s2 = covmat[1, 1]\n",
      "    cov = covmat[0, 1]\n",
      "    corr = cov / (np.sqrt(var_s1) * np.sqrt(var_s2))\n",
      "    stats = {'mean_s1': mean_s1, 'mean_s2': mean_s2, 'bias_s1': bias_s1, 'bias_s2': bias_s2, 'var_s1': var_s1, 'var_s2': var_s2, 'cov': cov, 'corr': corr}\n",
      "    return stats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data_1 = generate_trainset(500000)\n",
      "valid_data_1 = generate_testset(3000)\n",
      "#nn_rms, nnx_rms, valid_mse_rms = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.0001, n_epochs=100, rho=.9)\n",
      "#nn_rms, nnx_rms, valid_mse_rms = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.0001, n_epochs=100, rho=.95)\n",
      "#nn_rms, nnx_rms, valid_mse_rms = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.0001, n_epochs=100, rho=.99)\n",
      "#nn_rms_m9, nnx_rms_m9, valid_mse_rms_m9 = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100, rho=.99, nesterov=True, momentum=0.9)\n",
      "#nn_rms_m9f, nnx_rms_m9f, valid_mse_rms_m9f = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100, rho=.99, nesterov=False, momentum=0.9)\n",
      "#nn_rms_m95, nnx_rms_m95, valid_mse_rms_m95 = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100, rho=.99, momentum=0.95)\n",
      "nn_rms_m99, nnx_rms_m99, valid_mse_rms_m99 = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.0001, n_epochs=100, rho=.99, momentum=0.99)\n",
      "#nn_plain, nnx_plain, valid_mse_plain = train_nn(train_data_1, valid_dataset=valid_data_1, n_hidden=20, learning_rate=.001, n_epochs=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "... building the model\n",
        "... training"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 0, minibatch 25000/25000, validation error 1032.920649"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 1, minibatch 25000/25000, validation error 861.401813"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 2, minibatch 25000/25000, validation error 824.774819"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 3, minibatch 25000/25000, validation error 791.336012"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 4, minibatch 25000/25000, validation error 766.000183"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 5, minibatch 25000/25000, validation error 759.344574"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 6, minibatch 25000/25000, validation error 747.348766"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 7, minibatch 25000/25000, validation error 733.235272"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 8, minibatch 25000/25000, validation error 725.544285"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 9, minibatch 25000/25000, validation error 716.869827"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 10, minibatch 25000/25000, validation error 709.159933"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 11, minibatch 25000/25000, validation error 701.767765"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 12, minibatch 25000/25000, validation error 695.584249"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 13, minibatch 25000/25000, validation error 691.035632"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 14, minibatch 25000/25000, validation error 686.622936"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 15, minibatch 25000/25000, validation error 682.925393"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 16, minibatch 25000/25000, validation error 677.657052"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 17, minibatch 25000/25000, validation error 672.178694"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 18, minibatch 25000/25000, validation error 667.026162"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 19, minibatch 25000/25000, validation error 661.648123"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 20, minibatch 25000/25000, validation error 657.978461"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 21, minibatch 25000/25000, validation error 654.414461"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 22, minibatch 25000/25000, validation error 650.752580"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 23, minibatch 25000/25000, validation error 648.074541"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 24, minibatch 25000/25000, validation error 643.659259"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 25, minibatch 25000/25000, validation error 638.740178"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 26, minibatch 25000/25000, validation error 636.364507"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 27, minibatch 25000/25000, validation error 633.203666"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 28, minibatch 25000/25000, validation error 631.533747"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 29, minibatch 25000/25000, validation error 630.628106"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 30, minibatch 25000/25000, validation error 631.818229"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 31, minibatch 25000/25000, validation error 631.312983"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 32, minibatch 25000/25000, validation error 631.387522"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 33, minibatch 25000/25000, validation error 630.607456"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 34, minibatch 25000/25000, validation error 630.068555"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 35, minibatch 25000/25000, validation error 629.226196"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 36, minibatch 25000/25000, validation error 629.455190"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 37, minibatch 25000/25000, validation error 629.767146"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 38, minibatch 25000/25000, validation error 630.814377"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 39, minibatch 25000/25000, validation error 628.544019"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 40, minibatch 25000/25000, validation error 628.557828"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 41, minibatch 25000/25000, validation error 625.112142"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 42, minibatch 25000/25000, validation error 621.328007"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 43, minibatch 25000/25000, validation error 620.124026"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 44, minibatch 25000/25000, validation error 619.454065"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 45, minibatch 25000/25000, validation error 616.082618"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 46, minibatch 25000/25000, validation error 613.099591"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 47, minibatch 25000/25000, validation error 610.899517"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 48, minibatch 25000/25000, validation error 608.179020"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 49, minibatch 25000/25000, validation error 604.930750"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 50, minibatch 25000/25000, validation error 602.532105"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 51, minibatch 25000/25000, validation error 599.095012"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 52, minibatch 25000/25000, validation error 596.068015"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 53, minibatch 25000/25000, validation error 592.752765"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 54, minibatch 25000/25000, validation error 589.986255"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 55, minibatch 25000/25000, validation error 588.567166"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 56, minibatch 25000/25000, validation error 586.658105"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 57, minibatch 25000/25000, validation error 584.491968"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 58, minibatch 25000/25000, validation error 582.931574"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 59, minibatch 25000/25000, validation error 581.396108"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 60, minibatch 25000/25000, validation error 579.685998"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 61, minibatch 25000/25000, validation error 577.649194"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 62, minibatch 25000/25000, validation error 575.715689"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 63, minibatch 25000/25000, validation error 573.712346"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 64, minibatch 25000/25000, validation error 572.203529"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 65, minibatch 25000/25000, validation error 570.821261"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 66, minibatch 25000/25000, validation error 569.787999"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 67, minibatch 25000/25000, validation error 567.621610"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 68, minibatch 25000/25000, validation error 567.129979"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 69, minibatch 25000/25000, validation error 563.823663"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 70, minibatch 25000/25000, validation error 561.880756"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 71, minibatch 25000/25000, validation error 560.924787"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 72, minibatch 25000/25000, validation error 561.303349"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 73, minibatch 25000/25000, validation error 561.458175"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 74, minibatch 25000/25000, validation error 561.884042"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 75, minibatch 25000/25000, validation error 561.688446"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 76, minibatch 25000/25000, validation error 561.768936"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 77, minibatch 25000/25000, validation error 561.780819"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 78, minibatch 25000/25000, validation error 563.797099"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 79, minibatch 25000/25000, validation error 564.464384"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 80, minibatch 25000/25000, validation error 564.061767"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 81, minibatch 25000/25000, validation error 563.750196"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 82, minibatch 25000/25000, validation error 563.985319"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 83, minibatch 25000/25000, validation error 563.642112"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 84, minibatch 25000/25000, validation error 563.934375"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 85, minibatch 25000/25000, validation error 562.987168"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 86, minibatch 25000/25000, validation error 561.578190"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 87, minibatch 25000/25000, validation error 561.373122"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 88, minibatch 25000/25000, validation error 560.957779"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 89, minibatch 25000/25000, validation error 560.145984"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 90, minibatch 25000/25000, validation error 559.629672"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 91, minibatch 25000/25000, validation error 559.172268"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 92, minibatch 25000/25000, validation error 558.921482"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 93, minibatch 25000/25000, validation error 558.143146"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 94, minibatch 25000/25000, validation error 558.275967"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 95, minibatch 25000/25000, validation error 557.546522"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 96, minibatch 25000/25000, validation error 556.910172"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 97, minibatch 25000/25000, validation error 557.024486"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 98, minibatch 25000/25000, validation error 556.412894"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 99, minibatch 25000/25000, validation error 556.547093"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}