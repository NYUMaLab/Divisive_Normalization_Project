\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Zoccolan07112007}
\citation{Busse2009}
\citation{Britten15061999}
\citation{rice06statistics}
\citation{Ernst2002}
\citation{Stocker2006}
\citation{Griffiths01092006}
\citation{Ma2006}
\citation{Jordan99anintroduction}
\citation{Doucet00onsequential}
\citation{AndrieuFDJ03}
\citation{NIPS2009_3782}
\citation{NIPS2011_4346}
\citation{NIPS2013_4876}
\citation{NIPS2002_2152}
\citation{LeCun2015}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{2}{section.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Mean firing rate of every neuron in the input population as a function of stimulus input\relax }}{2}{figure.caption.1}}
\citation{NairH10}
\citation{Maas2013}
\citation{Sutskever2013}
\citation{Tieleman2012}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Responses of an example population of orientation-tuned neurons to two stimuli presented individually and together\relax }}{3}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{Neural_Response}{{2}{3}{Responses of an example population of orientation-tuned neurons to two stimuli presented individually and together\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Diagram of three-layer network architecture. First layer contains 61 input units, second layer contains 20 hidden units and final layer contains two units, representing the estimate\relax }}{3}{figure.caption.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{4}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Estimation Performance}{4}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Fixed Gain}{4}{subsubsection.3.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Error of the posterior mean and network estimates in the fixed gains condition. The pair of gains used for training the networks, computing the posterior and testing for each plot is listed above the plot.\relax }}{4}{figure.caption.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Variable Gain}{5}{subsubsection.3.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Error of the posterior mean and network estimates in the variable gains condition. The pair of gains used for testing in each plot is listed above the plot. However, the same networks are used for all plots and in all plots the posterior mean marginalizes over gain.\relax }}{5}{figure.caption.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Generalization}{5}{subsubsection.3.1.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Variable Number of Stimuli}{5}{subsubsection.3.1.4}}
\citation{Qamar10122013}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Error of the posterior mean and network estimates in the generalization condition. The pair of gains used for testing in each plot is listed above the plot. However, the same networks are used for all plots.\relax }}{6}{figure.caption.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Error of the posterior mean and network estimates in the variable number of stimuli condition.\relax }}{6}{figure.caption.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Precision Readout}{6}{subsection.3.2}}
\citation{Qamar10122013}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Correlation of the sum of the hidden unit activations and the posterior precision (r=-.1, 95\% CI [-0.23, 0.04])\relax }}{7}{figure.caption.8}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{7}{section.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Correlation of trained linear combination of the hidden unit activations and the posterior precision (r=0.83, 95\% CI [0.78, 0.87])\relax }}{8}{figure.caption.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Correlation of trained linear combination of the hidden unit activations from network only trained on a single gain combination and the posterior precision (r=0.74, 95\% CI [0.67, 0.8])\relax }}{8}{figure.caption.10}}
\bibdata{abdnn}
\bibcite{AndrieuFDJ03}{1}
\bibcite{Britten15061999}{2}
\bibcite{Busse2009}{3}
\bibcite{Doucet00onsequential}{4}
\bibcite{Ernst2002}{5}
\bibcite{NIPS2013_4876}{6}
\bibcite{Griffiths01092006}{7}
\bibcite{NIPS2002_2152}{8}
\bibcite{Jordan99anintroduction}{9}
\bibcite{LeCun2015}{10}
\bibcite{Ma2006}{11}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Correlation of trained network estimates of precision and the posterior precision (r=0.91, 95\% CI [0.88, 0.93])\relax }}{9}{figure.caption.11}}
\bibcite{Maas2013}{12}
\bibcite{NairH10}{13}
\bibcite{Qamar10122013}{14}
\bibcite{rice06statistics}{15}
\bibcite{NIPS2011_4346}{16}
\bibcite{NIPS2009_3782}{17}
\bibcite{Stocker2006}{18}
\bibcite{Sutskever2013}{19}
\bibcite{Tieleman2012}{20}
\bibcite{Zoccolan07112007}{21}
\bibstyle{plain}
