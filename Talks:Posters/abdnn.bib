@inproceedings{NairH10,
  author    = {Vinod Nair and
               Geoffrey E. Hinton},
  title     = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  booktitle = {Proceedings of the 27th International Conference on Machine Learning
               (ICML-10), June 21-24, 2010, Haifa, Israel},
  pages     = {807--814},
  year      = {2010},
  url       = {http://www.icml2010.org/papers/432.pdf},
  timestamp = {Fri, 12 Jun 2015 19:15:11 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/icml/NairH10},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@inproceedings{Sutskever2013,
    Publisher = {JMLR Workshop and Conference Proceedings},
    Author = {Ilya Sutskever and James Martens and George E. Dahl and Geoffrey E. Hinton},
    Url = {http://jmlr.org/proceedings/papers/v28/sutskever13.pdf},
    Booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML-13)},
    Title = {On the importance of initialization and momentum in deep learning},
    Month = may,
    Volume = {28},
    Editor = {Sanjoy Dasgupta and David Mcallester},
    Year = {2013},
    Pages = {1139-1147}
   } 
@misc{Tieleman2012,
  title={{Lecture 6.5---RmsProp: Divide the gradient by a running average of its recent magnitude}},
  author={Tieleman, T. and Hinton, G.},
  howpublished={COURSERA: Neural Networks for Machine Learning},
  year={2012}
}
@inproceedings{Maas2013,
author = {Maas, Andrew L. and Hannun, Awni Y. and Ng, Andrew Y.},
booktitle = "International Conference on Machine Learning (ICML)",
title = {Rectifier Nonlinearities Improve Neural Network Acoustic Models},
url = {http://ai.stanford.edu/~amaas/papers/relu\_hybrid\_icml2013\_final.pdf},
year = {2013}
}
@article{Qamar10122013,
author = {Qamar, Ahmad T. and Cotton, R. James and George, Ryan G. and Beck, Jeffrey M. and Prezhdo, Eugenia and Laudano, Allison and Tolias, Andreas S. and Ma, Wei Ji}, 
title = {Trial-to-trial, uncertainty-based adjustment of decision boundaries in visual categorization},
volume = {110}, 
number = {50}, 
pages = {20332-20337}, 
year = {2013}, 
doi = {10.1073/pnas.1219756110}, 
abstract ={Categorization is a cornerstone of perception and cognition. Computationally, categorization amounts to applying decision boundaries in the space of stimulus features. We designed a visual categorization task in which optimal performance requires observers to incorporate trial-to-trial knowledge of the level of sensory uncertainty when setting their decision boundaries. We found that humans and monkeys did adjust their decision boundaries from trial to trial as the level of sensory noise varied, with some subjects performing near optimally. We constructed a neural network that implements uncertainty-based, near-optimal adjustment of decision boundaries. Divisive normalization emerges automatically as a key neural operation in this network. Our results offer an integrated computational and mechanistic framework for categorization under uncertainty.}, 
URL = {http://www.pnas.org/content/110/50/20332.abstract}, 
eprint = {http://www.pnas.org/content/110/50/20332.full.pdf}, 
journal = {Proceedings of the National Academy of Sciences} 
}
@article{Zoccolan07112007,
author = {Zoccolan, Davide and Kouh, Minjoon and Poggio, Tomaso and DiCarlo, James J.}, 
title = {Trade-Off between Object Selectivity and Tolerance in Monkey Inferotemporal Cortex},
volume = {27}, 
number = {45}, 
pages = {12292-12307}, 
year = {2007}, 
doi = {10.1523/JNEUROSCI.1897-07.2007}, 
abstract ={Object recognition requires both selectivity among different objects and tolerance to vastly different retinal images of the same object, resulting from natural variation in (e.g.) position, size, illumination, and clutter. Thus, discovering neuronal responses that have object selectivity and tolerance to identity-preserving transformations is fundamental to understanding object recognition. Although selectivity and tolerance are found at the highest level of the primate ventral visual stream [the inferotemporal cortex (IT)], both properties are highly varied and poorly understood. If an IT neuron has very sharp selectivity for a unique combination of object features (“diagnostic features”), this might automatically endow it with high tolerance. However, this relationship cannot be taken as given; although some IT neurons are highly object selective and some are highly tolerant, the empirical connection of these key properties is unknown. In this study, we systematically measured both object selectivity and tolerance to different identity-preserving image transformations in the spiking responses of a population of monkey IT neurons. We found that IT neurons with high object selectivity typically have low tolerance (and vice versa), regardless of how object selectivity was quantified and the type of tolerance examined. The discovery of this trade-off illuminates object selectivity and tolerance in IT and unifies a range of previous, seemingly disparate results. This finding also argues against the idea that diagnostic conjunctions of features guarantee tolerance. Instead, it is naturally explained by object recognition models in which object selectivity is built through AND-like tuning mechanisms.}, 
URL = {http://www.jneurosci.org/content/27/45/12292.abstract}, 
eprint = {http://www.jneurosci.org/content/27/45/12292.full.pdf+html}, 
journal = {The Journal of Neuroscience} 
}
@article{Busse2009,
abstract = {How do neuronal populations represent concurrent stimuli? We measured population responses in cat primary visual cortex (V1) using electrode arrays. Population responses to two superimposed gratings were weighted sums of the individual grating responses. The weights depended strongly on the relative contrasts of the gratings. When the contrasts were similar, the population performed an approximately equal summation. When the contrasts differed markedly, however, the population performed approximately a winner-take-all competition. Stimuli that were intermediate to these extremes elicited intermediate responses. This entire range of behaviors was explained by a single model of contrast normalization. Normalization captured both the spike responses and the local field potential responses; it even predicted visually evoked currents source-localized to V1 in human subjects. Normalization has profound effects on V1 population responses and is likely to shape the interpretation of these responses by higher cortical areas.},
author = {Busse, Laura and Wade, Alex R and Carandini, Matteo},
doi = {10.1016/j.neuron.2009.11.004},
file = {:Users/DavidHalpern/Library/Application Support/Mendeley Desktop/Downloaded/Busse, Wade, Carandini - 2009 - Representation of concurrent stimuli by population activity in visual cortex.pdf:pdf},
issn = {1097-4199},
journal = {Neuron},
keywords = {Action Potentials,Action Potentials: physiology,Animals,Cats,Computer Simulation,Contrast Sensitivity,Contrast Sensitivity: physiology,Electroencephalography,Electrophysiology,Electrophysiology: instrumentation,Electrophysiology: methods,Evoked Potentials,Evoked Potentials, Visual,Evoked Potentials, Visual: physiology,Evoked Potentials: physiology,Neurons,Neurons: physiology,Neuropsychological Tests,Photic Stimulation,Signal Processing, Computer-Assisted,Visual Cortex,Visual Cortex: anatomy \& histology,Visual Cortex: physiology,Visual Pathways,Visual Pathways: physiology,Visual Perception,Visual Perception: physiology},
month = dec,
number = {6},
pages = {931--42},
pmid = {20064398},
publisher = {Elsevier Ltd},
title = {{Representation of concurrent stimuli by population activity in visual cortex.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2807406\&tool=pmcentrez\&rendertype=abstract},
volume = {64},
year = {2009}
}
@article{Britten15061999,
author = {Britten, Kenneth H. and Heuer, Hilary W.}, 
title = {Spatial Summation in the Receptive Fields of MT Neurons},
volume = {19}, 
number = {12}, 
pages = {5074-5084}, 
year = {1999}, 
abstract ={Receptive fields (RFs) of cells in the middle temporal area (MT or V5) of monkeys will often encompass multiple objects under normal image viewing. We therefore have studied how multiple moving stimuli interact when presented within and near the RF of single MT cells. We used moving Gabor function stimuli, <1° in spatial extent and ?100 msec in duration, presented on a grid of possible locations over the RF of the cell. Responses to these stimuli were typically robust, and their small spatial and temporal extent allowed detailed mapping of RFs and of interactions between stimuli. The responses to pairs of such stimuli were compared against the responses to the same stimuli presented singly. The responses were substantially less than the sum of the responses to the component stimuli and were well described by a power-law summation model with divisive inhibition. Such divisive inhibition is a key component of recently proposed “normalization” models of cortical physiology and is presumed to arise from lateral interconnections within a region. One open question is whether the normalization occurs only once in primary visual cortex or multiple times in different cortical areas. We addressed this question by exploring the spatial extent over which one stimulus would divide the response to another and found effective normalization from stimuli quite far removed from the RF center. This supports models under which normalization occurs both in MT and in earlier stages.}, 
URL = {http://www.jneurosci.org/content/19/12/5074.abstract}, 
eprint = {http://www.jneurosci.org/content/19/12/5074.full.pdf+html}, 
journal = {The Journal of Neuroscience} 
}
@article{Ma2006,
annote = {10.1038/nn1790},
author = {Ma, Wei Ji and Beck, Jeffrey M and Latham, Peter E and Pouget, Alexandre},
issn = {1097-6256},
journal = {Nat Neurosci},
month = nov,
number = {11},
pages = {1432--1438},
title = {{Bayesian inference with probabilistic population codes}},
url = {http://dx.doi.org/10.1038/nn1790 http://www.nature.com/neuro/journal/v9/n11/suppinfo/nn1790\_S1.html},
volume = {9},
year = {2006}
}
@Book{rice06statistics,
author = {John A. Rice},
title = {Mathematical Statistics and Data Analysis},
publisher = {Duxbury Press},
year = 2006,
}
@article{Ernst2002,
annote = {10.1038/415429a},
author = {Ernst, Marc O and Banks, Martin S},
issn = {0028-0836},
journal = {Nature},
month = jan,
number = {6870},
pages = {429--433},
title = {{Humans integrate visual and haptic information in a statistically optimal fashion}},
url = {http://dx.doi.org/10.1038/415429a},
volume = {415},
year = {2002}
}
@article{Stocker2006,
annote = {10.1038/nn1669},
author = {Stocker, Alan A and Simoncelli, Eero P},
issn = {1097-6256},
journal = {Nat Neurosci},
month = apr,
number = {4},
pages = {578--585},
publisher = {Nature Publishing Group},
title = {{Noise characteristics and prior expectations in human visual speed perception}},
url = {http://dx.doi.org/10.1038/nn1669 http://www.nature.com/neuro/journal/v9/n4/suppinfo/nn1669\_S1.html},
volume = {9},
year = {2006}
}
@article{Griffiths01092006,
author = {Griffiths, Thomas L. and Tenenbaum, Joshua B.}, 
title = {Optimal Predictions in Everyday Cognition},
volume = {17}, 
number = {9}, 
pages = {767-773}, 
year = {2006}, 
doi = {10.1111/j.1467-9280.2006.01780.x}, 
abstract ={Human perception and memory are often explained as optimal statistical inferences that are informed by accurate prior probabilities. In contrast, cognitive judgments are usually viewed as following error-prone heuristics that are insensitive to priors. We examined the optimality of human cognition in a more realistic context than typical laboratory studies, asking people to make predictions about the duration or extent of everyday phenomena such as human life spans and the box-office take of movies. Our results suggest that everyday cognitive judgments follow the same optimal statistical principles as perception and memory, and reveal a close correspondence between people's implicit probabilistic models and the statistics of the world.}, 
URL = {http://pss.sagepub.com/content/17/9/767.abstract}, 
eprint = {http://pss.sagepub.com/content/17/9/767.full.pdf+html}, 
journal = {Psychological Science} 
}
@INPROCEEDINGS{Jordan99anintroduction,
    author = {Michael I. Jordan and Zoubin Ghahramani and et al.},
    title = {An introduction to variational methods for graphical models},
    booktitle = {MACHINE LEARNING},
    year = {1999},
    pages = {183--233},
    publisher = {MIT Press}
}
@ARTICLE{Doucet00onsequential,
    author = {Arnaud Doucet and Simon Godsill and Christophe Andrieu},
    title = {On Sequential Monte Carlo Sampling Methods for Bayesian Filtering},
    journal = {STATISTICS AND COMPUTING},
    year = {2000},
    volume = {10},
    number = {3},
    pages = {197--208}
}
@article{AndrieuFDJ03,
  author    = {Christophe Andrieu and
               Nando de Freitas and
               Arnaud Doucet and
               Michael I. Jordan},
  title     = {An Introduction to {MCMC} for Machine Learning},
  journal   = {Machine Learning},
  volume    = {50},
  number    = {1-2},
  pages     = {5--43},
  year      = {2003},
  url       = {http://dx.doi.org/10.1023/A:1020281327116},
  doi       = {10.1023/A:1020281327116},
  timestamp = {Thu, 26 May 2011 15:25:22 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/ml/AndrieuFDJ03},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@incollection{NIPS2009_3782,
title = {Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling},
author = {Shi, Lei and Thomas L. Griffiths},
booktitle = {Advances in Neural Information Processing Systems 22},
editor = {Y. Bengio and D. Schuurmans and J.D. Lafferty and C.K.I. Williams and A. Culotta},
pages = {1669--1677},
year = {2009},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3782-neural-implementation-of-hierarchical-bayesian-inference-by-importance-sampling.pdf}
}
@incollection{NIPS2011_4346,
title = {Select and Sample - A Model of Efficient Neural Inference and Learning},
author = {Jacquelyn A. Shelton and Abdul S. Sheikh and Pietro Berkes and Joerg Bornschein and Joerg Luecke},
booktitle = {Advances in Neural Information Processing Systems 24},
editor = {J. Shawe-Taylor and R.S. Zemel and P.L. Bartlett and F. Pereira and K.Q. Weinberger},
pages = {2618--2626},
year = {2011},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4346-select-and-sample-a-model-of-efficient-neural-inference-and-learning.pdf}
}
@incollection{NIPS2013_4876,
title = {Demixing odors - fast inference in olfaction},
author = {Grabska-Barwinska, Agnieszka and Beck, Jeff and Pouget, Alexandre and Latham, Peter},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {C.J.C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
pages = {1968--1976},
year = {2013},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4876-demixing-odors-fast-inference-in-olfaction.pdf}
}
@incollection{NIPS2002_2152,
title = {Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior},
author = {Patrik O. Hoyer and Aapo Hyv\"{a}rinen},
booktitle = {Advances in Neural Information Processing Systems 15},
editor = {S. Becker and S. Thrun and K. Obermayer},
pages = {293--300},
year = {2003},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/2152-interpreting-neural-response-variability-as-monte-carlo-sampling-of-the-posterior.pdf}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
issn = {0028-0836},
journal = {Nature},
month = may,
number = {7553},
pages = {436--444},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
title = {{Deep learning}},
url = {http://dx.doi.org/10.1038/nature14539 10.1038/nature14539},
volume = {521},
year = {2015}
}